{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kLbLUDthaYUx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Documents\\New folder\n"
          ]
        }
      ],
      "source": [
        "# find your current directory\n",
        "import os\n",
        "curDir = os.getcwd()\n",
        "print(curDir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t6pAdluZa-UF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from dnn_app_utils_v3 import *\n",
        "from sklearn.metrics import accuracy_score\n",
        "from planar_utils import plot_decision_boundary, sigmoid\n",
        "from testCases_v2 import *\n",
        "from sklearn.linear_model import LogisticRegressionCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wt8UKz_YbCx2"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def convert_to_rgb(image):\n",
        "    if image.mode != 'RGB':\n",
        "        return image.convert('RGB')\n",
        "    return image\n",
        "\n",
        "def load_images_from_folder(base_path, size=(64, 64)):\n",
        "    X = []\n",
        "    y = []\n",
        "    class_labels = sorted(os.listdir(base_path))  # Ensure consistent label assignment\n",
        "\n",
        "    for label, class_folder in enumerate(class_labels):\n",
        "        class_path = os.path.join(base_path, class_folder)\n",
        "        for img_file in os.listdir(class_path):\n",
        "            if img_file.endswith('.jpg'):  # Adjust depending on your image file types\n",
        "                img_path = os.path.join(class_path, img_file)\n",
        "                with Image.open(img_path) as img:\n",
        "                    img = convert_to_rgb(img)  # Convert all images to RGB\n",
        "                    img = img.resize(size)\n",
        "                    img_array = np.array(img)\n",
        "                    X.append(img_array)\n",
        "                    y.append(label)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "# Load images\n",
        "train_folder = r\"C:\\Users\\xiang\\Documents\\New folder\\archive\\train\"\n",
        "test_folder = r\"C:\\Users\\xiang\\Documents\\New folder\\archive\\test\"\n",
        "\n",
        "X_train, y_train = load_images_from_folder(train_folder)\n",
        "X_test, y_test = load_images_from_folder(test_folder)\n",
        "\n",
        "# Combine training and testing data\n",
        "X_combined = np.concatenate((X_train, X_test), axis=0)\n",
        "y_combined = np.concatenate((y_train, y_test), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2280,)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_combined.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary Statistics (Pre Processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Description       Mean  Standard Deviation  Median  Min  Max  Sample Size\n",
            "0           X  99.118354           78.994050    84.0    0  255     28016640\n",
            "1           Y   0.485965            0.499913     0.0    0    1         2280\n"
          ]
        }
      ],
      "source": [
        "flattened_data = X_combined.reshape(X_combined.shape[0], -1)  # Shape will be (2280, 64*64*3)\n",
        "df_x = pd.DataFrame(flattened_data)\n",
        "df_y = pd.DataFrame(y_combined, columns=['Y'])\n",
        "\n",
        "# Flatten the DataFrame into a single series\n",
        "flattened_series_x = df_x.stack()\n",
        "\n",
        "# Calculate the statistics for X\n",
        "mean_value_x = flattened_series_x.mean()\n",
        "std_value_x = flattened_series_x.std()\n",
        "median_value_x = flattened_series_x.median()\n",
        "min_value_x = flattened_series_x.min()\n",
        "max_value_x = flattened_series_x.max()\n",
        "count_value_x = flattened_series_x.count()\n",
        "\n",
        "# Calculate the statistics for Y\n",
        "mean_value_y = df_y['Y'].mean()\n",
        "std_value_y = df_y['Y'].std()\n",
        "median_value_y = df_y['Y'].median()\n",
        "min_value_y = df_y['Y'].min()\n",
        "max_value_y = df_y['Y'].max()\n",
        "count_value_y = df_y['Y'].count()\n",
        "\n",
        "# Create a summary DataFrame\n",
        "summary_stats = pd.DataFrame({\n",
        "    'Description': ['X', 'Y'],\n",
        "    'Mean': [mean_value_x, mean_value_y],\n",
        "    'Standard Deviation': [std_value_x, std_value_y],\n",
        "    'Median': [median_value_x, median_value_y],\n",
        "    'Min': [min_value_x, min_value_y],\n",
        "    'Max': [max_value_x, max_value_y],\n",
        "    'Sample Size': [count_value_x, count_value_y]\n",
        "})\n",
        "\n",
        "# Display the summary statistics\n",
        "print(summary_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-_Pm24YJbF0m"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "# Split dataset into train+cv combined and test\n",
        "X1, X_test, y1, y_test = train_test_split(X_combined, y_combined, test_size=0.2)\n",
        "# Further split train+cv into train and cross-validation\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(X1, y1, test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Twn4KzUCbbmE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cv_y shape: (456,)\n",
            "Number of training examples: x_train = 1368\n",
            "Number of testing examples: x_test = 456\n",
            "Number of testing examples: cv_train = 456\n",
            "Height/Width of each image: num_px = 64\n",
            "Each image is of size: (64, 64, 3)\n",
            "X_train shape: (1368, 64, 64, 3)\n",
            "y_train shape: (1368,)\n",
            "X_test shape: (456, 64, 64, 3)\n",
            "y_test shape: (456,)\n"
          ]
        }
      ],
      "source": [
        "# Checking dimensions\n",
        "\n",
        "m_train = X_train.shape[0]\n",
        "y_train_shape = y_train.shape[0]\n",
        "m_test = X_test.shape[0]\n",
        "cv_train = X_cv.shape[0]\n",
        "cv_test = y_cv.shape[0]\n",
        "num_px = X_train.shape[1]\n",
        "print(\"cv_y shape: \" +str (y_cv.shape))\n",
        "print(\"Number of training examples: x_train = \" + str(m_train))\n",
        "print (\"Number of testing examples: x_test = \" + str(m_test))\n",
        "print (\"Number of testing examples: cv_train = \" + str(cv_train))\n",
        "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"y_train shape: \" + str(y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"y_test shape: \" + str(y_test.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TfwB1zbxbhJT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train_flatten shape: (1368, 12288)\n",
            "train_set_y shape: (1368,)\n",
            "X_test_flatten shape: (12288, 456)\n"
          ]
        }
      ],
      "source": [
        "# Reshape the training and test examples\n",
        "X_train_flatten = X_train.reshape(X_train.shape[0],-1)\n",
        "X_test_flatten = X_test.reshape(X_test.shape[0],-1).T\n",
        "cv_set_x_flatten = X_cv.reshape(X_cv.shape[0],-1)\n",
        "\n",
        "\n",
        "print (\"X_train_flatten shape: \" + str(X_train_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(y_train.shape))\n",
        "print (\"X_test_flatten shape: \" + str(X_test_flatten.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ontbF8TjIim3"
      },
      "outputs": [],
      "source": [
        "train_set_x = X_train_flatten/255\n",
        "test_set_x = X_test_flatten/255\n",
        "cv_x = cv_set_x_flatten/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(456, 12288)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv_x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary Statistics (Post Processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary statistics for Train_set_x:\n",
            "count    1.680998e+07\n",
            "mean     3.850026e-01\n",
            "std      3.095029e-01\n",
            "min      0.000000e+00\n",
            "25%      9.803922e-02\n",
            "50%      3.215686e-01\n",
            "75%      6.235294e-01\n",
            "max      1.000000e+00\n",
            "dtype: float64\n",
            "\n",
            "Summary statistics for CV_x:\n",
            "count    5.603328e+06\n",
            "mean     3.853041e-01\n",
            "std      3.084803e-01\n",
            "min      0.000000e+00\n",
            "25%      9.803922e-02\n",
            "50%      3.215686e-01\n",
            "75%      6.235294e-01\n",
            "max      1.000000e+00\n",
            "dtype: float64\n",
            "\n",
            "Summary statistics for y_train:\n",
            "           Y_train\n",
            "count  1368.000000\n",
            "mean      0.496345\n",
            "std       0.500169\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max       1.000000\n"
          ]
        }
      ],
      "source": [
        "df_y_train = pd.DataFrame(y_train, columns=['Y_train'])\n",
        "df_train_set_x = pd.DataFrame(train_set_x)\n",
        "df_cv_x = pd.DataFrame(cv_x)\n",
        "flattened_series_train_x = df_train_set_x.stack()\n",
        "flattened_series_cv_x = df_cv_x.stack()\n",
        "\n",
        "print(\"\\nSummary statistics for Train_set_x:\")\n",
        "print(flattened_series_train_x.describe())\n",
        "\n",
        "print(\"\\nSummary statistics for CV_x:\")\n",
        "print(flattened_series_cv_x.describe())\n",
        "\n",
        "print(\"\\nSummary statistics for y_train:\")\n",
        "print(df_y_train.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TLI_Y3C-bRBy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TY8-DeP2bTUf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout,BatchNormalization, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.preprocessing import image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Introduce function to test model accuracy\n",
        "\n",
        "def get_test_accuracy(model, x_test, y_test):\n",
        "    test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "    print('accuracy: {acc:0.3f}'.format(acc=test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5cLlsBKN_aD"
      },
      "source": [
        "### PART 2: FIT TRAINING SET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "\n",
        "    w = np.zeros((dim,1))\n",
        "    b = 0\n",
        "    \n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b\n",
        "\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    A = sigmoid(np.dot(w.T,X)+b)                                    \n",
        "    cost = -1/m*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                                 \n",
        "    \n",
        "    \n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    Dw = (1/m)*np.dot(X,(A-Y).T)\n",
        "    Db = (1/m)*np.sum(A-Y)\n",
        "\n",
        "    assert(Dw.shape == w.shape)\n",
        "    assert(Db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    grads = {\"Dw\": Dw,\n",
        "             \"Db\": Db}\n",
        "    \n",
        "    return grads, cost\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        grads, cost = propagate(w,b,X,Y)\n",
        "\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        Dw = grads[\"Dw\"]\n",
        "        Db = grads[\"Db\"]\n",
        "        \n",
        "        # update rule \n",
        "        w = w - learning_rate*Dw\n",
        "        b = b - learning_rate*Db\n",
        "        \n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"Dw\": Dw,\n",
        "             \"Db\": Db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "\n",
        "\n",
        "\n",
        "def predict(w, b, X):\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
        "    A = sigmoid(np.dot(w.T,X) + b)\n",
        "        \n",
        "    for i in range(A.shape[1]):  \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        Y_prediction[0,i] = 1 if A[0,i] >0.5 else 0\n",
        "\n",
        "    \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "    \n",
        "    return Y_prediction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model(X_train, Y_train, X_CV, Y_CV, num_iterations = 2000, learning_rate = 0.050353614650626294, print_cost = False):\n",
        "    # initialize parameters with zeros \n",
        "    X_train = X_train.T\n",
        "    Y_train = Y_train.T\n",
        "    X_CV = X_CV.T\n",
        "    Y_CV = Y_CV.T\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent \n",
        "    parameters, grads, costs = optimize(w,b,X_train, Y_train,num_iterations,learning_rate,print_cost)\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples \n",
        "    X_prediction_CV = predict(w,b,X_CV)\n",
        "    Y_prediction_train = predict(w,b,X_train)\n",
        "\n",
        "    \n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"CV accuracy: {} %\".format(100 - np.mean(np.abs(X_prediction_CV - Y_CV)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": X_prediction_CV, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.068924\n",
            "Cost after iteration 200: 0.054246\n",
            "Cost after iteration 300: 0.046075\n",
            "Cost after iteration 400: 0.040285\n",
            "Cost after iteration 500: 0.035797\n",
            "Cost after iteration 600: 0.032145\n",
            "Cost after iteration 700: 0.029079\n",
            "Cost after iteration 800: 0.026452\n",
            "Cost after iteration 900: 0.024171\n",
            "Cost after iteration 1000: 0.022171\n",
            "Cost after iteration 1100: 0.020411\n",
            "Cost after iteration 1200: 0.018858\n",
            "Cost after iteration 1300: 0.017489\n",
            "Cost after iteration 1400: 0.016281\n",
            "Cost after iteration 1500: 0.015215\n",
            "Cost after iteration 1600: 0.014271\n",
            "Cost after iteration 1700: 0.013430\n",
            "Cost after iteration 1800: 0.012679\n",
            "Cost after iteration 1900: 0.012004\n",
            "train accuracy: 100.0 %\n",
            "CV accuracy: 97.80701754385964 %\n"
          ]
        }
      ],
      "source": [
        "model_logreg = model(train_set_x, y_train, cv_x, y_cv, num_iterations = 2000, learning_rate = 0.050353614650626294, print_cost = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Different Neural Networks (layers and hidden units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "6/6 - 2s - 315ms/step - binary_accuracy: 0.5183 - loss: 162.0510 - val_binary_accuracy: 0.8180 - val_loss: 3.6673\n",
            "Epoch 2/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7259 - loss: 13.2368 - val_binary_accuracy: 0.7741 - val_loss: 6.3496\n",
            "Epoch 3/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7734 - loss: 5.6644 - val_binary_accuracy: 0.8816 - val_loss: 1.5399\n",
            "Epoch 4/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8545 - loss: 1.5260 - val_binary_accuracy: 0.8816 - val_loss: 0.8610\n",
            "Epoch 5/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8582 - loss: 0.9480 - val_binary_accuracy: 0.8991 - val_loss: 0.5806\n",
            "Epoch 6/20\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9123 - loss: 0.5311 - val_binary_accuracy: 0.9320 - val_loss: 0.4024\n",
            "Epoch 7/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9327 - loss: 0.2956 - val_binary_accuracy: 0.9561 - val_loss: 0.2669\n",
            "Epoch 8/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9598 - loss: 0.1864 - val_binary_accuracy: 0.9605 - val_loss: 0.2417\n",
            "Epoch 9/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9635 - loss: 0.1352 - val_binary_accuracy: 0.9605 - val_loss: 0.2042\n",
            "Epoch 10/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9664 - loss: 0.1132 - val_binary_accuracy: 0.9539 - val_loss: 0.2036\n",
            "Epoch 11/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9744 - loss: 0.0925 - val_binary_accuracy: 0.9649 - val_loss: 0.1619\n",
            "Epoch 12/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9803 - loss: 0.0804 - val_binary_accuracy: 0.9649 - val_loss: 0.1581\n",
            "Epoch 13/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9781 - loss: 0.0766 - val_binary_accuracy: 0.9649 - val_loss: 0.1506\n",
            "Epoch 14/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9795 - loss: 0.0681 - val_binary_accuracy: 0.9649 - val_loss: 0.1493\n",
            "Epoch 15/20\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9788 - loss: 0.0653 - val_binary_accuracy: 0.9605 - val_loss: 0.1512\n",
            "Epoch 16/20\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9817 - loss: 0.0608 - val_binary_accuracy: 0.9649 - val_loss: 0.1483\n",
            "Epoch 17/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9817 - loss: 0.0586 - val_binary_accuracy: 0.9649 - val_loss: 0.1418\n",
            "Epoch 18/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9846 - loss: 0.0534 - val_binary_accuracy: 0.9583 - val_loss: 0.1541\n",
            "Epoch 19/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9861 - loss: 0.0564 - val_binary_accuracy: 0.9583 - val_loss: 0.1549\n",
            "Epoch 20/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9839 - loss: 0.0581 - val_binary_accuracy: 0.9539 - val_loss: 0.1770\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.518275</td>\n",
              "      <td>162.050995</td>\n",
              "      <td>0.817982</td>\n",
              "      <td>3.667337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.725877</td>\n",
              "      <td>13.236787</td>\n",
              "      <td>0.774123</td>\n",
              "      <td>6.349623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.773392</td>\n",
              "      <td>5.664357</td>\n",
              "      <td>0.881579</td>\n",
              "      <td>1.539901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.854532</td>\n",
              "      <td>1.526041</td>\n",
              "      <td>0.881579</td>\n",
              "      <td>0.861024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.858187</td>\n",
              "      <td>0.947992</td>\n",
              "      <td>0.899123</td>\n",
              "      <td>0.580574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.531051</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.402421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.932749</td>\n",
              "      <td>0.295562</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.266925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.959795</td>\n",
              "      <td>0.186362</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.241685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>0.135190</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.204227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>0.113161</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.203563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.092508</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.161944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.080396</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.158063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.076608</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.150639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.068054</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.149278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>0.065295</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.151226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.060817</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.148328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.058552</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.141827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.053394</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.154071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.056428</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.154874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.058141</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.176960</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy  val_loss\n",
              "0          0.518275  162.050995             0.817982  3.667337\n",
              "1          0.725877   13.236787             0.774123  6.349623\n",
              "2          0.773392    5.664357             0.881579  1.539901\n",
              "3          0.854532    1.526041             0.881579  0.861024\n",
              "4          0.858187    0.947992             0.899123  0.580574\n",
              "5          0.912281    0.531051             0.932018  0.402421\n",
              "6          0.932749    0.295562             0.956140  0.266925\n",
              "7          0.959795    0.186362             0.960526  0.241685\n",
              "8          0.963450    0.135190             0.960526  0.204227\n",
              "9          0.966374    0.113161             0.953947  0.203563\n",
              "10         0.974415    0.092508             0.964912  0.161944\n",
              "11         0.980263    0.080396             0.964912  0.158063\n",
              "12         0.978070    0.076608             0.964912  0.150639\n",
              "13         0.979532    0.068054             0.964912  0.149278\n",
              "14         0.978801    0.065295             0.960526  0.151226\n",
              "15         0.981725    0.060817             0.964912  0.148328\n",
              "16         0.981725    0.058552             0.964912  0.141827\n",
              "17         0.984649    0.053394             0.958333  0.154071\n",
              "18         0.986111    0.056428             0.958333  0.154874\n",
              "19         0.983918    0.058141             0.953947  0.176960"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_nn1 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "Dense(64, activation='relu'),  # Additional hidden layer\n",
        "Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_nn1.compile(\n",
        "optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "loss = 'binary_crossentropy',\n",
        "metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_nn1 = model_nn1.fit(train_set_x, y_train, epochs = 20, batch_size = 256, verbose = 2, validation_data=(cv_x, y_cv))\n",
        "df_nn1 = pd.DataFrame(history_nn1.history)\n",
        "df_nn1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 283ms/step - binary_accuracy: 0.5095 - loss: 134.4615 - val_binary_accuracy: 0.5548 - val_loss: 57.6481\n",
            "Epoch 2/20\n",
            "6/6 - 0s - 34ms/step - binary_accuracy: 0.6067 - loss: 32.3233 - val_binary_accuracy: 0.8333 - val_loss: 5.6928\n",
            "Epoch 3/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7251 - loss: 13.0051 - val_binary_accuracy: 0.7807 - val_loss: 8.5319\n",
            "Epoch 4/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8260 - loss: 4.4196 - val_binary_accuracy: 0.8860 - val_loss: 2.2096\n",
            "Epoch 5/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8443 - loss: 2.6971 - val_binary_accuracy: 0.8882 - val_loss: 0.5629\n",
            "Epoch 6/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.8107 - loss: 1.2471 - val_binary_accuracy: 0.9232 - val_loss: 0.8230\n",
            "Epoch 7/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9291 - loss: 0.8309 - val_binary_accuracy: 0.9298 - val_loss: 0.8978\n",
            "Epoch 8/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9284 - loss: 0.7771 - val_binary_accuracy: 0.9386 - val_loss: 0.6871\n",
            "Epoch 9/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9408 - loss: 0.5245 - val_binary_accuracy: 0.9408 - val_loss: 0.4731\n",
            "Epoch 10/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9466 - loss: 0.3352 - val_binary_accuracy: 0.9474 - val_loss: 0.3713\n",
            "Epoch 11/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9569 - loss: 0.2293 - val_binary_accuracy: 0.9408 - val_loss: 0.3025\n",
            "Epoch 12/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9583 - loss: 0.1819 - val_binary_accuracy: 0.9276 - val_loss: 0.2761\n",
            "Epoch 13/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9503 - loss: 0.1717 - val_binary_accuracy: 0.9276 - val_loss: 0.2642\n",
            "Epoch 14/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9532 - loss: 0.1630 - val_binary_accuracy: 0.9298 - val_loss: 0.2503\n",
            "Epoch 15/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9525 - loss: 0.1574 - val_binary_accuracy: 0.9364 - val_loss: 0.2434\n",
            "Epoch 16/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9576 - loss: 0.1422 - val_binary_accuracy: 0.9342 - val_loss: 0.2256\n",
            "Epoch 17/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9635 - loss: 0.1335 - val_binary_accuracy: 0.9452 - val_loss: 0.2178\n",
            "Epoch 18/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9642 - loss: 0.1259 - val_binary_accuracy: 0.9430 - val_loss: 0.2088\n",
            "Epoch 19/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9649 - loss: 0.1216 - val_binary_accuracy: 0.9452 - val_loss: 0.1986\n",
            "Epoch 20/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9649 - loss: 0.1150 - val_binary_accuracy: 0.9430 - val_loss: 0.1942\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.509503</td>\n",
              "      <td>134.461533</td>\n",
              "      <td>0.554825</td>\n",
              "      <td>57.648121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.606725</td>\n",
              "      <td>32.323288</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>5.692783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.725146</td>\n",
              "      <td>13.005112</td>\n",
              "      <td>0.780702</td>\n",
              "      <td>8.531855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.826023</td>\n",
              "      <td>4.419582</td>\n",
              "      <td>0.885965</td>\n",
              "      <td>2.209615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.844298</td>\n",
              "      <td>2.697142</td>\n",
              "      <td>0.888158</td>\n",
              "      <td>0.562896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.810673</td>\n",
              "      <td>1.247120</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>0.823025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.929094</td>\n",
              "      <td>0.830884</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>0.897831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.928363</td>\n",
              "      <td>0.777078</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.687073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.524478</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.473078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.946637</td>\n",
              "      <td>0.335225</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.371266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.956871</td>\n",
              "      <td>0.229305</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.302505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.181918</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.276082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.950292</td>\n",
              "      <td>0.171680</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.264206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.953216</td>\n",
              "      <td>0.162999</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>0.250324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.952485</td>\n",
              "      <td>0.157401</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.243417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.957602</td>\n",
              "      <td>0.142174</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.225630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>0.133539</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.217758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>0.125924</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.208751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.121646</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.198572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.114955</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.194162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.509503  134.461533             0.554825  57.648121\n",
              "1          0.606725   32.323288             0.833333   5.692783\n",
              "2          0.725146   13.005112             0.780702   8.531855\n",
              "3          0.826023    4.419582             0.885965   2.209615\n",
              "4          0.844298    2.697142             0.888158   0.562896\n",
              "5          0.810673    1.247120             0.923246   0.823025\n",
              "6          0.929094    0.830884             0.929825   0.897831\n",
              "7          0.928363    0.777078             0.938596   0.687073\n",
              "8          0.940789    0.524478             0.940789   0.473078\n",
              "9          0.946637    0.335225             0.947368   0.371266\n",
              "10         0.956871    0.229305             0.940789   0.302505\n",
              "11         0.958333    0.181918             0.927632   0.276082\n",
              "12         0.950292    0.171680             0.927632   0.264206\n",
              "13         0.953216    0.162999             0.929825   0.250324\n",
              "14         0.952485    0.157401             0.936404   0.243417\n",
              "15         0.957602    0.142174             0.934211   0.225630\n",
              "16         0.963450    0.133539             0.945175   0.217758\n",
              "17         0.964181    0.125924             0.942982   0.208751\n",
              "18         0.964912    0.121646             0.945175   0.198572\n",
              "19         0.964912    0.114955             0.942982   0.194162"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_nn2 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "Dense(48, activation='relu'),  # Additional hidden layer\n",
        "Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_nn2.compile(\n",
        "optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "loss = 'binary_crossentropy',\n",
        "metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_nn2 = model_nn2.fit(train_set_x, y_train, epochs = 20, batch_size = 256, verbose = 2, validation_data=(cv_x, y_cv))\n",
        "df_nn2 = pd.DataFrame(history_nn2.history)\n",
        "df_nn2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history_nn2.history['loss'])\n",
        "plt.plot(history_nn2.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.963\n"
          ]
        }
      ],
      "source": [
        "get_test_accuracy(model_nn2, test_set_x.T, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 1s - 232ms/step - binary_accuracy: 0.5585 - loss: 37.2495 - val_binary_accuracy: 0.8004 - val_loss: 8.3485\n",
            "Epoch 2/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.8268 - loss: 6.5905 - val_binary_accuracy: 0.8947 - val_loss: 3.9904\n",
            "Epoch 3/20\n",
            "6/6 - 0s - 18ms/step - binary_accuracy: 0.9218 - loss: 3.0789 - val_binary_accuracy: 0.9364 - val_loss: 2.1987\n",
            "Epoch 4/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9364 - loss: 1.9202 - val_binary_accuracy: 0.9276 - val_loss: 2.4920\n",
            "Epoch 5/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9423 - loss: 1.7288 - val_binary_accuracy: 0.9408 - val_loss: 2.0331\n",
            "Epoch 6/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9547 - loss: 1.4778 - val_binary_accuracy: 0.9364 - val_loss: 1.9971\n",
            "Epoch 7/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9547 - loss: 1.3956 - val_binary_accuracy: 0.9430 - val_loss: 1.8524\n",
            "Epoch 8/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9576 - loss: 1.1771 - val_binary_accuracy: 0.9452 - val_loss: 1.7362\n",
            "Epoch 9/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9598 - loss: 1.0453 - val_binary_accuracy: 0.9430 - val_loss: 1.5953\n",
            "Epoch 10/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9649 - loss: 0.9310 - val_binary_accuracy: 0.9474 - val_loss: 1.5997\n",
            "Epoch 11/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9627 - loss: 0.8388 - val_binary_accuracy: 0.9518 - val_loss: 1.4654\n",
            "Epoch 12/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9678 - loss: 0.7633 - val_binary_accuracy: 0.9518 - val_loss: 1.3622\n",
            "Epoch 13/20\n",
            "6/6 - 0s - 18ms/step - binary_accuracy: 0.9671 - loss: 0.6728 - val_binary_accuracy: 0.9474 - val_loss: 1.3730\n",
            "Epoch 14/20\n",
            "6/6 - 0s - 18ms/step - binary_accuracy: 0.9656 - loss: 0.6539 - val_binary_accuracy: 0.9474 - val_loss: 1.3127\n",
            "Epoch 15/20\n",
            "6/6 - 0s - 18ms/step - binary_accuracy: 0.9693 - loss: 0.5475 - val_binary_accuracy: 0.9539 - val_loss: 1.2537\n",
            "Epoch 16/20\n",
            "6/6 - 0s - 17ms/step - binary_accuracy: 0.9708 - loss: 0.5224 - val_binary_accuracy: 0.9496 - val_loss: 1.2191\n",
            "Epoch 17/20\n",
            "6/6 - 0s - 18ms/step - binary_accuracy: 0.9730 - loss: 0.5009 - val_binary_accuracy: 0.9518 - val_loss: 1.1979\n",
            "Epoch 18/20\n",
            "6/6 - 0s - 17ms/step - binary_accuracy: 0.9715 - loss: 0.4804 - val_binary_accuracy: 0.9561 - val_loss: 1.1839\n",
            "Epoch 19/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9751 - loss: 0.4155 - val_binary_accuracy: 0.9496 - val_loss: 1.1822\n",
            "Epoch 20/20\n",
            "6/6 - 0s - 18ms/step - binary_accuracy: 0.9737 - loss: 0.4650 - val_binary_accuracy: 0.9518 - val_loss: 1.1307\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.558480</td>\n",
              "      <td>37.249550</td>\n",
              "      <td>0.800439</td>\n",
              "      <td>8.348518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.826754</td>\n",
              "      <td>6.590453</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>3.990359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.921784</td>\n",
              "      <td>3.078893</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>2.198684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.936404</td>\n",
              "      <td>1.920184</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>2.491972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.942251</td>\n",
              "      <td>1.728758</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>2.033085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>1.477771</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>1.997052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>1.395644</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>1.852430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.957602</td>\n",
              "      <td>1.177128</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.736157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.959795</td>\n",
              "      <td>1.045317</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>1.595263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.930951</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.599715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.838809</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.465383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.967836</td>\n",
              "      <td>0.763253</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.362211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.672810</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.372997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.965643</td>\n",
              "      <td>0.653880</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.312729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.547453</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.253710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>0.522366</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.219054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.500906</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.197938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.480385</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>1.183931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.415481</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.182230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.465037</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.130750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy       loss  val_binary_accuracy  val_loss\n",
              "0          0.558480  37.249550             0.800439  8.348518\n",
              "1          0.826754   6.590453             0.894737  3.990359\n",
              "2          0.921784   3.078893             0.936404  2.198684\n",
              "3          0.936404   1.920184             0.927632  2.491972\n",
              "4          0.942251   1.728758             0.940789  2.033085\n",
              "5          0.954678   1.477771             0.936404  1.997052\n",
              "6          0.954678   1.395644             0.942982  1.852430\n",
              "7          0.957602   1.177128             0.945175  1.736157\n",
              "8          0.959795   1.045317             0.942982  1.595263\n",
              "9          0.964912   0.930951             0.947368  1.599715\n",
              "10         0.962719   0.838809             0.951754  1.465383\n",
              "11         0.967836   0.763253             0.951754  1.362211\n",
              "12         0.967105   0.672810             0.947368  1.372997\n",
              "13         0.965643   0.653880             0.947368  1.312729\n",
              "14         0.969298   0.547453             0.953947  1.253710\n",
              "15         0.970760   0.522366             0.949561  1.219054\n",
              "16         0.972953   0.500906             0.951754  1.197938\n",
              "17         0.971491   0.480385             0.956140  1.183931\n",
              "18         0.975146   0.415481             0.949561  1.182230\n",
              "19         0.973684   0.465037             0.951754  1.130750"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_nn3 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "Dense(24, activation = 'relu', input_shape = (12288,)),\n",
        "Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_nn3.compile(\n",
        "optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "loss = 'binary_crossentropy',\n",
        "metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_nn3 = model_nn3.fit(train_set_x, y_train, epochs = 20, batch_size = 256, verbose = 2, validation_data=(cv_x, y_cv))\n",
        "df_nn3 = pd.DataFrame(history_nn3.history)\n",
        "df_nn3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 1s - 249ms/step - binary_accuracy: 0.5512 - loss: 107.2187 - val_binary_accuracy: 0.7807 - val_loss: 17.1692\n",
            "Epoch 2/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.6879 - loss: 41.0947 - val_binary_accuracy: 0.8662 - val_loss: 8.9181\n",
            "Epoch 3/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.8480 - loss: 13.5978 - val_binary_accuracy: 0.9035 - val_loss: 4.7786\n",
            "Epoch 4/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.8940 - loss: 5.3095 - val_binary_accuracy: 0.8860 - val_loss: 4.5098\n",
            "Epoch 5/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9101 - loss: 3.3584 - val_binary_accuracy: 0.9364 - val_loss: 2.5982\n",
            "Epoch 6/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9437 - loss: 2.2295 - val_binary_accuracy: 0.9364 - val_loss: 2.4401\n",
            "Epoch 7/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9547 - loss: 1.6933 - val_binary_accuracy: 0.9452 - val_loss: 1.9344\n",
            "Epoch 8/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9437 - loss: 1.4618 - val_binary_accuracy: 0.9364 - val_loss: 1.8906\n",
            "Epoch 9/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9539 - loss: 1.1842 - val_binary_accuracy: 0.9518 - val_loss: 1.4559\n",
            "Epoch 10/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9583 - loss: 1.0136 - val_binary_accuracy: 0.9452 - val_loss: 1.2849\n",
            "Epoch 11/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9576 - loss: 0.8493 - val_binary_accuracy: 0.9605 - val_loss: 1.1218\n",
            "Epoch 12/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9627 - loss: 0.6844 - val_binary_accuracy: 0.9539 - val_loss: 1.0163\n",
            "Epoch 13/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9635 - loss: 0.5700 - val_binary_accuracy: 0.9474 - val_loss: 0.8877\n",
            "Epoch 14/20\n",
            "6/6 - 0s - 18ms/step - binary_accuracy: 0.9642 - loss: 0.4640 - val_binary_accuracy: 0.9474 - val_loss: 0.8728\n",
            "Epoch 15/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9664 - loss: 0.3907 - val_binary_accuracy: 0.9518 - val_loss: 0.7271\n",
            "Epoch 16/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9678 - loss: 0.3423 - val_binary_accuracy: 0.9474 - val_loss: 0.6730\n",
            "Epoch 17/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9693 - loss: 0.2965 - val_binary_accuracy: 0.9474 - val_loss: 0.6419\n",
            "Epoch 18/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9700 - loss: 0.2476 - val_binary_accuracy: 0.9474 - val_loss: 0.5949\n",
            "Epoch 19/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9708 - loss: 0.2174 - val_binary_accuracy: 0.9539 - val_loss: 0.5548\n",
            "Epoch 20/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9700 - loss: 0.2038 - val_binary_accuracy: 0.9518 - val_loss: 0.5206\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.551170</td>\n",
              "      <td>107.218704</td>\n",
              "      <td>0.780702</td>\n",
              "      <td>17.169197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.687865</td>\n",
              "      <td>41.094711</td>\n",
              "      <td>0.866228</td>\n",
              "      <td>8.918114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.847953</td>\n",
              "      <td>13.597800</td>\n",
              "      <td>0.903509</td>\n",
              "      <td>4.778612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.894006</td>\n",
              "      <td>5.309497</td>\n",
              "      <td>0.885965</td>\n",
              "      <td>4.509770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.910088</td>\n",
              "      <td>3.358371</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>2.598178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.943713</td>\n",
              "      <td>2.229478</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>2.440068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>1.693305</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.934409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.943713</td>\n",
              "      <td>1.461778</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>1.890583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.184172</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.455929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.013623</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.284928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.957602</td>\n",
              "      <td>0.849272</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>1.121842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.684440</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.016343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>0.569982</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.887684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>0.464006</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.872770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>0.390718</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.727135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.967836</td>\n",
              "      <td>0.342254</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.672993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.296535</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.641861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.247565</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.594938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>0.217432</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.554776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.203796</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.520587</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.551170  107.218704             0.780702  17.169197\n",
              "1          0.687865   41.094711             0.866228   8.918114\n",
              "2          0.847953   13.597800             0.903509   4.778612\n",
              "3          0.894006    5.309497             0.885965   4.509770\n",
              "4          0.910088    3.358371             0.936404   2.598178\n",
              "5          0.943713    2.229478             0.936404   2.440068\n",
              "6          0.954678    1.693305             0.945175   1.934409\n",
              "7          0.943713    1.461778             0.936404   1.890583\n",
              "8          0.953947    1.184172             0.951754   1.455929\n",
              "9          0.958333    1.013623             0.945175   1.284928\n",
              "10         0.957602    0.849272             0.960526   1.121842\n",
              "11         0.962719    0.684440             0.953947   1.016343\n",
              "12         0.963450    0.569982             0.947368   0.887684\n",
              "13         0.964181    0.464006             0.947368   0.872770\n",
              "14         0.966374    0.390718             0.951754   0.727135\n",
              "15         0.967836    0.342254             0.947368   0.672993\n",
              "16         0.969298    0.296535             0.947368   0.641861\n",
              "17         0.970029    0.247565             0.947368   0.594938\n",
              "18         0.970760    0.217432             0.953947   0.554776\n",
              "19         0.970029    0.203796             0.951754   0.520587"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_nn4 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "Dense(48, activation = 'relu', input_shape = (12288,)),\n",
        "Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_nn4.compile(\n",
        "optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "loss = 'binary_crossentropy',\n",
        "metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_nn4 = model_nn4.fit(train_set_x, y_train, epochs = 20, batch_size = 256, verbose = 2, validation_data=(cv_x, y_cv))\n",
        "df_nn4 = pd.DataFrame(history_nn4.history)\n",
        "df_nn4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history_nn4.history['loss'])\n",
        "plt.plot(history_nn4.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04IP2C8KMp2x"
      },
      "source": [
        "#### 3 DIFFERENT OPTIMIZATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OQo0_rOXMuSE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# MODEL 1\n",
        "# building a NN model\n",
        "# the input layer with 16 units\n",
        "# one hidden layer, with 64 units, with a relu activation function.\n",
        "# the output layer is binary with a sigmoid activation function.\n",
        "model = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "Dense(48, activation='relu'),  # Additional hidden layer\n",
        "Dense(1, activation = 'sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XsVUnzbRMyJD"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">786,496</span> \n",
              "\n",
              " dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,120</span> \n",
              "\n",
              " dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " dense_10 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                    \u001b[38;5;34m786,496\u001b[0m \n",
              "\n",
              " dense_11 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)                      \u001b[38;5;34m3,120\u001b[0m \n",
              "\n",
              " dense_12 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m49\u001b[0m \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">789,665</span> (3.01 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m789,665\u001b[0m (3.01 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">789,665</span> (3.01 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m789,665\u001b[0m (3.01 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Z7lsXFbSM0zO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "6/6 - 2s - 283ms/step - binary_accuracy: 0.5541 - loss: 135.3885 - val_binary_accuracy: 0.5548 - val_loss: 38.5826\n",
            "Epoch 2/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.6798 - loss: 16.2143 - val_binary_accuracy: 0.8180 - val_loss: 4.5210\n",
            "Epoch 3/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.8523 - loss: 2.8535 - val_binary_accuracy: 0.8224 - val_loss: 3.3323\n",
            "Epoch 4/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.8348 - loss: 2.3277 - val_binary_accuracy: 0.9057 - val_loss: 0.5653\n",
            "Epoch 5/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.8713 - loss: 0.6416 - val_binary_accuracy: 0.9254 - val_loss: 0.4513\n",
            "Epoch 6/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9335 - loss: 0.3721 - val_binary_accuracy: 0.9232 - val_loss: 0.3616\n",
            "Epoch 7/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9408 - loss: 0.2122 - val_binary_accuracy: 0.9057 - val_loss: 0.3012\n",
            "Epoch 8/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9225 - loss: 0.1989 - val_binary_accuracy: 0.9298 - val_loss: 0.2503\n",
            "Epoch 9/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9474 - loss: 0.1749 - val_binary_accuracy: 0.9320 - val_loss: 0.2415\n",
            "Epoch 10/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9518 - loss: 0.1682 - val_binary_accuracy: 0.9364 - val_loss: 0.2284\n",
            "Epoch 11/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9503 - loss: 0.1555 - val_binary_accuracy: 0.9276 - val_loss: 0.2217\n",
            "Epoch 12/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9496 - loss: 0.1537 - val_binary_accuracy: 0.9364 - val_loss: 0.2153\n",
            "Epoch 13/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9525 - loss: 0.1475 - val_binary_accuracy: 0.9364 - val_loss: 0.2052\n",
            "Epoch 14/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9525 - loss: 0.1433 - val_binary_accuracy: 0.9408 - val_loss: 0.1938\n",
            "Epoch 15/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9554 - loss: 0.1378 - val_binary_accuracy: 0.9408 - val_loss: 0.1833\n",
            "Epoch 16/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9576 - loss: 0.1309 - val_binary_accuracy: 0.9386 - val_loss: 0.1758\n",
            "Epoch 17/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9576 - loss: 0.1233 - val_binary_accuracy: 0.9386 - val_loss: 0.1651\n",
            "Epoch 18/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9539 - loss: 0.1295 - val_binary_accuracy: 0.9364 - val_loss: 0.1781\n",
            "Epoch 19/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9561 - loss: 0.1314 - val_binary_accuracy: 0.9254 - val_loss: 0.1796\n",
            "Epoch 20/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9561 - loss: 0.1201 - val_binary_accuracy: 0.9386 - val_loss: 0.1765\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.554094</td>\n",
              "      <td>135.388474</td>\n",
              "      <td>0.554825</td>\n",
              "      <td>38.582588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.679825</td>\n",
              "      <td>16.214342</td>\n",
              "      <td>0.817982</td>\n",
              "      <td>4.520988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.852339</td>\n",
              "      <td>2.853477</td>\n",
              "      <td>0.822368</td>\n",
              "      <td>3.332314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.834795</td>\n",
              "      <td>2.327736</td>\n",
              "      <td>0.905702</td>\n",
              "      <td>0.565252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.871345</td>\n",
              "      <td>0.641629</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>0.451268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.933480</td>\n",
              "      <td>0.372114</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>0.361589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.212224</td>\n",
              "      <td>0.905702</td>\n",
              "      <td>0.301220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.922515</td>\n",
              "      <td>0.198925</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>0.250290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.174895</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.241507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.168186</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.228428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.950292</td>\n",
              "      <td>0.155481</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.221653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.153709</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.215307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.952485</td>\n",
              "      <td>0.147507</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.205223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.952485</td>\n",
              "      <td>0.143262</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.193824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.955409</td>\n",
              "      <td>0.137805</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.183281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.957602</td>\n",
              "      <td>0.130882</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.175826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.957602</td>\n",
              "      <td>0.123259</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.165083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.129469</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.178112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.131369</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>0.179649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.120122</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.176459</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.554094  135.388474             0.554825  38.582588\n",
              "1          0.679825   16.214342             0.817982   4.520988\n",
              "2          0.852339    2.853477             0.822368   3.332314\n",
              "3          0.834795    2.327736             0.905702   0.565252\n",
              "4          0.871345    0.641629             0.925439   0.451268\n",
              "5          0.933480    0.372114             0.923246   0.361589\n",
              "6          0.940789    0.212224             0.905702   0.301220\n",
              "7          0.922515    0.198925             0.929825   0.250290\n",
              "8          0.947368    0.174895             0.932018   0.241507\n",
              "9          0.951754    0.168186             0.936404   0.228428\n",
              "10         0.950292    0.155481             0.927632   0.221653\n",
              "11         0.949561    0.153709             0.936404   0.215307\n",
              "12         0.952485    0.147507             0.936404   0.205223\n",
              "13         0.952485    0.143262             0.940789   0.193824\n",
              "14         0.955409    0.137805             0.940789   0.183281\n",
              "15         0.957602    0.130882             0.938596   0.175826\n",
              "16         0.957602    0.123259             0.938596   0.165083\n",
              "17         0.953947    0.129469             0.936404   0.178112\n",
              "18         0.956140    0.131369             0.925439   0.179649\n",
              "19         0.956140    0.120122             0.938596   0.176459"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(\n",
        "optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "loss = 'binary_crossentropy',\n",
        "metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history = model.fit(train_set_x, y_train, epochs = 20, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "df = pd.DataFrame(history.history)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "L2wClxfOM8kN"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.967\n"
          ]
        }
      ],
      "source": [
        "get_test_accuracy(model, test_set_x.T, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0f33RnVgNBBW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 326ms/step - binary_accuracy: 0.4898 - loss: 308.5745 - val_binary_accuracy: 0.4715 - val_loss: 97.9607\n",
            "Epoch 2/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5899 - loss: 37.6373 - val_binary_accuracy: 0.7281 - val_loss: 10.2539\n",
            "Epoch 3/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7829 - loss: 5.4305 - val_binary_accuracy: 0.8596 - val_loss: 1.3210\n",
            "Epoch 4/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8231 - loss: 1.4787 - val_binary_accuracy: 0.8618 - val_loss: 1.3325\n",
            "Epoch 5/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8553 - loss: 1.2331 - val_binary_accuracy: 0.9211 - val_loss: 0.4531\n",
            "Epoch 6/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8918 - loss: 0.5237 - val_binary_accuracy: 0.8969 - val_loss: 0.3705\n",
            "Epoch 7/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9203 - loss: 0.2681 - val_binary_accuracy: 0.9211 - val_loss: 0.2683\n",
            "Epoch 8/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9276 - loss: 0.2411 - val_binary_accuracy: 0.9254 - val_loss: 0.2395\n",
            "Epoch 9/20\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9313 - loss: 0.2032 - val_binary_accuracy: 0.9167 - val_loss: 0.2334\n",
            "Epoch 10/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9327 - loss: 0.1911 - val_binary_accuracy: 0.9320 - val_loss: 0.2097\n",
            "Epoch 11/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9357 - loss: 0.1757 - val_binary_accuracy: 0.9342 - val_loss: 0.2044\n",
            "Epoch 12/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9386 - loss: 0.1633 - val_binary_accuracy: 0.9232 - val_loss: 0.1972\n",
            "Epoch 13/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9452 - loss: 0.1566 - val_binary_accuracy: 0.9342 - val_loss: 0.1888\n",
            "Epoch 14/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9437 - loss: 0.1497 - val_binary_accuracy: 0.9342 - val_loss: 0.1802\n",
            "Epoch 15/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9518 - loss: 0.1410 - val_binary_accuracy: 0.9342 - val_loss: 0.1728\n",
            "Epoch 16/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9539 - loss: 0.1361 - val_binary_accuracy: 0.9430 - val_loss: 0.1698\n",
            "Epoch 17/20\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9554 - loss: 0.1332 - val_binary_accuracy: 0.9364 - val_loss: 0.1671\n",
            "Epoch 18/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9554 - loss: 0.1315 - val_binary_accuracy: 0.9408 - val_loss: 0.1663\n",
            "Epoch 19/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9539 - loss: 0.1309 - val_binary_accuracy: 0.9386 - val_loss: 0.1634\n",
            "Epoch 20/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9569 - loss: 0.1286 - val_binary_accuracy: 0.9408 - val_loss: 0.1685\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.489766</td>\n",
              "      <td>308.574493</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>97.960670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.589912</td>\n",
              "      <td>37.637329</td>\n",
              "      <td>0.728070</td>\n",
              "      <td>10.253917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.782895</td>\n",
              "      <td>5.430455</td>\n",
              "      <td>0.859649</td>\n",
              "      <td>1.320985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.823099</td>\n",
              "      <td>1.478657</td>\n",
              "      <td>0.861842</td>\n",
              "      <td>1.332543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.855263</td>\n",
              "      <td>1.233060</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.453142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.891813</td>\n",
              "      <td>0.523722</td>\n",
              "      <td>0.896930</td>\n",
              "      <td>0.370518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.920322</td>\n",
              "      <td>0.268093</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.268284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.241067</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>0.239457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.931287</td>\n",
              "      <td>0.203179</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.233434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.932749</td>\n",
              "      <td>0.191055</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.209722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.935673</td>\n",
              "      <td>0.175697</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.204358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.163293</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>0.197250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.156650</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.188819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.943713</td>\n",
              "      <td>0.149734</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.180193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.141021</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.172779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.136091</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.169843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.955409</td>\n",
              "      <td>0.133244</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.167107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.955409</td>\n",
              "      <td>0.131547</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.166340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.130873</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.163406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.956871</td>\n",
              "      <td>0.128562</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.168539</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.489766  308.574493             0.471491  97.960670\n",
              "1          0.589912   37.637329             0.728070  10.253917\n",
              "2          0.782895    5.430455             0.859649   1.320985\n",
              "3          0.823099    1.478657             0.861842   1.332543\n",
              "4          0.855263    1.233060             0.921053   0.453142\n",
              "5          0.891813    0.523722             0.896930   0.370518\n",
              "6          0.920322    0.268093             0.921053   0.268284\n",
              "7          0.927632    0.241067             0.925439   0.239457\n",
              "8          0.931287    0.203179             0.916667   0.233434\n",
              "9          0.932749    0.191055             0.932018   0.209722\n",
              "10         0.935673    0.175697             0.934211   0.204358\n",
              "11         0.938596    0.163293             0.923246   0.197250\n",
              "12         0.945175    0.156650             0.934211   0.188819\n",
              "13         0.943713    0.149734             0.934211   0.180193\n",
              "14         0.951754    0.141021             0.934211   0.172779\n",
              "15         0.953947    0.136091             0.942982   0.169843\n",
              "16         0.955409    0.133244             0.936404   0.167107\n",
              "17         0.955409    0.131547             0.940789   0.166340\n",
              "18         0.953947    0.130873             0.938596   0.163406\n",
              "19         0.956871    0.128562             0.940789   0.168539"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model 2\n",
        "model2 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "Dense(48, activation='relu'),  # Additional hidden layer\n",
        "Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model2.compile(\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.050353614650626294),\n",
        "loss = 'binary_crossentropy',\n",
        "metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history2 = model2.fit(train_set_x, y_train, epochs = 20, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df2 = pd.DataFrame(history2.history)\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "akiAuWLINTKI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 1s - 217ms/step - binary_accuracy: 0.5431 - loss: 1.9707 - val_binary_accuracy: 0.8333 - val_loss: 0.4682\n",
            "Epoch 2/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.6067 - loss: 0.9201 - val_binary_accuracy: 0.5395 - val_loss: 0.5830\n",
            "Epoch 3/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.5950 - loss: 0.5583 - val_binary_accuracy: 0.5548 - val_loss: 0.5998\n",
            "Epoch 4/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.7858 - loss: 0.4370 - val_binary_accuracy: 0.5395 - val_loss: 0.8682\n",
            "Epoch 5/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.7434 - loss: 0.5100 - val_binary_accuracy: 0.8092 - val_loss: 0.3358\n",
            "Epoch 6/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9035 - loss: 0.2312 - val_binary_accuracy: 0.9408 - val_loss: 0.1818\n",
            "Epoch 7/20\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9532 - loss: 0.1629 - val_binary_accuracy: 0.9474 - val_loss: 0.1599\n",
            "Epoch 8/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9320 - loss: 0.1929 - val_binary_accuracy: 0.9452 - val_loss: 0.1624\n",
            "Epoch 9/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9620 - loss: 0.1180 - val_binary_accuracy: 0.9496 - val_loss: 0.1413\n",
            "Epoch 10/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9627 - loss: 0.1148 - val_binary_accuracy: 0.9518 - val_loss: 0.1375\n",
            "Epoch 11/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9686 - loss: 0.0938 - val_binary_accuracy: 0.9518 - val_loss: 0.1376\n",
            "Epoch 12/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9693 - loss: 0.0895 - val_binary_accuracy: 0.9386 - val_loss: 0.1856\n",
            "Epoch 13/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9408 - loss: 0.1747 - val_binary_accuracy: 0.9539 - val_loss: 0.1373\n",
            "Epoch 14/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9722 - loss: 0.0853 - val_binary_accuracy: 0.9518 - val_loss: 0.1581\n",
            "Epoch 15/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9700 - loss: 0.0808 - val_binary_accuracy: 0.9561 - val_loss: 0.1307\n",
            "Epoch 16/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9730 - loss: 0.0761 - val_binary_accuracy: 0.9518 - val_loss: 0.1629\n",
            "Epoch 17/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9759 - loss: 0.0766 - val_binary_accuracy: 0.9605 - val_loss: 0.1246\n",
            "Epoch 18/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9773 - loss: 0.0680 - val_binary_accuracy: 0.9561 - val_loss: 0.1257\n",
            "Epoch 19/20\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9795 - loss: 0.0653 - val_binary_accuracy: 0.9605 - val_loss: 0.1247\n",
            "Epoch 20/20\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9803 - loss: 0.0708 - val_binary_accuracy: 0.9539 - val_loss: 0.1420\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.543129</td>\n",
              "      <td>1.970683</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.468220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.606725</td>\n",
              "      <td>0.920102</td>\n",
              "      <td>0.539474</td>\n",
              "      <td>0.583040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.595029</td>\n",
              "      <td>0.558267</td>\n",
              "      <td>0.554825</td>\n",
              "      <td>0.599787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.785819</td>\n",
              "      <td>0.436998</td>\n",
              "      <td>0.539474</td>\n",
              "      <td>0.868196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.743421</td>\n",
              "      <td>0.509983</td>\n",
              "      <td>0.809211</td>\n",
              "      <td>0.335792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.903509</td>\n",
              "      <td>0.231213</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.181833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.953216</td>\n",
              "      <td>0.162881</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.159933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.192932</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.162444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>0.117972</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.141340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.114758</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.137458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>0.093800</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.137616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.089501</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.185584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.174662</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.137315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.085265</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.158054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.080849</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.130660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.076113</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.162924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.076562</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.124638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>0.067954</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.125670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.065331</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.124726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.070841</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.142017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.543129  1.970683             0.833333  0.468220\n",
              "1          0.606725  0.920102             0.539474  0.583040\n",
              "2          0.595029  0.558267             0.554825  0.599787\n",
              "3          0.785819  0.436998             0.539474  0.868196\n",
              "4          0.743421  0.509983             0.809211  0.335792\n",
              "5          0.903509  0.231213             0.940789  0.181833\n",
              "6          0.953216  0.162881             0.947368  0.159933\n",
              "7          0.932018  0.192932             0.945175  0.162444\n",
              "8          0.961988  0.117972             0.949561  0.141340\n",
              "9          0.962719  0.114758             0.951754  0.137458\n",
              "10         0.968567  0.093800             0.951754  0.137616\n",
              "11         0.969298  0.089501             0.938596  0.185584\n",
              "12         0.940789  0.174662             0.953947  0.137315\n",
              "13         0.972222  0.085265             0.951754  0.158054\n",
              "14         0.970029  0.080849             0.956140  0.130660\n",
              "15         0.972953  0.076113             0.951754  0.162924\n",
              "16         0.975877  0.076562             0.960526  0.124638\n",
              "17         0.977339  0.067954             0.956140  0.125670\n",
              "18         0.979532  0.065331             0.960526  0.124726\n",
              "19         0.980263  0.070841             0.953947  0.142017"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model 2\n",
        "model3 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "Dense(48, activation='relu'),  # Additional hidden layer\n",
        "Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model3.compile(\n",
        "optimizer = tf.keras.optimizers.Adagrad(learning_rate = 0.050353614650626294),\n",
        "loss = 'binary_crossentropy',\n",
        "metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history3 = model3.fit(train_set_x, y_train, epochs = 20, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df3 = pd.DataFrame(history3.history)\n",
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history3.history['loss'])\n",
        "plt.plot(history3.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEW4gwgFNk37"
      },
      "source": [
        "#### TESTING DIFFERENT EPOCHS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Epoch = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bNM-alLdNkpJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "6/6 - 0s - 32ms/step - binary_accuracy: 0.9598 - loss: 0.1151 - val_binary_accuracy: 0.9320 - val_loss: 0.1728\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9576 - loss: 0.1132 - val_binary_accuracy: 0.9320 - val_loss: 0.1703\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9561 - loss: 0.1094 - val_binary_accuracy: 0.9342 - val_loss: 0.1488\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9613 - loss: 0.0967 - val_binary_accuracy: 0.9386 - val_loss: 0.1513\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9605 - loss: 0.0923 - val_binary_accuracy: 0.9276 - val_loss: 0.1483\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9635 - loss: 0.0902 - val_binary_accuracy: 0.9408 - val_loss: 0.1430\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9693 - loss: 0.0873 - val_binary_accuracy: 0.9364 - val_loss: 0.1433\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9635 - loss: 0.0884 - val_binary_accuracy: 0.9430 - val_loss: 0.1417\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9715 - loss: 0.0842 - val_binary_accuracy: 0.9364 - val_loss: 0.1420\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9642 - loss: 0.0781 - val_binary_accuracy: 0.9452 - val_loss: 0.1415\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9730 - loss: 0.0803 - val_binary_accuracy: 0.9298 - val_loss: 0.1446\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9656 - loss: 0.0797 - val_binary_accuracy: 0.9518 - val_loss: 0.1318\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9788 - loss: 0.0682 - val_binary_accuracy: 0.9496 - val_loss: 0.1310\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9759 - loss: 0.0648 - val_binary_accuracy: 0.9474 - val_loss: 0.1285\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9810 - loss: 0.0602 - val_binary_accuracy: 0.9496 - val_loss: 0.1252\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9788 - loss: 0.0583 - val_binary_accuracy: 0.9474 - val_loss: 0.1238\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9788 - loss: 0.0564 - val_binary_accuracy: 0.9496 - val_loss: 0.1215\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9795 - loss: 0.0540 - val_binary_accuracy: 0.9496 - val_loss: 0.1189\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9825 - loss: 0.0539 - val_binary_accuracy: 0.9518 - val_loss: 0.1155\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9846 - loss: 0.0512 - val_binary_accuracy: 0.9539 - val_loss: 0.1183\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9795 - loss: 0.0503 - val_binary_accuracy: 0.9518 - val_loss: 0.1251\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9832 - loss: 0.0480 - val_binary_accuracy: 0.9496 - val_loss: 0.1201\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9825 - loss: 0.0459 - val_binary_accuracy: 0.9539 - val_loss: 0.1160\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 29ms/step - binary_accuracy: 0.9839 - loss: 0.0436 - val_binary_accuracy: 0.9539 - val_loss: 0.1172\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9854 - loss: 0.0418 - val_binary_accuracy: 0.9539 - val_loss: 0.1138\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9868 - loss: 0.0401 - val_binary_accuracy: 0.9583 - val_loss: 0.1125\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9883 - loss: 0.0382 - val_binary_accuracy: 0.9605 - val_loss: 0.1109\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9876 - loss: 0.0381 - val_binary_accuracy: 0.9605 - val_loss: 0.1093\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9876 - loss: 0.0354 - val_binary_accuracy: 0.9605 - val_loss: 0.1083\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9898 - loss: 0.0333 - val_binary_accuracy: 0.9539 - val_loss: 0.1075\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9890 - loss: 0.0323 - val_binary_accuracy: 0.9561 - val_loss: 0.1055\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9898 - loss: 0.0310 - val_binary_accuracy: 0.9561 - val_loss: 0.1003\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9920 - loss: 0.0293 - val_binary_accuracy: 0.9649 - val_loss: 0.0965\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9912 - loss: 0.0283 - val_binary_accuracy: 0.9627 - val_loss: 0.1015\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9920 - loss: 0.0269 - val_binary_accuracy: 0.9627 - val_loss: 0.1006\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9920 - loss: 0.0249 - val_binary_accuracy: 0.9627 - val_loss: 0.0984\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9927 - loss: 0.0242 - val_binary_accuracy: 0.9605 - val_loss: 0.0953\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9942 - loss: 0.0234 - val_binary_accuracy: 0.9583 - val_loss: 0.0915\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9934 - loss: 0.0220 - val_binary_accuracy: 0.9649 - val_loss: 0.0958\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9956 - loss: 0.0207 - val_binary_accuracy: 0.9671 - val_loss: 0.0939\n"
          ]
        }
      ],
      "source": [
        "history_e1 = model.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "V8G2XFHDNwuy"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.024936</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.098409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.024181</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.095252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.023385</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.091456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.022020</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.095809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.020715</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.093924</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "35         0.991959  0.024936             0.962719  0.098409\n",
              "36         0.992690  0.024181             0.960526  0.095252\n",
              "37         0.994152  0.023385             0.958333  0.091456\n",
              "38         0.993421  0.022020             0.964912  0.095809\n",
              "39         0.995614  0.020715             0.967105  0.093924"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_h1 = pd.DataFrame(history_e1.history)\n",
        "df_h1.tail()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Epoch = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "i_HzUsoFNys5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "6/6 - 0s - 37ms/step - binary_accuracy: 0.9942 - loss: 0.0199 - val_binary_accuracy: 0.9605 - val_loss: 0.0959\n",
            "Epoch 2/30\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9963 - loss: 0.0194 - val_binary_accuracy: 0.9583 - val_loss: 0.0878\n",
            "Epoch 3/30\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9942 - loss: 0.0192 - val_binary_accuracy: 0.9649 - val_loss: 0.0915\n",
            "Epoch 4/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0176 - val_binary_accuracy: 0.9649 - val_loss: 0.0920\n",
            "Epoch 5/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0166 - val_binary_accuracy: 0.9605 - val_loss: 0.0935\n",
            "Epoch 6/30\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9963 - loss: 0.0161 - val_binary_accuracy: 0.9605 - val_loss: 0.0947\n",
            "Epoch 7/30\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9956 - loss: 0.0157 - val_binary_accuracy: 0.9627 - val_loss: 0.0982\n",
            "Epoch 8/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9956 - loss: 0.0150 - val_binary_accuracy: 0.9605 - val_loss: 0.0957\n",
            "Epoch 9/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0138 - val_binary_accuracy: 0.9605 - val_loss: 0.0941\n",
            "Epoch 10/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0132 - val_binary_accuracy: 0.9605 - val_loss: 0.0980\n",
            "Epoch 11/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0140 - val_binary_accuracy: 0.9605 - val_loss: 0.0895\n",
            "Epoch 12/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0124 - val_binary_accuracy: 0.9627 - val_loss: 0.0954\n",
            "Epoch 13/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0130 - val_binary_accuracy: 0.9605 - val_loss: 0.1006\n",
            "Epoch 14/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0106 - val_binary_accuracy: 0.9627 - val_loss: 0.0892\n",
            "Epoch 15/30\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9971 - loss: 0.0110 - val_binary_accuracy: 0.9627 - val_loss: 0.1051\n",
            "Epoch 16/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9985 - loss: 0.0102 - val_binary_accuracy: 0.9627 - val_loss: 0.0937\n",
            "Epoch 17/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9971 - loss: 0.0097 - val_binary_accuracy: 0.9627 - val_loss: 0.0937\n",
            "Epoch 18/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9971 - loss: 0.0094 - val_binary_accuracy: 0.9627 - val_loss: 0.1007\n",
            "Epoch 19/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9978 - loss: 0.0094 - val_binary_accuracy: 0.9627 - val_loss: 0.0963\n",
            "Epoch 20/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0085 - val_binary_accuracy: 0.9627 - val_loss: 0.0970\n",
            "Epoch 21/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9985 - loss: 0.0084 - val_binary_accuracy: 0.9627 - val_loss: 0.1027\n",
            "Epoch 22/30\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9985 - loss: 0.0077 - val_binary_accuracy: 0.9649 - val_loss: 0.0936\n",
            "Epoch 23/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0071 - val_binary_accuracy: 0.9649 - val_loss: 0.1050\n",
            "Epoch 24/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0070 - val_binary_accuracy: 0.9649 - val_loss: 0.0977\n",
            "Epoch 25/30\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9993 - loss: 0.0072 - val_binary_accuracy: 0.9649 - val_loss: 0.1009\n",
            "Epoch 26/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9985 - loss: 0.0070 - val_binary_accuracy: 0.9649 - val_loss: 0.1011\n",
            "Epoch 27/30\n",
            "6/6 - 0s - 33ms/step - binary_accuracy: 0.9993 - loss: 0.0073 - val_binary_accuracy: 0.9649 - val_loss: 0.0898\n",
            "Epoch 28/30\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0080 - val_binary_accuracy: 0.9627 - val_loss: 0.1098\n",
            "Epoch 29/30\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9993 - loss: 0.0064 - val_binary_accuracy: 0.9671 - val_loss: 0.0906\n",
            "Epoch 30/30\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9993 - loss: 0.0067 - val_binary_accuracy: 0.9627 - val_loss: 0.1149\n"
          ]
        }
      ],
      "source": [
        "history_e2 = model.fit(train_set_x, y_train, epochs = 30, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uSPmls-4N1OV"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.006982</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.101147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.007288</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.089844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.007985</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.109766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.006401</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.090604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.006719</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.114940</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "25         0.998538  0.006982             0.964912  0.101147\n",
              "26         0.999269  0.007288             0.964912  0.089844\n",
              "27         0.999269  0.007985             0.962719  0.109766\n",
              "28         0.999269  0.006401             0.967105  0.090604\n",
              "29         0.999269  0.006719             0.962719  0.114940"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_h2 = pd.DataFrame(history_e2.history)\n",
        "df_h2.tail()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Epoch = 35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hvn9XKNnN21m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "6/6 - 0s - 35ms/step - binary_accuracy: 0.9993 - loss: 0.0053 - val_binary_accuracy: 0.9649 - val_loss: 0.1016\n",
            "Epoch 2/35\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 1.0000 - loss: 0.0054 - val_binary_accuracy: 0.9649 - val_loss: 0.1031\n",
            "Epoch 3/35\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0048 - val_binary_accuracy: 0.9671 - val_loss: 0.1085\n",
            "Epoch 4/35\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9993 - loss: 0.0048 - val_binary_accuracy: 0.9649 - val_loss: 0.1057\n",
            "Epoch 5/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0042 - val_binary_accuracy: 0.9649 - val_loss: 0.1046\n",
            "Epoch 6/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0049 - val_binary_accuracy: 0.9649 - val_loss: 0.1065\n",
            "Epoch 7/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0048 - val_binary_accuracy: 0.9671 - val_loss: 0.0981\n",
            "Epoch 8/35\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0044 - val_binary_accuracy: 0.9649 - val_loss: 0.1208\n",
            "Epoch 9/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0038 - val_binary_accuracy: 0.9649 - val_loss: 0.0905\n",
            "Epoch 10/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0043 - val_binary_accuracy: 0.9671 - val_loss: 0.1110\n",
            "Epoch 11/35\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0035 - val_binary_accuracy: 0.9649 - val_loss: 0.1242\n",
            "Epoch 12/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0033 - val_binary_accuracy: 0.9671 - val_loss: 0.1074\n",
            "Epoch 13/35\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0032 - val_binary_accuracy: 0.9671 - val_loss: 0.1061\n",
            "Epoch 14/35\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 1.0000 - loss: 0.0030 - val_binary_accuracy: 0.9671 - val_loss: 0.1138\n",
            "Epoch 15/35\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0029 - val_binary_accuracy: 0.9649 - val_loss: 0.1121\n",
            "Epoch 16/35\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0028 - val_binary_accuracy: 0.9649 - val_loss: 0.1022\n",
            "Epoch 17/35\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0029 - val_binary_accuracy: 0.9649 - val_loss: 0.1047\n",
            "Epoch 18/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0031 - val_binary_accuracy: 0.9649 - val_loss: 0.1184\n",
            "Epoch 19/35\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0027 - val_binary_accuracy: 0.9671 - val_loss: 0.0996\n",
            "Epoch 20/35\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 1.0000 - loss: 0.0026 - val_binary_accuracy: 0.9649 - val_loss: 0.1066\n",
            "Epoch 21/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0024 - val_binary_accuracy: 0.9649 - val_loss: 0.1131\n",
            "Epoch 22/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0023 - val_binary_accuracy: 0.9649 - val_loss: 0.1123\n",
            "Epoch 23/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0023 - val_binary_accuracy: 0.9649 - val_loss: 0.1173\n",
            "Epoch 24/35\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0022 - val_binary_accuracy: 0.9671 - val_loss: 0.1083\n",
            "Epoch 25/35\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 1.0000 - loss: 0.0022 - val_binary_accuracy: 0.9649 - val_loss: 0.1134\n",
            "Epoch 26/35\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0021 - val_binary_accuracy: 0.9649 - val_loss: 0.1194\n",
            "Epoch 27/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0020 - val_binary_accuracy: 0.9649 - val_loss: 0.1096\n",
            "Epoch 28/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0020 - val_binary_accuracy: 0.9649 - val_loss: 0.1126\n",
            "Epoch 29/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0019 - val_binary_accuracy: 0.9649 - val_loss: 0.1208\n",
            "Epoch 30/35\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0019 - val_binary_accuracy: 0.9649 - val_loss: 0.1195\n",
            "Epoch 31/35\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0019 - val_binary_accuracy: 0.9671 - val_loss: 0.1072\n",
            "Epoch 32/35\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0018 - val_binary_accuracy: 0.9649 - val_loss: 0.1148\n",
            "Epoch 33/35\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0017 - val_binary_accuracy: 0.9649 - val_loss: 0.1177\n",
            "Epoch 34/35\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0017 - val_binary_accuracy: 0.9649 - val_loss: 0.1163\n",
            "Epoch 35/35\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0016 - val_binary_accuracy: 0.9671 - val_loss: 0.1158\n"
          ]
        }
      ],
      "source": [
        "history_e3 = model.fit(train_set_x, y_train, epochs = 35, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zhu44JaRN6lw"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.107241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001820</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.114770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001712</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.117673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001654</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.116349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001612</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.115808</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "30              1.0  0.001873             0.967105  0.107241\n",
              "31              1.0  0.001820             0.964912  0.114770\n",
              "32              1.0  0.001712             0.964912  0.117673\n",
              "33              1.0  0.001654             0.964912  0.116349\n",
              "34              1.0  0.001612             0.967105  0.115808"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_h3 = pd.DataFrame(history_e3.history)\n",
        "df_h3.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad3rbKjJOSeD"
      },
      "source": [
        "#### WEIGHT INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_spCqDaIOUKa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<KerasVariable shape=(12288, 64), dtype=float32, path=sequential_4/dense_10/kernel>,\n",
              " <KerasVariable shape=(64,), dtype=float32, path=sequential_4/dense_10/bias>,\n",
              " <KerasVariable shape=(64, 48), dtype=float32, path=sequential_4/dense_11/kernel>,\n",
              " <KerasVariable shape=(48,), dtype=float32, path=sequential_4/dense_11/bias>,\n",
              " <KerasVariable shape=(48, 1), dtype=float32, path=sequential_4/dense_12/kernel>,\n",
              " <KerasVariable shape=(1,), dtype=float32, path=sequential_4/dense_12/bias>]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9CRn1e8_OUrZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 275ms/step - binary_accuracy: 0.5022 - loss: 185.3811 - val_binary_accuracy: 0.4715 - val_loss: 12.5061\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5461 - loss: 9.7847 - val_binary_accuracy: 0.8838 - val_loss: 0.4964\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.7551 - loss: 1.9272 - val_binary_accuracy: 0.8092 - val_loss: 1.7147\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.7822 - loss: 1.9986 - val_binary_accuracy: 0.8706 - val_loss: 0.6849\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.8801 - loss: 0.6495 - val_binary_accuracy: 0.8289 - val_loss: 0.9356\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8925 - loss: 0.6040 - val_binary_accuracy: 0.9101 - val_loss: 0.3731\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.8940 - loss: 0.4711 - val_binary_accuracy: 0.9013 - val_loss: 0.4855\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.9079 - loss: 0.3818 - val_binary_accuracy: 0.9189 - val_loss: 0.3078\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9291 - loss: 0.2929 - val_binary_accuracy: 0.9145 - val_loss: 0.3144\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 30ms/step - binary_accuracy: 0.9408 - loss: 0.2415 - val_binary_accuracy: 0.9342 - val_loss: 0.2523\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9298 - loss: 0.2242 - val_binary_accuracy: 0.9276 - val_loss: 0.2674\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9335 - loss: 0.2061 - val_binary_accuracy: 0.9364 - val_loss: 0.2250\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9488 - loss: 0.1814 - val_binary_accuracy: 0.9386 - val_loss: 0.2202\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9532 - loss: 0.1709 - val_binary_accuracy: 0.9386 - val_loss: 0.2093\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9408 - loss: 0.1660 - val_binary_accuracy: 0.9386 - val_loss: 0.2148\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9503 - loss: 0.1550 - val_binary_accuracy: 0.9386 - val_loss: 0.1957\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9547 - loss: 0.1461 - val_binary_accuracy: 0.9386 - val_loss: 0.1931\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9569 - loss: 0.1455 - val_binary_accuracy: 0.9430 - val_loss: 0.1864\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9569 - loss: 0.1378 - val_binary_accuracy: 0.9408 - val_loss: 0.1875\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9569 - loss: 0.1340 - val_binary_accuracy: 0.9452 - val_loss: 0.1793\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 28ms/step - binary_accuracy: 0.9591 - loss: 0.1292 - val_binary_accuracy: 0.9452 - val_loss: 0.1758\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 27ms/step - binary_accuracy: 0.9591 - loss: 0.1260 - val_binary_accuracy: 0.9452 - val_loss: 0.1728\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9591 - loss: 0.1238 - val_binary_accuracy: 0.9474 - val_loss: 0.1699\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9620 - loss: 0.1209 - val_binary_accuracy: 0.9496 - val_loss: 0.1703\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9620 - loss: 0.1192 - val_binary_accuracy: 0.9496 - val_loss: 0.1672\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 28ms/step - binary_accuracy: 0.9620 - loss: 0.1164 - val_binary_accuracy: 0.9474 - val_loss: 0.1648\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9613 - loss: 0.1163 - val_binary_accuracy: 0.9474 - val_loss: 0.1661\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9627 - loss: 0.1128 - val_binary_accuracy: 0.9496 - val_loss: 0.1617\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 29ms/step - binary_accuracy: 0.9627 - loss: 0.1114 - val_binary_accuracy: 0.9518 - val_loss: 0.1602\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9627 - loss: 0.1101 - val_binary_accuracy: 0.9496 - val_loss: 0.1629\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9656 - loss: 0.1083 - val_binary_accuracy: 0.9518 - val_loss: 0.1585\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9642 - loss: 0.1062 - val_binary_accuracy: 0.9518 - val_loss: 0.1577\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9656 - loss: 0.1046 - val_binary_accuracy: 0.9496 - val_loss: 0.1582\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9664 - loss: 0.1038 - val_binary_accuracy: 0.9496 - val_loss: 0.1568\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9656 - loss: 0.1025 - val_binary_accuracy: 0.9539 - val_loss: 0.1551\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9671 - loss: 0.1025 - val_binary_accuracy: 0.9496 - val_loss: 0.1565\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 28ms/step - binary_accuracy: 0.9656 - loss: 0.1027 - val_binary_accuracy: 0.9474 - val_loss: 0.1531\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9671 - loss: 0.0998 - val_binary_accuracy: 0.9496 - val_loss: 0.1549\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9678 - loss: 0.0981 - val_binary_accuracy: 0.9539 - val_loss: 0.1525\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9693 - loss: 0.0968 - val_binary_accuracy: 0.9518 - val_loss: 0.1539\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.502193</td>\n",
              "      <td>185.381088</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>12.506138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.546053</td>\n",
              "      <td>9.784682</td>\n",
              "      <td>0.883772</td>\n",
              "      <td>0.496436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.755117</td>\n",
              "      <td>1.927222</td>\n",
              "      <td>0.809211</td>\n",
              "      <td>1.714694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.782164</td>\n",
              "      <td>1.998560</td>\n",
              "      <td>0.870614</td>\n",
              "      <td>0.684940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.880117</td>\n",
              "      <td>0.649498</td>\n",
              "      <td>0.828947</td>\n",
              "      <td>0.935618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.892544</td>\n",
              "      <td>0.603971</td>\n",
              "      <td>0.910088</td>\n",
              "      <td>0.373057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.894006</td>\n",
              "      <td>0.471060</td>\n",
              "      <td>0.901316</td>\n",
              "      <td>0.485475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.907895</td>\n",
              "      <td>0.381756</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>0.307812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.929094</td>\n",
              "      <td>0.292859</td>\n",
              "      <td>0.914474</td>\n",
              "      <td>0.314387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.241542</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.252265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.929825</td>\n",
              "      <td>0.224163</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.267371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.933480</td>\n",
              "      <td>0.206108</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.225048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.948830</td>\n",
              "      <td>0.181427</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.220234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.953216</td>\n",
              "      <td>0.170878</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.209265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.165952</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.214761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.950292</td>\n",
              "      <td>0.155006</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.195727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>0.146143</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.193129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.956871</td>\n",
              "      <td>0.145539</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.186442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.956871</td>\n",
              "      <td>0.137804</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.187535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.956871</td>\n",
              "      <td>0.133980</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.179323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.959064</td>\n",
              "      <td>0.129169</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.175844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.959064</td>\n",
              "      <td>0.125991</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.172824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.959064</td>\n",
              "      <td>0.123830</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.169945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>0.120945</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.170324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>0.119177</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.167243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>0.116382</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.164811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.961257</td>\n",
              "      <td>0.116286</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.166087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.112812</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.161731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.111442</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.160203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.110138</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.162914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.965643</td>\n",
              "      <td>0.108345</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.158475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>0.106153</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.157691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.965643</td>\n",
              "      <td>0.104613</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.158175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>0.103755</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.156804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.965643</td>\n",
              "      <td>0.102503</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.155133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.102513</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.156511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.965643</td>\n",
              "      <td>0.102659</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.153055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.099804</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.154944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.967836</td>\n",
              "      <td>0.098068</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.152476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.096818</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.153935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.502193  185.381088             0.471491  12.506138\n",
              "1          0.546053    9.784682             0.883772   0.496436\n",
              "2          0.755117    1.927222             0.809211   1.714694\n",
              "3          0.782164    1.998560             0.870614   0.684940\n",
              "4          0.880117    0.649498             0.828947   0.935618\n",
              "5          0.892544    0.603971             0.910088   0.373057\n",
              "6          0.894006    0.471060             0.901316   0.485475\n",
              "7          0.907895    0.381756             0.918860   0.307812\n",
              "8          0.929094    0.292859             0.914474   0.314387\n",
              "9          0.940789    0.241542             0.934211   0.252265\n",
              "10         0.929825    0.224163             0.927632   0.267371\n",
              "11         0.933480    0.206108             0.936404   0.225048\n",
              "12         0.948830    0.181427             0.938596   0.220234\n",
              "13         0.953216    0.170878             0.938596   0.209265\n",
              "14         0.940789    0.165952             0.938596   0.214761\n",
              "15         0.950292    0.155006             0.938596   0.195727\n",
              "16         0.954678    0.146143             0.938596   0.193129\n",
              "17         0.956871    0.145539             0.942982   0.186442\n",
              "18         0.956871    0.137804             0.940789   0.187535\n",
              "19         0.956871    0.133980             0.945175   0.179323\n",
              "20         0.959064    0.129169             0.945175   0.175844\n",
              "21         0.959064    0.125991             0.945175   0.172824\n",
              "22         0.959064    0.123830             0.947368   0.169945\n",
              "23         0.961988    0.120945             0.949561   0.170324\n",
              "24         0.961988    0.119177             0.949561   0.167243\n",
              "25         0.961988    0.116382             0.947368   0.164811\n",
              "26         0.961257    0.116286             0.947368   0.166087\n",
              "27         0.962719    0.112812             0.949561   0.161731\n",
              "28         0.962719    0.111442             0.951754   0.160203\n",
              "29         0.962719    0.110138             0.949561   0.162914\n",
              "30         0.965643    0.108345             0.951754   0.158475\n",
              "31         0.964181    0.106153             0.951754   0.157691\n",
              "32         0.965643    0.104613             0.949561   0.158175\n",
              "33         0.966374    0.103755             0.949561   0.156804\n",
              "34         0.965643    0.102503             0.953947   0.155133\n",
              "35         0.967105    0.102513             0.949561   0.156511\n",
              "36         0.965643    0.102659             0.947368   0.153055\n",
              "37         0.967105    0.099804             0.949561   0.154944\n",
              "38         0.967836    0.098068             0.953947   0.152476\n",
              "39         0.969298    0.096818             0.951754   0.153935"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_he = Sequential([\n",
        "    Dense(64, kernel_initializer=tf.keras.initializers.HeUniform(seed = 3),\n",
        "          bias_initializer=tf.keras.initializers.Constant(value=0.4), activation='relu',input_shape = (12288,)),\n",
        "    Dense(48, kernel_initializer=tf.keras.initializers.HeUniform(seed = 3),\n",
        "          bias_initializer=tf.keras.initializers.Constant(value=0.4), activation='relu'),      \n",
        "    Dense(1, kernel_initializer=tf.keras.initializers.HeUniform(seed = 3),\n",
        "          bias_initializer=tf.keras.initializers.Constant(value=0.4), activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_he.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_he = model_he.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_he = pd.DataFrame(history_he.history)\n",
        "df_he\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WYg9co8IOa8i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 300ms/step - binary_accuracy: 0.5022 - loss: 156.3824 - val_binary_accuracy: 0.5285 - val_loss: 32.7112\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.6096 - loss: 23.2809 - val_binary_accuracy: 0.8224 - val_loss: 7.0764\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.7880 - loss: 10.2119 - val_binary_accuracy: 0.8991 - val_loss: 3.5254\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8889 - loss: 4.4810 - val_binary_accuracy: 0.9057 - val_loss: 2.2268\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.8743 - loss: 2.8074 - val_binary_accuracy: 0.9342 - val_loss: 1.8395\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 29ms/step - binary_accuracy: 0.9364 - loss: 1.8166 - val_binary_accuracy: 0.9430 - val_loss: 1.9643\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9225 - loss: 1.4583 - val_binary_accuracy: 0.9408 - val_loss: 1.5775\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9569 - loss: 0.9378 - val_binary_accuracy: 0.9430 - val_loss: 1.4895\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9503 - loss: 0.6998 - val_binary_accuracy: 0.9430 - val_loss: 0.9976\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9561 - loss: 0.8141 - val_binary_accuracy: 0.9605 - val_loss: 0.8156\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9466 - loss: 0.6862 - val_binary_accuracy: 0.9518 - val_loss: 0.8580\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9598 - loss: 0.4767 - val_binary_accuracy: 0.9561 - val_loss: 0.6443\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9649 - loss: 0.3160 - val_binary_accuracy: 0.9518 - val_loss: 0.6552\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9759 - loss: 0.2747 - val_binary_accuracy: 0.9561 - val_loss: 0.5479\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 0.2291 - val_binary_accuracy: 0.9583 - val_loss: 0.5045\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9803 - loss: 0.1842 - val_binary_accuracy: 0.9583 - val_loss: 0.4689\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9795 - loss: 0.1710 - val_binary_accuracy: 0.9605 - val_loss: 0.4468\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9846 - loss: 0.1573 - val_binary_accuracy: 0.9605 - val_loss: 0.4351\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9810 - loss: 0.1476 - val_binary_accuracy: 0.9671 - val_loss: 0.4145\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9810 - loss: 0.1482 - val_binary_accuracy: 0.9649 - val_loss: 0.4124\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9825 - loss: 0.1371 - val_binary_accuracy: 0.9671 - val_loss: 0.3853\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.1252 - val_binary_accuracy: 0.9671 - val_loss: 0.3756\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9883 - loss: 0.1228 - val_binary_accuracy: 0.9693 - val_loss: 0.3609\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9817 - loss: 0.1292 - val_binary_accuracy: 0.9671 - val_loss: 0.3468\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9876 - loss: 0.1167 - val_binary_accuracy: 0.9671 - val_loss: 0.3453\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9846 - loss: 0.1133 - val_binary_accuracy: 0.9671 - val_loss: 0.3245\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9868 - loss: 0.1136 - val_binary_accuracy: 0.9671 - val_loss: 0.3206\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9832 - loss: 0.1090 - val_binary_accuracy: 0.9649 - val_loss: 0.3004\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9890 - loss: 0.1046 - val_binary_accuracy: 0.9693 - val_loss: 0.2941\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.1005 - val_binary_accuracy: 0.9693 - val_loss: 0.2796\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9876 - loss: 0.0998 - val_binary_accuracy: 0.9693 - val_loss: 0.2716\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9898 - loss: 0.0859 - val_binary_accuracy: 0.9693 - val_loss: 0.2620\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9890 - loss: 0.0855 - val_binary_accuracy: 0.9671 - val_loss: 0.2566\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9898 - loss: 0.0783 - val_binary_accuracy: 0.9671 - val_loss: 0.2448\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9861 - loss: 0.0825 - val_binary_accuracy: 0.9649 - val_loss: 0.2452\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9868 - loss: 0.0810 - val_binary_accuracy: 0.9671 - val_loss: 0.2289\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9898 - loss: 0.0736 - val_binary_accuracy: 0.9693 - val_loss: 0.2250\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9905 - loss: 0.0687 - val_binary_accuracy: 0.9693 - val_loss: 0.2121\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0698 - val_binary_accuracy: 0.9693 - val_loss: 0.2052\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9868 - loss: 0.0735 - val_binary_accuracy: 0.9693 - val_loss: 0.1992\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.502193</td>\n",
              "      <td>156.382446</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>32.711159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.609649</td>\n",
              "      <td>23.280886</td>\n",
              "      <td>0.822368</td>\n",
              "      <td>7.076355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.788012</td>\n",
              "      <td>10.211886</td>\n",
              "      <td>0.899123</td>\n",
              "      <td>3.525421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.888889</td>\n",
              "      <td>4.480951</td>\n",
              "      <td>0.905702</td>\n",
              "      <td>2.226835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.874269</td>\n",
              "      <td>2.807415</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>1.839518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.936404</td>\n",
              "      <td>1.816643</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>1.964334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.922515</td>\n",
              "      <td>1.458332</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>1.577458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.956871</td>\n",
              "      <td>0.937832</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>1.489484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.950292</td>\n",
              "      <td>0.699772</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.997622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.814078</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.815566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.946637</td>\n",
              "      <td>0.686202</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.857997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.959795</td>\n",
              "      <td>0.476730</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.644334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.316039</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.655191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.274681</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.547929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.229064</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.504472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.184161</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.468898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.171049</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.446814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.157330</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.435098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>0.147555</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.414493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>0.148210</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.412370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.137113</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.385342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.125230</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.375551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.122751</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.360890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.129247</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.346752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.116679</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.345269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.113301</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.324545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.113589</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.320639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.983187</td>\n",
              "      <td>0.108975</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.300404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.104631</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.294056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.100509</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.279587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.099829</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.271629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.085903</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.262001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.085510</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.256575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.078345</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.244754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.082491</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.245233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.081019</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.228916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.073638</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.224952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.068688</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.212084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.069814</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.205249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.073508</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.199207</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.502193  156.382446             0.528509  32.711159\n",
              "1          0.609649   23.280886             0.822368   7.076355\n",
              "2          0.788012   10.211886             0.899123   3.525421\n",
              "3          0.888889    4.480951             0.905702   2.226835\n",
              "4          0.874269    2.807415             0.934211   1.839518\n",
              "5          0.936404    1.816643             0.942982   1.964334\n",
              "6          0.922515    1.458332             0.940789   1.577458\n",
              "7          0.956871    0.937832             0.942982   1.489484\n",
              "8          0.950292    0.699772             0.942982   0.997622\n",
              "9          0.956140    0.814078             0.960526   0.815566\n",
              "10         0.946637    0.686202             0.951754   0.857997\n",
              "11         0.959795    0.476730             0.956140   0.644334\n",
              "12         0.964912    0.316039             0.951754   0.655191\n",
              "13         0.975877    0.274681             0.956140   0.547929\n",
              "14         0.969298    0.229064             0.958333   0.504472\n",
              "15         0.980263    0.184161             0.958333   0.468898\n",
              "16         0.979532    0.171049             0.960526   0.446814\n",
              "17         0.984649    0.157330             0.960526   0.435098\n",
              "18         0.980994    0.147555             0.967105   0.414493\n",
              "19         0.980994    0.148210             0.964912   0.412370\n",
              "20         0.982456    0.137113             0.967105   0.385342\n",
              "21         0.988304    0.125230             0.967105   0.375551\n",
              "22         0.988304    0.122751             0.969298   0.360890\n",
              "23         0.981725    0.129247             0.967105   0.346752\n",
              "24         0.987573    0.116679             0.967105   0.345269\n",
              "25         0.984649    0.113301             0.967105   0.324545\n",
              "26         0.986842    0.113589             0.967105   0.320639\n",
              "27         0.983187    0.108975             0.964912   0.300404\n",
              "28         0.989035    0.104631             0.969298   0.294056\n",
              "29         0.983918    0.100509             0.969298   0.279587\n",
              "30         0.987573    0.099829             0.969298   0.271629\n",
              "31         0.989766    0.085903             0.969298   0.262001\n",
              "32         0.989035    0.085510             0.967105   0.256575\n",
              "33         0.989766    0.078345             0.967105   0.244754\n",
              "34         0.986111    0.082491             0.964912   0.245233\n",
              "35         0.986842    0.081019             0.967105   0.228916\n",
              "36         0.989766    0.073638             0.969298   0.224952\n",
              "37         0.990497    0.068688             0.969298   0.212084\n",
              "38         0.988304    0.069814             0.969298   0.205249\n",
              "39         0.986842    0.073508             0.969298   0.199207"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_glo = Sequential([\n",
        "    Dense(64, kernel_initializer=tf.keras.initializers.GlorotUniform(seed = 3),\n",
        "          bias_initializer=tf.keras.initializers.Constant(value=0.4), activation='relu',input_shape = (12288,)),\n",
        "    Dense(48, kernel_initializer=tf.keras.initializers.GlorotUniform(seed = 3),\n",
        "          bias_initializer=tf.keras.initializers.Constant(value=0.4), activation='relu'),   \n",
        "    Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed = 3),\n",
        "          bias_initializer=tf.keras.initializers.Constant(value=0.4), activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_glo.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_glo = model_glo.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_glo = pd.DataFrame(history_glo.history)\n",
        "df_glo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GNip4l7HOdDf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 283ms/step - binary_accuracy: 0.4846 - loss: 69.6753 - val_binary_accuracy: 0.4715 - val_loss: 192.6669\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 35ms/step - binary_accuracy: 0.5395 - loss: 88.5339 - val_binary_accuracy: 0.8596 - val_loss: 0.3975\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5694 - loss: 0.6578 - val_binary_accuracy: 0.4715 - val_loss: 0.7255\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.4963 - loss: 0.7103 - val_binary_accuracy: 0.4715 - val_loss: 0.7062\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.4963 - loss: 0.6983 - val_binary_accuracy: 0.4715 - val_loss: 0.6985\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.4963 - loss: 0.6948 - val_binary_accuracy: 0.4715 - val_loss: 0.6943\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.4934 - loss: 0.6934 - val_binary_accuracy: 0.5285 - val_loss: 0.6928\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 27ms/step - binary_accuracy: 0.5037 - loss: 0.6933 - val_binary_accuracy: 0.5285 - val_loss: 0.6923\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6924\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6926\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6930\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.4788 - loss: 0.6932 - val_binary_accuracy: 0.4715 - val_loss: 0.6934\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.4963 - loss: 0.6933 - val_binary_accuracy: 0.4715 - val_loss: 0.6935\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.4963 - loss: 0.6932 - val_binary_accuracy: 0.4715 - val_loss: 0.6934\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.4846 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6931\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6928\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6926\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6928\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 19ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6928\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6928\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6925\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6924\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6923\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6923\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6923\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6926\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6927\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6927\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6927\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6930\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6929\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.4920 - loss: 0.6932 - val_binary_accuracy: 0.4715 - val_loss: 0.6932\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.4905 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6930\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6928\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6930\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6928\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6925\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5037 - loss: 0.6932 - val_binary_accuracy: 0.5285 - val_loss: 0.6926\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6927\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.5037 - loss: 0.6931 - val_binary_accuracy: 0.5285 - val_loss: 0.6929\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.484649</td>\n",
              "      <td>69.675301</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>192.666870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.539474</td>\n",
              "      <td>88.533852</td>\n",
              "      <td>0.859649</td>\n",
              "      <td>0.397472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.569444</td>\n",
              "      <td>0.657765</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.725498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.496345</td>\n",
              "      <td>0.710254</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.706228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.496345</td>\n",
              "      <td>0.698263</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.698497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.496345</td>\n",
              "      <td>0.694846</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.694285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.493421</td>\n",
              "      <td>0.693364</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693327</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693175</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693114</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693089</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.693013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.478801</td>\n",
              "      <td>0.693245</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.693439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.496345</td>\n",
              "      <td>0.693263</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.693524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.496345</td>\n",
              "      <td>0.693223</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.693421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.484649</td>\n",
              "      <td>0.693192</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.693051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693189</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693178</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693128</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693129</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693135</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693155</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693141</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693199</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693188</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693222</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693148</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693132</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693143</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693146</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693134</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693149</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.491959</td>\n",
              "      <td>0.693198</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.693234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.490497</td>\n",
              "      <td>0.693161</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693157</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693191</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693126</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693154</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693169</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693132</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.503655</td>\n",
              "      <td>0.693117</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>0.692894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy       loss  val_binary_accuracy    val_loss\n",
              "0          0.484649  69.675301             0.471491  192.666870\n",
              "1          0.539474  88.533852             0.859649    0.397472\n",
              "2          0.569444   0.657765             0.471491    0.725498\n",
              "3          0.496345   0.710254             0.471491    0.706228\n",
              "4          0.496345   0.698263             0.471491    0.698497\n",
              "5          0.496345   0.694846             0.471491    0.694285\n",
              "6          0.493421   0.693364             0.528509    0.692789\n",
              "7          0.503655   0.693327             0.528509    0.692263\n",
              "8          0.503655   0.693175             0.528509    0.692361\n",
              "9          0.503655   0.693114             0.528509    0.692642\n",
              "10         0.503655   0.693089             0.528509    0.693013\n",
              "11         0.478801   0.693245             0.471491    0.693439\n",
              "12         0.496345   0.693263             0.471491    0.693524\n",
              "13         0.496345   0.693223             0.471491    0.693421\n",
              "14         0.484649   0.693192             0.528509    0.693051\n",
              "15         0.503655   0.693189             0.528509    0.692755\n",
              "16         0.503655   0.693178             0.528509    0.692641\n",
              "17         0.503655   0.693128             0.528509    0.692754\n",
              "18         0.503655   0.693129             0.528509    0.692807\n",
              "19         0.503655   0.693135             0.528509    0.692758\n",
              "20         0.503655   0.693155             0.528509    0.692544\n",
              "21         0.503655   0.693141             0.528509    0.692385\n",
              "22         0.503655   0.693199             0.528509    0.692251\n",
              "23         0.503655   0.693188             0.528509    0.692283\n",
              "24         0.503655   0.693222             0.528509    0.692338\n",
              "25         0.503655   0.693148             0.528509    0.692622\n",
              "26         0.503655   0.693132             0.528509    0.692677\n",
              "27         0.503655   0.693143             0.528509    0.692712\n",
              "28         0.503655   0.693146             0.528509    0.692699\n",
              "29         0.503655   0.693134             0.528509    0.692961\n",
              "30         0.503655   0.693149             0.528509    0.692944\n",
              "31         0.491959   0.693198             0.471491    0.693234\n",
              "32         0.490497   0.693161             0.528509    0.692955\n",
              "33         0.503655   0.693157             0.528509    0.692790\n",
              "34         0.503655   0.693191             0.528509    0.692993\n",
              "35         0.503655   0.693126             0.528509    0.692783\n",
              "36         0.503655   0.693154             0.528509    0.692510\n",
              "37         0.503655   0.693169             0.528509    0.692626\n",
              "38         0.503655   0.693132             0.528509    0.692671\n",
              "39         0.503655   0.693117             0.528509    0.692894"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_ru = Sequential([\n",
        "    Dense(64, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
        "                bias_initializer=tf.keras.initializers.Constant(value=0.4),\n",
        "                activation='relu',input_shape = (12288,)),\n",
        "    Dense(48, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
        "                bias_initializer=tf.keras.initializers.Constant(value=0.4), activation='relu'),   \n",
        "    Dense(1, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
        "          bias_initializer=tf.keras.initializers.Constant(value=0.4), activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model_ru.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_ru = model_ru.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_ru = pd.DataFrame(history_ru.history)\n",
        "df_ru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.531\n"
          ]
        }
      ],
      "source": [
        "get_test_accuracy(model_ru, test_set_x.T, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbBkSfMhOlI6"
      },
      "source": [
        "### PART 3: FIT VALIDATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check for overfitting on best model so far"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history_e1.history['loss'])\n",
        "plt.plot(history_e1.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### L2 Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmcmQcKkJR61"
      },
      "source": [
        "L2 REGULARIZATION - Penalty Rate 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TAo4NYkUbbJH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "6/6 - 2s - 330ms/step - binary_accuracy: 0.5687 - loss: 77.5009 - val_binary_accuracy: 0.8772 - val_loss: 11.0545\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8779 - loss: 12.0882 - val_binary_accuracy: 0.8947 - val_loss: 12.0247\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9057 - loss: 11.8714 - val_binary_accuracy: 0.9430 - val_loss: 10.8476\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9466 - loss: 10.2838 - val_binary_accuracy: 0.9474 - val_loss: 9.5730\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9532 - loss: 8.8116 - val_binary_accuracy: 0.8925 - val_loss: 8.6014\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9415 - loss: 7.6967 - val_binary_accuracy: 0.7961 - val_loss: 8.0250\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9108 - loss: 6.9203 - val_binary_accuracy: 0.9452 - val_loss: 6.3120\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9481 - loss: 5.8669 - val_binary_accuracy: 0.9364 - val_loss: 5.5939\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9620 - loss: 5.1697 - val_binary_accuracy: 0.9276 - val_loss: 5.0659\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9598 - loss: 4.5675 - val_binary_accuracy: 0.9430 - val_loss: 4.3132\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9686 - loss: 3.9774 - val_binary_accuracy: 0.9539 - val_loss: 3.8615\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9773 - loss: 3.5022 - val_binary_accuracy: 0.9430 - val_loss: 3.4959\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9730 - loss: 3.1290 - val_binary_accuracy: 0.9539 - val_loss: 3.0618\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9744 - loss: 2.8160 - val_binary_accuracy: 0.9561 - val_loss: 2.7498\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9817 - loss: 2.5119 - val_binary_accuracy: 0.9561 - val_loss: 2.4977\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9810 - loss: 2.2861 - val_binary_accuracy: 0.9342 - val_loss: 2.4059\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 32ms/step - binary_accuracy: 0.9664 - loss: 2.1218 - val_binary_accuracy: 0.9561 - val_loss: 2.0724\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9795 - loss: 1.9040 - val_binary_accuracy: 0.9627 - val_loss: 1.9118\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9810 - loss: 1.7387 - val_binary_accuracy: 0.9496 - val_loss: 1.7645\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9810 - loss: 1.6024 - val_binary_accuracy: 0.9627 - val_loss: 1.6085\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9825 - loss: 1.4618 - val_binary_accuracy: 0.9518 - val_loss: 1.5259\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9708 - loss: 1.3748 - val_binary_accuracy: 0.9561 - val_loss: 1.3675\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9788 - loss: 1.2638 - val_binary_accuracy: 0.9605 - val_loss: 1.2892\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9737 - loss: 1.1823 - val_binary_accuracy: 0.9496 - val_loss: 1.2443\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9737 - loss: 1.0896 - val_binary_accuracy: 0.9364 - val_loss: 1.2263\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9671 - loss: 1.0330 - val_binary_accuracy: 0.9605 - val_loss: 1.0502\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9788 - loss: 0.9406 - val_binary_accuracy: 0.9583 - val_loss: 0.9746\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9854 - loss: 0.8661 - val_binary_accuracy: 0.9627 - val_loss: 0.9047\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9868 - loss: 0.8060 - val_binary_accuracy: 0.9627 - val_loss: 0.8439\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9876 - loss: 0.7491 - val_binary_accuracy: 0.9627 - val_loss: 0.7897\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9868 - loss: 0.7005 - val_binary_accuracy: 0.9605 - val_loss: 0.7411\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9825 - loss: 0.6584 - val_binary_accuracy: 0.9649 - val_loss: 0.7013\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9868 - loss: 0.6173 - val_binary_accuracy: 0.9539 - val_loss: 0.7054\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9795 - loss: 0.5952 - val_binary_accuracy: 0.9583 - val_loss: 0.6449\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9737 - loss: 0.5685 - val_binary_accuracy: 0.9649 - val_loss: 0.6022\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.5253 - val_binary_accuracy: 0.9649 - val_loss: 0.5810\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9832 - loss: 0.4963 - val_binary_accuracy: 0.9627 - val_loss: 0.5384\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9839 - loss: 0.4689 - val_binary_accuracy: 0.9627 - val_loss: 0.5236\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9686 - loss: 0.4827 - val_binary_accuracy: 0.9627 - val_loss: 0.4963\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9751 - loss: 0.4350 - val_binary_accuracy: 0.9583 - val_loss: 0.4873\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.568713</td>\n",
              "      <td>77.500946</td>\n",
              "      <td>0.877193</td>\n",
              "      <td>11.054540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.877924</td>\n",
              "      <td>12.088163</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>12.024737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.905702</td>\n",
              "      <td>11.871367</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>10.847569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.946637</td>\n",
              "      <td>10.283791</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>9.572973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.953216</td>\n",
              "      <td>8.811644</td>\n",
              "      <td>0.892544</td>\n",
              "      <td>8.601402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.941520</td>\n",
              "      <td>7.696733</td>\n",
              "      <td>0.796053</td>\n",
              "      <td>8.024966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.910819</td>\n",
              "      <td>6.920317</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>6.312004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.948099</td>\n",
              "      <td>5.866872</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>5.593880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>5.169750</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>5.065873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.959795</td>\n",
              "      <td>4.567506</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>4.313213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>3.977413</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>3.861453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>3.502169</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>3.495945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>3.128959</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>3.061790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>2.815974</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>2.749829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>2.511947</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>2.497748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>2.286127</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>2.405887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>2.121840</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>2.072375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>1.904028</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>1.911803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>1.738671</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.764466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>1.602425</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>1.608466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>1.461848</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.525895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>1.374844</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>1.367461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>1.263778</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>1.289162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>1.182257</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.244252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>1.089575</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>1.226302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.967105</td>\n",
              "      <td>1.032967</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>1.050156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>0.940600</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.974584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.866129</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.904665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.806011</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.843932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.749116</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.789696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.700534</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.741143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.658415</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.701288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.617307</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.705449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.595214</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.644907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.568452</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.602157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.525313</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.581033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.983187</td>\n",
              "      <td>0.496301</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.538385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.468868</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.523607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>0.482738</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.496279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.434973</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.487336</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy       loss  val_binary_accuracy   val_loss\n",
              "0          0.568713  77.500946             0.877193  11.054540\n",
              "1          0.877924  12.088163             0.894737  12.024737\n",
              "2          0.905702  11.871367             0.942982  10.847569\n",
              "3          0.946637  10.283791             0.947368   9.572973\n",
              "4          0.953216   8.811644             0.892544   8.601402\n",
              "5          0.941520   7.696733             0.796053   8.024966\n",
              "6          0.910819   6.920317             0.945175   6.312004\n",
              "7          0.948099   5.866872             0.936404   5.593880\n",
              "8          0.961988   5.169750             0.927632   5.065873\n",
              "9          0.959795   4.567506             0.942982   4.313213\n",
              "10         0.968567   3.977413             0.953947   3.861453\n",
              "11         0.977339   3.502169             0.942982   3.495945\n",
              "12         0.972953   3.128959             0.953947   3.061790\n",
              "13         0.974415   2.815974             0.956140   2.749829\n",
              "14         0.981725   2.511947             0.956140   2.497748\n",
              "15         0.980994   2.286127             0.934211   2.405887\n",
              "16         0.966374   2.121840             0.956140   2.072375\n",
              "17         0.979532   1.904028             0.962719   1.911803\n",
              "18         0.980994   1.738671             0.949561   1.764466\n",
              "19         0.980994   1.602425             0.962719   1.608466\n",
              "20         0.982456   1.461848             0.951754   1.525895\n",
              "21         0.970760   1.374844             0.956140   1.367461\n",
              "22         0.978801   1.263778             0.960526   1.289162\n",
              "23         0.973684   1.182257             0.949561   1.244252\n",
              "24         0.973684   1.089575             0.936404   1.226302\n",
              "25         0.967105   1.032967             0.960526   1.050156\n",
              "26         0.978801   0.940600             0.958333   0.974584\n",
              "27         0.985380   0.866129             0.962719   0.904665\n",
              "28         0.986842   0.806011             0.962719   0.843932\n",
              "29         0.987573   0.749116             0.962719   0.789696\n",
              "30         0.986842   0.700534             0.960526   0.741143\n",
              "31         0.982456   0.658415             0.964912   0.701288\n",
              "32         0.986842   0.617307             0.953947   0.705449\n",
              "33         0.979532   0.595214             0.958333   0.644907\n",
              "34         0.973684   0.568452             0.964912   0.602157\n",
              "35         0.983918   0.525313             0.964912   0.581033\n",
              "36         0.983187   0.496301             0.962719   0.538385\n",
              "37         0.983918   0.468868             0.962719   0.523607\n",
              "38         0.968567   0.482738             0.962719   0.496279\n",
              "39         0.975146   0.434973             0.958333   0.487336"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelL2 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,),kernel_regularizer=tf.keras.regularizers.l2(0.002)),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002)), \n",
        "                    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.002))\n",
        "])\n",
        "\n",
        "modelL2.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "historyL2 = modelL2.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "dfL2 = pd.DataFrame(historyL2.history)\n",
        "dfL2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(historyL2.history['loss'])\n",
        "plt.plot(historyL2.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.982\n"
          ]
        }
      ],
      "source": [
        "get_test_accuracy(modelL2, test_set_x.T, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L2 REGULARIZATION - Penalty Rate 0.003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 342ms/step - binary_accuracy: 0.5848 - loss: 257.7785 - val_binary_accuracy: 0.5285 - val_loss: 47.8823\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.6374 - loss: 27.0520 - val_binary_accuracy: 0.7368 - val_loss: 19.3915\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8355 - loss: 16.9153 - val_binary_accuracy: 0.8816 - val_loss: 15.1565\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8648 - loss: 15.1843 - val_binary_accuracy: 0.9232 - val_loss: 14.4092\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9401 - loss: 13.6578 - val_binary_accuracy: 0.9342 - val_loss: 12.9108\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9518 - loss: 12.0698 - val_binary_accuracy: 0.9452 - val_loss: 11.4272\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9598 - loss: 10.6666 - val_binary_accuracy: 0.9474 - val_loss: 10.0768\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9591 - loss: 9.4501 - val_binary_accuracy: 0.9496 - val_loss: 8.9681\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9693 - loss: 8.4193 - val_binary_accuracy: 0.9561 - val_loss: 8.0006\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9773 - loss: 7.4958 - val_binary_accuracy: 0.9496 - val_loss: 7.2082\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9737 - loss: 6.7462 - val_binary_accuracy: 0.9539 - val_loss: 6.4417\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9708 - loss: 6.0713 - val_binary_accuracy: 0.9539 - val_loss: 5.7974\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9700 - loss: 5.4931 - val_binary_accuracy: 0.9583 - val_loss: 5.2608\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9708 - loss: 4.9812 - val_binary_accuracy: 0.9583 - val_loss: 4.7789\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9766 - loss: 4.5317 - val_binary_accuracy: 0.9342 - val_loss: 4.4562\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9649 - loss: 4.1589 - val_binary_accuracy: 0.9496 - val_loss: 3.9929\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9642 - loss: 3.8196 - val_binary_accuracy: 0.9561 - val_loss: 3.6566\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9664 - loss: 3.4951 - val_binary_accuracy: 0.9583 - val_loss: 3.3604\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9737 - loss: 3.1976 - val_binary_accuracy: 0.9452 - val_loss: 3.1360\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9686 - loss: 2.9706 - val_binary_accuracy: 0.9430 - val_loss: 2.8989\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9686 - loss: 2.7329 - val_binary_accuracy: 0.9518 - val_loss: 2.6500\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9788 - loss: 2.5128 - val_binary_accuracy: 0.9561 - val_loss: 2.4651\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9773 - loss: 2.3180 - val_binary_accuracy: 0.9583 - val_loss: 2.2671\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9839 - loss: 2.1420 - val_binary_accuracy: 0.9627 - val_loss: 2.1094\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9876 - loss: 1.9857 - val_binary_accuracy: 0.9605 - val_loss: 1.9584\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9861 - loss: 1.8457 - val_binary_accuracy: 0.9452 - val_loss: 1.8776\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9678 - loss: 1.7448 - val_binary_accuracy: 0.9605 - val_loss: 1.7082\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9744 - loss: 1.6345 - val_binary_accuracy: 0.9430 - val_loss: 1.6723\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9605 - loss: 1.5603 - val_binary_accuracy: 0.9583 - val_loss: 1.5133\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9627 - loss: 1.4586 - val_binary_accuracy: 0.9496 - val_loss: 1.4840\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9408 - loss: 1.4476 - val_binary_accuracy: 0.9123 - val_loss: 1.5095\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9554 - loss: 1.3585 - val_binary_accuracy: 0.9583 - val_loss: 1.2765\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9686 - loss: 1.2148 - val_binary_accuracy: 0.9474 - val_loss: 1.2344\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9788 - loss: 1.1255 - val_binary_accuracy: 0.9583 - val_loss: 1.1287\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9810 - loss: 1.0448 - val_binary_accuracy: 0.9561 - val_loss: 1.0710\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9781 - loss: 0.9776 - val_binary_accuracy: 0.9583 - val_loss: 0.9891\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.9103 - val_binary_accuracy: 0.9627 - val_loss: 0.9298\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9854 - loss: 0.8532 - val_binary_accuracy: 0.9627 - val_loss: 0.8759\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9876 - loss: 0.8026 - val_binary_accuracy: 0.9671 - val_loss: 0.8268\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9861 - loss: 0.7593 - val_binary_accuracy: 0.9583 - val_loss: 0.7872\n"
          ]
        }
      ],
      "source": [
        "modelL2_2 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,),kernel_regularizer=tf.keras.regularizers.l2(0.003)),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.003)), \n",
        "                    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.003))\n",
        "])\n",
        "\n",
        "modelL2_2.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "historyL2_2 = modelL2_2.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(historyL2_2.history['loss'])\n",
        "plt.plot(historyL2_2.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L2 REGULARIZATION - Penalty Rate 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "6/6 - 2s - 330ms/step - binary_accuracy: 0.4868 - loss: 133.3384 - val_binary_accuracy: 0.4715 - val_loss: 73.6223\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.6133 - loss: 52.9572 - val_binary_accuracy: 0.6184 - val_loss: 36.3986\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.6155 - loss: 28.3279 - val_binary_accuracy: 0.5724 - val_loss: 15.8635\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7200 - loss: 13.8410 - val_binary_accuracy: 0.8355 - val_loss: 10.1554\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.6535 - loss: 10.3547 - val_binary_accuracy: 0.8750 - val_loss: 8.5647\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8977 - loss: 7.7873 - val_binary_accuracy: 0.9167 - val_loss: 5.4369\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9189 - loss: 4.7306 - val_binary_accuracy: 0.9320 - val_loss: 3.7966\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8947 - loss: 3.3853 - val_binary_accuracy: 0.8991 - val_loss: 2.8155\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8224 - loss: 2.9962 - val_binary_accuracy: 0.5504 - val_loss: 3.0014\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.7515 - loss: 2.7923 - val_binary_accuracy: 0.9408 - val_loss: 2.5053\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8925 - loss: 2.3282 - val_binary_accuracy: 0.9408 - val_loss: 2.0350\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9203 - loss: 1.9118 - val_binary_accuracy: 0.4715 - val_loss: 4.3288\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5322 - loss: 3.1926 - val_binary_accuracy: 0.6842 - val_loss: 3.1282\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8670 - loss: 2.6967 - val_binary_accuracy: 0.9408 - val_loss: 2.1806\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9327 - loss: 2.0697 - val_binary_accuracy: 0.9452 - val_loss: 1.9132\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9525 - loss: 1.8370 - val_binary_accuracy: 0.8048 - val_loss: 1.9631\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8092 - loss: 2.0604 - val_binary_accuracy: 0.9101 - val_loss: 2.0500\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9437 - loss: 1.8502 - val_binary_accuracy: 0.9452 - val_loss: 1.5968\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.7749 - loss: 1.9154 - val_binary_accuracy: 0.9342 - val_loss: 2.1790\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8670 - loss: 2.2897 - val_binary_accuracy: 0.8158 - val_loss: 1.9834\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9123 - loss: 1.7455 - val_binary_accuracy: 0.8904 - val_loss: 1.6759\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9218 - loss: 1.5840 - val_binary_accuracy: 0.9320 - val_loss: 1.4867\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9510 - loss: 1.4117 - val_binary_accuracy: 0.6776 - val_loss: 1.7447\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.5570 - loss: 2.4119 - val_binary_accuracy: 0.7895 - val_loss: 2.7092\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7749 - loss: 2.4220 - val_binary_accuracy: 0.9211 - val_loss: 1.7764\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8911 - loss: 1.6837 - val_binary_accuracy: 0.8421 - val_loss: 1.6497\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9196 - loss: 1.6677 - val_binary_accuracy: 0.9254 - val_loss: 1.6406\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9211 - loss: 1.5181 - val_binary_accuracy: 0.5636 - val_loss: 2.5716\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.7792 - loss: 2.3310 - val_binary_accuracy: 0.9101 - val_loss: 2.4690\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9094 - loss: 2.0228 - val_binary_accuracy: 0.9145 - val_loss: 1.5256\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9276 - loss: 1.4503 - val_binary_accuracy: 0.9254 - val_loss: 1.4122\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9481 - loss: 1.2970 - val_binary_accuracy: 0.9408 - val_loss: 1.2136\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8143 - loss: 1.5563 - val_binary_accuracy: 0.9254 - val_loss: 1.8784\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9159 - loss: 1.9731 - val_binary_accuracy: 0.9232 - val_loss: 1.6940\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9386 - loss: 1.4617 - val_binary_accuracy: 0.9101 - val_loss: 1.2790\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9349 - loss: 1.1958 - val_binary_accuracy: 0.8268 - val_loss: 1.3637\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9072 - loss: 1.2448 - val_binary_accuracy: 0.9342 - val_loss: 1.1882\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9459 - loss: 1.1280 - val_binary_accuracy: 0.6667 - val_loss: 1.4490\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.7719 - loss: 1.6916 - val_binary_accuracy: 0.8794 - val_loss: 1.8104\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9108 - loss: 1.6268 - val_binary_accuracy: 0.9342 - val_loss: 1.2810\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.486842</td>\n",
              "      <td>133.338364</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>73.622307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.613304</td>\n",
              "      <td>52.957180</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>36.398636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.615497</td>\n",
              "      <td>28.327860</td>\n",
              "      <td>0.572368</td>\n",
              "      <td>15.863531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.720029</td>\n",
              "      <td>13.840969</td>\n",
              "      <td>0.835526</td>\n",
              "      <td>10.155401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.653509</td>\n",
              "      <td>10.354695</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>8.564673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.897661</td>\n",
              "      <td>7.787309</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>5.436928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.918860</td>\n",
              "      <td>4.730588</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>3.796641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.894737</td>\n",
              "      <td>3.385299</td>\n",
              "      <td>0.899123</td>\n",
              "      <td>2.815495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.822368</td>\n",
              "      <td>2.996224</td>\n",
              "      <td>0.550439</td>\n",
              "      <td>3.001424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.751462</td>\n",
              "      <td>2.792290</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>2.505333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.892544</td>\n",
              "      <td>2.328172</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>2.035019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.920322</td>\n",
              "      <td>1.911778</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>4.328755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.532164</td>\n",
              "      <td>3.192603</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>3.128202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.866959</td>\n",
              "      <td>2.696656</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>2.180555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.932749</td>\n",
              "      <td>2.069725</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.913163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.952485</td>\n",
              "      <td>1.836960</td>\n",
              "      <td>0.804825</td>\n",
              "      <td>1.963124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.809211</td>\n",
              "      <td>2.060386</td>\n",
              "      <td>0.910088</td>\n",
              "      <td>2.050020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.943713</td>\n",
              "      <td>1.850167</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.596766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.774854</td>\n",
              "      <td>1.915398</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>2.178971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.866959</td>\n",
              "      <td>2.289710</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>1.983429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.912281</td>\n",
              "      <td>1.745513</td>\n",
              "      <td>0.890351</td>\n",
              "      <td>1.675861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.921784</td>\n",
              "      <td>1.583963</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>1.486653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.951023</td>\n",
              "      <td>1.411694</td>\n",
              "      <td>0.677632</td>\n",
              "      <td>1.744691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.557018</td>\n",
              "      <td>2.411934</td>\n",
              "      <td>0.789474</td>\n",
              "      <td>2.709152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.774854</td>\n",
              "      <td>2.421996</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>1.776366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.891082</td>\n",
              "      <td>1.683717</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>1.649688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.919591</td>\n",
              "      <td>1.667685</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>1.640582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.921053</td>\n",
              "      <td>1.518139</td>\n",
              "      <td>0.563596</td>\n",
              "      <td>2.571596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.779240</td>\n",
              "      <td>2.331034</td>\n",
              "      <td>0.910088</td>\n",
              "      <td>2.469046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.909357</td>\n",
              "      <td>2.022785</td>\n",
              "      <td>0.914474</td>\n",
              "      <td>1.525580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.927632</td>\n",
              "      <td>1.450301</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>1.412236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.948099</td>\n",
              "      <td>1.297009</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>1.213552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.814327</td>\n",
              "      <td>1.556348</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>1.878446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.915936</td>\n",
              "      <td>1.973109</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>1.694022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.938596</td>\n",
              "      <td>1.461667</td>\n",
              "      <td>0.910088</td>\n",
              "      <td>1.279001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.934942</td>\n",
              "      <td>1.195835</td>\n",
              "      <td>0.826754</td>\n",
              "      <td>1.363682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.907164</td>\n",
              "      <td>1.244764</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>1.188165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.945906</td>\n",
              "      <td>1.128015</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.448996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.771930</td>\n",
              "      <td>1.691607</td>\n",
              "      <td>0.879386</td>\n",
              "      <td>1.810381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.910819</td>\n",
              "      <td>1.626810</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>1.280972</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.486842  133.338364             0.471491  73.622307\n",
              "1          0.613304   52.957180             0.618421  36.398636\n",
              "2          0.615497   28.327860             0.572368  15.863531\n",
              "3          0.720029   13.840969             0.835526  10.155401\n",
              "4          0.653509   10.354695             0.875000   8.564673\n",
              "5          0.897661    7.787309             0.916667   5.436928\n",
              "6          0.918860    4.730588             0.932018   3.796641\n",
              "7          0.894737    3.385299             0.899123   2.815495\n",
              "8          0.822368    2.996224             0.550439   3.001424\n",
              "9          0.751462    2.792290             0.940789   2.505333\n",
              "10         0.892544    2.328172             0.940789   2.035019\n",
              "11         0.920322    1.911778             0.471491   4.328755\n",
              "12         0.532164    3.192603             0.684211   3.128202\n",
              "13         0.866959    2.696656             0.940789   2.180555\n",
              "14         0.932749    2.069725             0.945175   1.913163\n",
              "15         0.952485    1.836960             0.804825   1.963124\n",
              "16         0.809211    2.060386             0.910088   2.050020\n",
              "17         0.943713    1.850167             0.945175   1.596766\n",
              "18         0.774854    1.915398             0.934211   2.178971\n",
              "19         0.866959    2.289710             0.815789   1.983429\n",
              "20         0.912281    1.745513             0.890351   1.675861\n",
              "21         0.921784    1.583963             0.932018   1.486653\n",
              "22         0.951023    1.411694             0.677632   1.744691\n",
              "23         0.557018    2.411934             0.789474   2.709152\n",
              "24         0.774854    2.421996             0.921053   1.776366\n",
              "25         0.891082    1.683717             0.842105   1.649688\n",
              "26         0.919591    1.667685             0.925439   1.640582\n",
              "27         0.921053    1.518139             0.563596   2.571596\n",
              "28         0.779240    2.331034             0.910088   2.469046\n",
              "29         0.909357    2.022785             0.914474   1.525580\n",
              "30         0.927632    1.450301             0.925439   1.412236\n",
              "31         0.948099    1.297009             0.940789   1.213552\n",
              "32         0.814327    1.556348             0.925439   1.878446\n",
              "33         0.915936    1.973109             0.923246   1.694022\n",
              "34         0.938596    1.461667             0.910088   1.279001\n",
              "35         0.934942    1.195835             0.826754   1.363682\n",
              "36         0.907164    1.244764             0.934211   1.188165\n",
              "37         0.945906    1.128015             0.666667   1.448996\n",
              "38         0.771930    1.691607             0.879386   1.810381\n",
              "39         0.910819    1.626810             0.934211   1.280972"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelL2_3 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,),kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)), \n",
        "                    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.1))\n",
        "])\n",
        "\n",
        "modelL2_3.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "historyL2_3 = modelL2_3.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df2_3 = pd.DataFrame(historyL2_3.history)\n",
        "df2_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(historyL2_3.history['loss'])\n",
        "plt.plot(historyL2_3.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L2 REGULARIZATION - Penalty Rate 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 314ms/step - binary_accuracy: 0.5994 - loss: 69.6905 - val_binary_accuracy: 0.8158 - val_loss: 8.0724\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8648 - loss: 7.3945 - val_binary_accuracy: 0.9298 - val_loss: 6.2434\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9342 - loss: 5.9751 - val_binary_accuracy: 0.9320 - val_loss: 6.0165\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9503 - loss: 5.5896 - val_binary_accuracy: 0.9452 - val_loss: 5.3977\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 36ms/step - binary_accuracy: 0.9605 - loss: 4.9700 - val_binary_accuracy: 0.9364 - val_loss: 4.8164\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9627 - loss: 4.5055 - val_binary_accuracy: 0.9539 - val_loss: 4.3212\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9810 - loss: 4.0287 - val_binary_accuracy: 0.9627 - val_loss: 3.9229\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9737 - loss: 3.6548 - val_binary_accuracy: 0.9539 - val_loss: 3.5662\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9788 - loss: 3.3244 - val_binary_accuracy: 0.9474 - val_loss: 3.2440\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9817 - loss: 3.0244 - val_binary_accuracy: 0.9452 - val_loss: 3.0075\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 27ms/step - binary_accuracy: 0.9744 - loss: 2.7965 - val_binary_accuracy: 0.9189 - val_loss: 2.9367\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9554 - loss: 2.6676 - val_binary_accuracy: 0.9408 - val_loss: 2.6471\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9678 - loss: 2.4244 - val_binary_accuracy: 0.9364 - val_loss: 2.4745\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9744 - loss: 2.2460 - val_binary_accuracy: 0.9605 - val_loss: 2.2415\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9766 - loss: 2.0800 - val_binary_accuracy: 0.9496 - val_loss: 2.0976\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9730 - loss: 1.9566 - val_binary_accuracy: 0.9518 - val_loss: 2.0188\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9766 - loss: 1.8264 - val_binary_accuracy: 0.9605 - val_loss: 1.8601\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9868 - loss: 1.6858 - val_binary_accuracy: 0.9605 - val_loss: 1.7227\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9854 - loss: 1.5781 - val_binary_accuracy: 0.9496 - val_loss: 1.6451\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9868 - loss: 1.4814 - val_binary_accuracy: 0.9364 - val_loss: 1.5860\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9737 - loss: 1.4290 - val_binary_accuracy: 0.9364 - val_loss: 1.5132\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9737 - loss: 1.3471 - val_binary_accuracy: 0.9452 - val_loss: 1.4045\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9722 - loss: 1.2722 - val_binary_accuracy: 0.9583 - val_loss: 1.3068\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9876 - loss: 1.1688 - val_binary_accuracy: 0.9539 - val_loss: 1.2327\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9876 - loss: 1.0971 - val_binary_accuracy: 0.9627 - val_loss: 1.1548\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9898 - loss: 1.0305 - val_binary_accuracy: 0.9627 - val_loss: 1.0941\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9912 - loss: 0.9733 - val_binary_accuracy: 0.9627 - val_loss: 1.0320\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9927 - loss: 0.9184 - val_binary_accuracy: 0.9583 - val_loss: 0.9991\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9825 - loss: 0.8853 - val_binary_accuracy: 0.9583 - val_loss: 0.9681\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9832 - loss: 0.8435 - val_binary_accuracy: 0.9627 - val_loss: 0.9010\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9898 - loss: 0.7872 - val_binary_accuracy: 0.9605 - val_loss: 0.8597\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9883 - loss: 0.7508 - val_binary_accuracy: 0.9430 - val_loss: 0.8543\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9832 - loss: 0.7230 - val_binary_accuracy: 0.9408 - val_loss: 0.8292\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9832 - loss: 0.6931 - val_binary_accuracy: 0.9583 - val_loss: 0.7538\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9927 - loss: 0.6425 - val_binary_accuracy: 0.9649 - val_loss: 0.7140\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9934 - loss: 0.6089 - val_binary_accuracy: 0.9583 - val_loss: 0.6854\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9934 - loss: 0.5810 - val_binary_accuracy: 0.9605 - val_loss: 0.6587\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9934 - loss: 0.5520 - val_binary_accuracy: 0.9605 - val_loss: 0.6261\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9934 - loss: 0.5264 - val_binary_accuracy: 0.9605 - val_loss: 0.6029\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9927 - loss: 0.5015 - val_binary_accuracy: 0.9605 - val_loss: 0.5783\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.599415</td>\n",
              "      <td>69.690468</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>8.072395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.864766</td>\n",
              "      <td>7.394483</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>6.243408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.934211</td>\n",
              "      <td>5.975107</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>6.016506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.950292</td>\n",
              "      <td>5.589573</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>5.397685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.960526</td>\n",
              "      <td>4.970037</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>4.816406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>4.505491</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>4.321229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>4.028652</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>3.922852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>3.654835</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>3.566188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>3.324357</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>3.244020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>3.024363</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>3.007546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>2.796548</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>2.936739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.955409</td>\n",
              "      <td>2.667622</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>2.647109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.967836</td>\n",
              "      <td>2.424366</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>2.474540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>2.245996</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>2.241527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>2.080031</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>2.097605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>1.956577</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>2.018785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>1.826362</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>1.860085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>1.685825</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>1.722718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>1.578131</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.645062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>1.481409</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>1.585982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>1.428969</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>1.513167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>1.347087</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.404492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>1.272172</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.306773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>1.168776</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.232672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>1.097133</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>1.154824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>1.030459</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>1.094101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.973257</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>1.031997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.918383</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.999135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.885263</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.968130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.983187</td>\n",
              "      <td>0.843520</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.901008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.787188</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.859722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.750817</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.854305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.983187</td>\n",
              "      <td>0.722955</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.829159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.983187</td>\n",
              "      <td>0.693098</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.753809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.642469</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.713972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.608860</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.685404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.581030</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.658694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.552034</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.626111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.526402</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.602909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.501516</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.578316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy       loss  val_binary_accuracy  val_loss\n",
              "0          0.599415  69.690468             0.815789  8.072395\n",
              "1          0.864766   7.394483             0.929825  6.243408\n",
              "2          0.934211   5.975107             0.932018  6.016506\n",
              "3          0.950292   5.589573             0.945175  5.397685\n",
              "4          0.960526   4.970037             0.936404  4.816406\n",
              "5          0.962719   4.505491             0.953947  4.321229\n",
              "6          0.980994   4.028652             0.962719  3.922852\n",
              "7          0.973684   3.654835             0.953947  3.566188\n",
              "8          0.978801   3.324357             0.947368  3.244020\n",
              "9          0.981725   3.024363             0.945175  3.007546\n",
              "10         0.974415   2.796548             0.918860  2.936739\n",
              "11         0.955409   2.667622             0.940789  2.647109\n",
              "12         0.967836   2.424366             0.936404  2.474540\n",
              "13         0.974415   2.245996             0.960526  2.241527\n",
              "14         0.976608   2.080031             0.949561  2.097605\n",
              "15         0.972953   1.956577             0.951754  2.018785\n",
              "16         0.976608   1.826362             0.960526  1.860085\n",
              "17         0.986842   1.685825             0.960526  1.722718\n",
              "18         0.985380   1.578131             0.949561  1.645062\n",
              "19         0.986842   1.481409             0.936404  1.585982\n",
              "20         0.973684   1.428969             0.936404  1.513167\n",
              "21         0.973684   1.347087             0.945175  1.404492\n",
              "22         0.972222   1.272172             0.958333  1.306773\n",
              "23         0.987573   1.168776             0.953947  1.232672\n",
              "24         0.987573   1.097133             0.962719  1.154824\n",
              "25         0.989766   1.030459             0.962719  1.094101\n",
              "26         0.991228   0.973257             0.962719  1.031997\n",
              "27         0.992690   0.918383             0.958333  0.999135\n",
              "28         0.982456   0.885263             0.958333  0.968130\n",
              "29         0.983187   0.843520             0.962719  0.901008\n",
              "30         0.989766   0.787188             0.960526  0.859722\n",
              "31         0.988304   0.750817             0.942982  0.854305\n",
              "32         0.983187   0.722955             0.940789  0.829159\n",
              "33         0.983187   0.693098             0.958333  0.753809\n",
              "34         0.992690   0.642469             0.964912  0.713972\n",
              "35         0.993421   0.608860             0.958333  0.685404\n",
              "36         0.993421   0.581030             0.960526  0.658694\n",
              "37         0.993421   0.552034             0.960526  0.626111\n",
              "38         0.993421   0.526402             0.960526  0.602909\n",
              "39         0.992690   0.501516             0.960526  0.578316"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelL2_4 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,),kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), \n",
        "                    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "])\n",
        "\n",
        "modelL2_4.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "historyL2_4 = modelL2_4.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df2_4 = pd.DataFrame(historyL2_4.history)\n",
        "df2_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(historyL2_4.history['loss'])\n",
        "plt.plot(historyL2_4.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L2 REGULARIZATION - Penalty Rate 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 400ms/step - binary_accuracy: 0.5702 - loss: 94.1022 - val_binary_accuracy: 0.9013 - val_loss: 36.8534\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 64ms/step - binary_accuracy: 0.6652 - loss: 28.6730 - val_binary_accuracy: 0.9320 - val_loss: 17.0962\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8596 - loss: 14.0324 - val_binary_accuracy: 0.4825 - val_loss: 9.9844\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.5702 - loss: 9.0488 - val_binary_accuracy: 0.7588 - val_loss: 6.9583\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.7478 - loss: 5.8373 - val_binary_accuracy: 0.8246 - val_loss: 3.8654\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8699 - loss: 3.2158 - val_binary_accuracy: 0.9320 - val_loss: 2.3492\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8523 - loss: 2.2222 - val_binary_accuracy: 0.8618 - val_loss: 1.9568\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.8911 - loss: 1.7954 - val_binary_accuracy: 0.8947 - val_loss: 1.5898\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9459 - loss: 1.3993 - val_binary_accuracy: 0.9364 - val_loss: 1.2522\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9642 - loss: 1.1129 - val_binary_accuracy: 0.9276 - val_loss: 1.0757\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8633 - loss: 1.1741 - val_binary_accuracy: 0.8377 - val_loss: 1.2902\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9086 - loss: 1.1713 - val_binary_accuracy: 0.9386 - val_loss: 1.0080\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9547 - loss: 0.9554 - val_binary_accuracy: 0.9539 - val_loss: 0.8945\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8363 - loss: 1.1393 - val_binary_accuracy: 0.8904 - val_loss: 1.1373\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8896 - loss: 1.4496 - val_binary_accuracy: 0.9189 - val_loss: 1.5533\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9437 - loss: 1.3633 - val_binary_accuracy: 0.9342 - val_loss: 1.0970\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9605 - loss: 0.9696 - val_binary_accuracy: 0.9539 - val_loss: 0.9048\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8582 - loss: 1.0158 - val_binary_accuracy: 0.8158 - val_loss: 1.1847\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8757 - loss: 1.2406 - val_binary_accuracy: 0.9320 - val_loss: 1.1242\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9327 - loss: 1.0755 - val_binary_accuracy: 0.9211 - val_loss: 1.0239\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 36ms/step - binary_accuracy: 0.9547 - loss: 0.9213 - val_binary_accuracy: 0.9496 - val_loss: 0.8395\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9613 - loss: 0.8101 - val_binary_accuracy: 0.9452 - val_loss: 0.8319\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9335 - loss: 0.8275 - val_binary_accuracy: 0.8904 - val_loss: 0.9272\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9415 - loss: 0.8099 - val_binary_accuracy: 0.9386 - val_loss: 0.7799\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9649 - loss: 0.7244 - val_binary_accuracy: 0.9539 - val_loss: 0.7141\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9700 - loss: 0.6679 - val_binary_accuracy: 0.9561 - val_loss: 0.6867\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9715 - loss: 0.6455 - val_binary_accuracy: 0.9474 - val_loss: 0.6687\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9591 - loss: 0.6295 - val_binary_accuracy: 0.9101 - val_loss: 0.7601\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9415 - loss: 0.6888 - val_binary_accuracy: 0.9342 - val_loss: 0.7210\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9656 - loss: 0.6516 - val_binary_accuracy: 0.9518 - val_loss: 0.6612\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9700 - loss: 0.6188 - val_binary_accuracy: 0.8969 - val_loss: 0.7477\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.8794 - loss: 0.7816 - val_binary_accuracy: 0.9496 - val_loss: 0.7559\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 27ms/step - binary_accuracy: 0.9137 - loss: 0.8369 - val_binary_accuracy: 0.9254 - val_loss: 0.8320\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 27ms/step - binary_accuracy: 0.9123 - loss: 0.8463 - val_binary_accuracy: 0.9364 - val_loss: 0.7583\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9547 - loss: 0.7067 - val_binary_accuracy: 0.9320 - val_loss: 0.6781\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9686 - loss: 0.6209 - val_binary_accuracy: 0.9518 - val_loss: 0.6226\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9561 - loss: 0.6044 - val_binary_accuracy: 0.9474 - val_loss: 0.6099\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9510 - loss: 0.6147 - val_binary_accuracy: 0.9474 - val_loss: 0.6097\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9371 - loss: 0.6465 - val_binary_accuracy: 0.9364 - val_loss: 0.6449\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9525 - loss: 0.6279 - val_binary_accuracy: 0.9145 - val_loss: 0.7387\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.570175</td>\n",
              "      <td>94.102211</td>\n",
              "      <td>0.901316</td>\n",
              "      <td>36.853397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.665205</td>\n",
              "      <td>28.672985</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>17.096174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.859649</td>\n",
              "      <td>14.032405</td>\n",
              "      <td>0.482456</td>\n",
              "      <td>9.984385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.570175</td>\n",
              "      <td>9.048845</td>\n",
              "      <td>0.758772</td>\n",
              "      <td>6.958334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.747807</td>\n",
              "      <td>5.837348</td>\n",
              "      <td>0.824561</td>\n",
              "      <td>3.865376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.869883</td>\n",
              "      <td>3.215844</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>2.349234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.852339</td>\n",
              "      <td>2.222240</td>\n",
              "      <td>0.861842</td>\n",
              "      <td>1.956847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.891082</td>\n",
              "      <td>1.795396</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>1.589772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.945906</td>\n",
              "      <td>1.399317</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>1.252180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>1.112919</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>1.075737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.863304</td>\n",
              "      <td>1.174148</td>\n",
              "      <td>0.837719</td>\n",
              "      <td>1.290215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.908626</td>\n",
              "      <td>1.171319</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>1.007994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>0.955383</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.894515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.836257</td>\n",
              "      <td>1.139297</td>\n",
              "      <td>0.890351</td>\n",
              "      <td>1.137285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.889620</td>\n",
              "      <td>1.449625</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>1.553305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.943713</td>\n",
              "      <td>1.363303</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>1.096955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.969614</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.904801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.858187</td>\n",
              "      <td>1.015779</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>1.184729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.875731</td>\n",
              "      <td>1.240595</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>1.124229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.932749</td>\n",
              "      <td>1.075493</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>1.023883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>0.921293</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.839476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.961257</td>\n",
              "      <td>0.810148</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.831924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.933480</td>\n",
              "      <td>0.827537</td>\n",
              "      <td>0.890351</td>\n",
              "      <td>0.927245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.941520</td>\n",
              "      <td>0.809852</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.779878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.724413</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.714117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.667895</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.686717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.645491</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.668665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.959064</td>\n",
              "      <td>0.629480</td>\n",
              "      <td>0.910088</td>\n",
              "      <td>0.760140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.941520</td>\n",
              "      <td>0.688813</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.720952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.965643</td>\n",
              "      <td>0.651576</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.661248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.618841</td>\n",
              "      <td>0.896930</td>\n",
              "      <td>0.747667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.879386</td>\n",
              "      <td>0.781579</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.755876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.913743</td>\n",
              "      <td>0.836905</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>0.831983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.846337</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.758267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>0.706745</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.678109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>0.620889</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.622625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.604387</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.609943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.951023</td>\n",
              "      <td>0.614689</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.609706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.937135</td>\n",
              "      <td>0.646457</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.644889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.952485</td>\n",
              "      <td>0.627896</td>\n",
              "      <td>0.914474</td>\n",
              "      <td>0.738691</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy       loss  val_binary_accuracy   val_loss\n",
              "0          0.570175  94.102211             0.901316  36.853397\n",
              "1          0.665205  28.672985             0.932018  17.096174\n",
              "2          0.859649  14.032405             0.482456   9.984385\n",
              "3          0.570175   9.048845             0.758772   6.958334\n",
              "4          0.747807   5.837348             0.824561   3.865376\n",
              "5          0.869883   3.215844             0.932018   2.349234\n",
              "6          0.852339   2.222240             0.861842   1.956847\n",
              "7          0.891082   1.795396             0.894737   1.589772\n",
              "8          0.945906   1.399317             0.936404   1.252180\n",
              "9          0.964181   1.112919             0.927632   1.075737\n",
              "10         0.863304   1.174148             0.837719   1.290215\n",
              "11         0.908626   1.171319             0.938596   1.007994\n",
              "12         0.954678   0.955383             0.953947   0.894515\n",
              "13         0.836257   1.139297             0.890351   1.137285\n",
              "14         0.889620   1.449625             0.918860   1.553305\n",
              "15         0.943713   1.363303             0.934211   1.096955\n",
              "16         0.960526   0.969614             0.953947   0.904801\n",
              "17         0.858187   1.015779             0.815789   1.184729\n",
              "18         0.875731   1.240595             0.932018   1.124229\n",
              "19         0.932749   1.075493             0.921053   1.023883\n",
              "20         0.954678   0.921293             0.949561   0.839476\n",
              "21         0.961257   0.810148             0.945175   0.831924\n",
              "22         0.933480   0.827537             0.890351   0.927245\n",
              "23         0.941520   0.809852             0.938596   0.779878\n",
              "24         0.964912   0.724413             0.953947   0.714117\n",
              "25         0.970029   0.667895             0.956140   0.686717\n",
              "26         0.971491   0.645491             0.947368   0.668665\n",
              "27         0.959064   0.629480             0.910088   0.760140\n",
              "28         0.941520   0.688813             0.934211   0.720952\n",
              "29         0.965643   0.651576             0.951754   0.661248\n",
              "30         0.970029   0.618841             0.896930   0.747667\n",
              "31         0.879386   0.781579             0.949561   0.755876\n",
              "32         0.913743   0.836905             0.925439   0.831983\n",
              "33         0.912281   0.846337             0.936404   0.758267\n",
              "34         0.954678   0.706745             0.932018   0.678109\n",
              "35         0.968567   0.620889             0.951754   0.622625\n",
              "36         0.956140   0.604387             0.947368   0.609943\n",
              "37         0.951023   0.614689             0.947368   0.609706\n",
              "38         0.937135   0.646457             0.936404   0.644889\n",
              "39         0.952485   0.627896             0.914474   0.738691"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelL2_5 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,),kernel_regularizer=tf.keras.regularizers.l2(0.05)),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.05)), \n",
        "                    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.05))\n",
        "])\n",
        "\n",
        "modelL2_5.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "historyL2_5 = modelL2_5.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df2_5 = pd.DataFrame(historyL2_5.history)\n",
        "df2_5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Drop out regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WqyROMhO2B6"
      },
      "source": [
        "DROP OUT REGULARIZATION - Dropout Rate 0.2 all layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-TDvhv9cO3kS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 312ms/step - binary_accuracy: 0.5482 - loss: 207.1017 - val_binary_accuracy: 0.5285 - val_loss: 65.0723\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.5687 - loss: 32.8139 - val_binary_accuracy: 0.5263 - val_loss: 8.4125\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 32ms/step - binary_accuracy: 0.6871 - loss: 6.5206 - val_binary_accuracy: 0.8180 - val_loss: 1.6071\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.7953 - loss: 1.9812 - val_binary_accuracy: 0.9035 - val_loss: 0.7602\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8436 - loss: 1.0511 - val_binary_accuracy: 0.8991 - val_loss: 0.3677\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8289 - loss: 0.6209 - val_binary_accuracy: 0.8202 - val_loss: 0.3670\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8319 - loss: 0.4686 - val_binary_accuracy: 0.9079 - val_loss: 0.2279\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8699 - loss: 0.3445 - val_binary_accuracy: 0.9123 - val_loss: 0.2156\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8567 - loss: 0.3200 - val_binary_accuracy: 0.9145 - val_loss: 0.2345\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8735 - loss: 0.2867 - val_binary_accuracy: 0.9254 - val_loss: 0.2177\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8874 - loss: 0.2635 - val_binary_accuracy: 0.9145 - val_loss: 0.2088\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8904 - loss: 0.2561 - val_binary_accuracy: 0.9320 - val_loss: 0.2143\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.8918 - loss: 0.2330 - val_binary_accuracy: 0.9167 - val_loss: 0.2075\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9064 - loss: 0.2315 - val_binary_accuracy: 0.9167 - val_loss: 0.2045\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9181 - loss: 0.2107 - val_binary_accuracy: 0.9167 - val_loss: 0.2032\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9057 - loss: 0.2174 - val_binary_accuracy: 0.9189 - val_loss: 0.2007\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9079 - loss: 0.2279 - val_binary_accuracy: 0.9189 - val_loss: 0.1978\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9035 - loss: 0.2308 - val_binary_accuracy: 0.9232 - val_loss: 0.2003\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9181 - loss: 0.2169 - val_binary_accuracy: 0.9211 - val_loss: 0.2031\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9130 - loss: 0.2541 - val_binary_accuracy: 0.9232 - val_loss: 0.1988\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9240 - loss: 0.2062 - val_binary_accuracy: 0.9189 - val_loss: 0.1949\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9159 - loss: 0.2096 - val_binary_accuracy: 0.9189 - val_loss: 0.1916\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9269 - loss: 0.2132 - val_binary_accuracy: 0.9276 - val_loss: 0.1896\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9320 - loss: 0.1992 - val_binary_accuracy: 0.9364 - val_loss: 0.1874\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9276 - loss: 0.2005 - val_binary_accuracy: 0.9408 - val_loss: 0.1865\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9284 - loss: 0.2044 - val_binary_accuracy: 0.9430 - val_loss: 0.1845\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9225 - loss: 0.1888 - val_binary_accuracy: 0.9408 - val_loss: 0.1840\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9291 - loss: 0.1818 - val_binary_accuracy: 0.9408 - val_loss: 0.1797\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9423 - loss: 0.1760 - val_binary_accuracy: 0.9408 - val_loss: 0.1788\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9313 - loss: 0.1880 - val_binary_accuracy: 0.9430 - val_loss: 0.1797\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9379 - loss: 0.1886 - val_binary_accuracy: 0.9452 - val_loss: 0.1764\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9364 - loss: 0.1719 - val_binary_accuracy: 0.9452 - val_loss: 0.1710\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9415 - loss: 0.1619 - val_binary_accuracy: 0.9452 - val_loss: 0.1694\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9401 - loss: 0.1695 - val_binary_accuracy: 0.9452 - val_loss: 0.1683\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9430 - loss: 0.1684 - val_binary_accuracy: 0.9452 - val_loss: 0.1665\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9408 - loss: 0.1708 - val_binary_accuracy: 0.9408 - val_loss: 0.1632\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9430 - loss: 0.1528 - val_binary_accuracy: 0.9408 - val_loss: 0.1627\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9496 - loss: 0.1522 - val_binary_accuracy: 0.9408 - val_loss: 0.1612\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9474 - loss: 0.1541 - val_binary_accuracy: 0.9430 - val_loss: 0.1605\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9452 - loss: 0.1444 - val_binary_accuracy: 0.9430 - val_loss: 0.1588\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.548246</td>\n",
              "      <td>207.101746</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>65.072258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.568713</td>\n",
              "      <td>32.813931</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>8.412533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.687135</td>\n",
              "      <td>6.520636</td>\n",
              "      <td>0.817982</td>\n",
              "      <td>1.607143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.795322</td>\n",
              "      <td>1.981167</td>\n",
              "      <td>0.903509</td>\n",
              "      <td>0.760186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.843567</td>\n",
              "      <td>1.051121</td>\n",
              "      <td>0.899123</td>\n",
              "      <td>0.367725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.828947</td>\n",
              "      <td>0.620944</td>\n",
              "      <td>0.820175</td>\n",
              "      <td>0.367024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.831871</td>\n",
              "      <td>0.468612</td>\n",
              "      <td>0.907895</td>\n",
              "      <td>0.227853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.869883</td>\n",
              "      <td>0.344505</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.215625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.856725</td>\n",
              "      <td>0.319997</td>\n",
              "      <td>0.914474</td>\n",
              "      <td>0.234532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.873538</td>\n",
              "      <td>0.286721</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>0.217718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.887427</td>\n",
              "      <td>0.263541</td>\n",
              "      <td>0.914474</td>\n",
              "      <td>0.208799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.890351</td>\n",
              "      <td>0.256101</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.214293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.891813</td>\n",
              "      <td>0.233012</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.207483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.906433</td>\n",
              "      <td>0.231482</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.204532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.918129</td>\n",
              "      <td>0.210666</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.203239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.905702</td>\n",
              "      <td>0.217352</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>0.200659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.907895</td>\n",
              "      <td>0.227933</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>0.197816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.903509</td>\n",
              "      <td>0.230846</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>0.200293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.918129</td>\n",
              "      <td>0.216852</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.203136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.913012</td>\n",
              "      <td>0.254129</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>0.198751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.923977</td>\n",
              "      <td>0.206229</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>0.194949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.915936</td>\n",
              "      <td>0.209627</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>0.191646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.926901</td>\n",
              "      <td>0.213194</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.189571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.199162</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.187436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.200477</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.186532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.928363</td>\n",
              "      <td>0.204430</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.184513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.922515</td>\n",
              "      <td>0.188803</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.184018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.929094</td>\n",
              "      <td>0.181823</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.179727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.942251</td>\n",
              "      <td>0.175986</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.178829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.931287</td>\n",
              "      <td>0.188038</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.179738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.937865</td>\n",
              "      <td>0.188615</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.176377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.171940</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.171027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.941520</td>\n",
              "      <td>0.161930</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.169382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.940058</td>\n",
              "      <td>0.169450</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.168281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.168418</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.166524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.170797</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.163213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.152780</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.162718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.152217</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.161228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.154084</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.160465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.144414</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.158806</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.548246  207.101746             0.528509  65.072258\n",
              "1          0.568713   32.813931             0.526316   8.412533\n",
              "2          0.687135    6.520636             0.817982   1.607143\n",
              "3          0.795322    1.981167             0.903509   0.760186\n",
              "4          0.843567    1.051121             0.899123   0.367725\n",
              "5          0.828947    0.620944             0.820175   0.367024\n",
              "6          0.831871    0.468612             0.907895   0.227853\n",
              "7          0.869883    0.344505             0.912281   0.215625\n",
              "8          0.856725    0.319997             0.914474   0.234532\n",
              "9          0.873538    0.286721             0.925439   0.217718\n",
              "10         0.887427    0.263541             0.914474   0.208799\n",
              "11         0.890351    0.256101             0.932018   0.214293\n",
              "12         0.891813    0.233012             0.916667   0.207483\n",
              "13         0.906433    0.231482             0.916667   0.204532\n",
              "14         0.918129    0.210666             0.916667   0.203239\n",
              "15         0.905702    0.217352             0.918860   0.200659\n",
              "16         0.907895    0.227933             0.918860   0.197816\n",
              "17         0.903509    0.230846             0.923246   0.200293\n",
              "18         0.918129    0.216852             0.921053   0.203136\n",
              "19         0.913012    0.254129             0.923246   0.198751\n",
              "20         0.923977    0.206229             0.918860   0.194949\n",
              "21         0.915936    0.209627             0.918860   0.191646\n",
              "22         0.926901    0.213194             0.927632   0.189571\n",
              "23         0.932018    0.199162             0.936404   0.187436\n",
              "24         0.927632    0.200477             0.940789   0.186532\n",
              "25         0.928363    0.204430             0.942982   0.184513\n",
              "26         0.922515    0.188803             0.940789   0.184018\n",
              "27         0.929094    0.181823             0.940789   0.179727\n",
              "28         0.942251    0.175986             0.940789   0.178829\n",
              "29         0.931287    0.188038             0.942982   0.179738\n",
              "30         0.937865    0.188615             0.945175   0.176377\n",
              "31         0.936404    0.171940             0.945175   0.171027\n",
              "32         0.941520    0.161930             0.945175   0.169382\n",
              "33         0.940058    0.169450             0.945175   0.168281\n",
              "34         0.942982    0.168418             0.945175   0.166524\n",
              "35         0.940789    0.170797             0.940789   0.163213\n",
              "36         0.942982    0.152780             0.940789   0.162718\n",
              "37         0.949561    0.152217             0.940789   0.161228\n",
              "38         0.947368    0.154084             0.942982   0.160465\n",
              "39         0.945175    0.144414             0.942982   0.158806"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dr = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,)),\n",
        "                     Dropout(0.2),\n",
        "                    Dense(48, activation='relu'), \n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_dr.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_dr = model_dr.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_dr = pd.DataFrame(history_dr.history)\n",
        "df_dr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DROP OUT REGULARIZATION - Dropout Rate 0.1 all layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 317ms/step - binary_accuracy: 0.5461 - loss: 136.0684 - val_binary_accuracy: 0.5044 - val_loss: 15.0343\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7178 - loss: 9.2558 - val_binary_accuracy: 0.7873 - val_loss: 4.9008\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8472 - loss: 2.3847 - val_binary_accuracy: 0.8662 - val_loss: 1.2776\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8779 - loss: 1.2276 - val_binary_accuracy: 0.9298 - val_loss: 0.3010\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8575 - loss: 0.5917 - val_binary_accuracy: 0.9430 - val_loss: 0.2117\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9057 - loss: 0.4803 - val_binary_accuracy: 0.9232 - val_loss: 0.2733\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 35ms/step - binary_accuracy: 0.9232 - loss: 0.3254 - val_binary_accuracy: 0.9276 - val_loss: 0.2028\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9050 - loss: 0.3426 - val_binary_accuracy: 0.9342 - val_loss: 0.2058\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9211 - loss: 0.2454 - val_binary_accuracy: 0.9342 - val_loss: 0.2000\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9379 - loss: 0.2378 - val_binary_accuracy: 0.9364 - val_loss: 0.1949\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 28ms/step - binary_accuracy: 0.9379 - loss: 0.2201 - val_binary_accuracy: 0.9474 - val_loss: 0.1760\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9306 - loss: 0.1995 - val_binary_accuracy: 0.9518 - val_loss: 0.1664\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9459 - loss: 0.1884 - val_binary_accuracy: 0.9539 - val_loss: 0.1596\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9481 - loss: 0.1835 - val_binary_accuracy: 0.9474 - val_loss: 0.1585\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9430 - loss: 0.2070 - val_binary_accuracy: 0.9518 - val_loss: 0.1529\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9408 - loss: 0.1919 - val_binary_accuracy: 0.9518 - val_loss: 0.1471\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9459 - loss: 0.1817 - val_binary_accuracy: 0.9539 - val_loss: 0.1438\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9437 - loss: 0.1665 - val_binary_accuracy: 0.9518 - val_loss: 0.1382\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9561 - loss: 0.1443 - val_binary_accuracy: 0.9539 - val_loss: 0.1315\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9554 - loss: 0.1562 - val_binary_accuracy: 0.9561 - val_loss: 0.1276\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9532 - loss: 0.1561 - val_binary_accuracy: 0.9561 - val_loss: 0.1318\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9635 - loss: 0.1530 - val_binary_accuracy: 0.9583 - val_loss: 0.1300\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9561 - loss: 0.1487 - val_binary_accuracy: 0.9583 - val_loss: 0.1259\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9583 - loss: 0.1315 - val_binary_accuracy: 0.9583 - val_loss: 0.1225\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9656 - loss: 0.1294 - val_binary_accuracy: 0.9583 - val_loss: 0.1223\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9642 - loss: 0.1133 - val_binary_accuracy: 0.9605 - val_loss: 0.1224\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9547 - loss: 0.1217 - val_binary_accuracy: 0.9583 - val_loss: 0.1167\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9539 - loss: 0.1306 - val_binary_accuracy: 0.9583 - val_loss: 0.1127\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9576 - loss: 0.1233 - val_binary_accuracy: 0.9583 - val_loss: 0.1111\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9635 - loss: 0.1189 - val_binary_accuracy: 0.9605 - val_loss: 0.1052\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9656 - loss: 0.1134 - val_binary_accuracy: 0.9627 - val_loss: 0.1052\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9642 - loss: 0.1197 - val_binary_accuracy: 0.9605 - val_loss: 0.1053\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9642 - loss: 0.1071 - val_binary_accuracy: 0.9539 - val_loss: 0.1058\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9459 - loss: 0.1359 - val_binary_accuracy: 0.9583 - val_loss: 0.1009\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9686 - loss: 0.0900 - val_binary_accuracy: 0.9627 - val_loss: 0.0993\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9635 - loss: 0.0967 - val_binary_accuracy: 0.9605 - val_loss: 0.1021\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9656 - loss: 0.0995 - val_binary_accuracy: 0.9627 - val_loss: 0.0983\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9700 - loss: 0.0931 - val_binary_accuracy: 0.9627 - val_loss: 0.0977\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9605 - loss: 0.1169 - val_binary_accuracy: 0.9627 - val_loss: 0.0956\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9635 - loss: 0.0941 - val_binary_accuracy: 0.9627 - val_loss: 0.0935\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.546053</td>\n",
              "      <td>136.068436</td>\n",
              "      <td>0.504386</td>\n",
              "      <td>15.034286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.717836</td>\n",
              "      <td>9.255754</td>\n",
              "      <td>0.787281</td>\n",
              "      <td>4.900808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.847222</td>\n",
              "      <td>2.384672</td>\n",
              "      <td>0.866228</td>\n",
              "      <td>1.277582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.877924</td>\n",
              "      <td>1.227555</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>0.300989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.857456</td>\n",
              "      <td>0.591655</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.211693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.905702</td>\n",
              "      <td>0.480319</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>0.273343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.923246</td>\n",
              "      <td>0.325420</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.202816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.904971</td>\n",
              "      <td>0.342602</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.205810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.245381</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.200004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.937865</td>\n",
              "      <td>0.237763</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.194928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.937865</td>\n",
              "      <td>0.220147</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.176048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.930556</td>\n",
              "      <td>0.199508</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.166419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.945906</td>\n",
              "      <td>0.188427</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.159581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.948099</td>\n",
              "      <td>0.183503</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.158475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.207030</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.152860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.191859</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.147143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.945906</td>\n",
              "      <td>0.181651</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.143817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.943713</td>\n",
              "      <td>0.166546</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.138188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.144268</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.131515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.955409</td>\n",
              "      <td>0.156179</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.127605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.953216</td>\n",
              "      <td>0.156073</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.131820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>0.153029</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.129980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.148726</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.125915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.131525</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.122511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.965643</td>\n",
              "      <td>0.129368</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.122279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>0.113280</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.122405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>0.121659</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.116720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.130623</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.112723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.957602</td>\n",
              "      <td>0.123302</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.111056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>0.118863</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.105224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.965643</td>\n",
              "      <td>0.113366</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.105212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>0.119736</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.105329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>0.107089</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.105836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.945906</td>\n",
              "      <td>0.135923</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.100851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>0.089954</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.099309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>0.096691</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.102076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.965643</td>\n",
              "      <td>0.099541</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.098330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.093105</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.097701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.116937</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.095621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>0.094075</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.093536</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.546053  136.068436             0.504386  15.034286\n",
              "1          0.717836    9.255754             0.787281   4.900808\n",
              "2          0.847222    2.384672             0.866228   1.277582\n",
              "3          0.877924    1.227555             0.929825   0.300989\n",
              "4          0.857456    0.591655             0.942982   0.211693\n",
              "5          0.905702    0.480319             0.923246   0.273343\n",
              "6          0.923246    0.325420             0.927632   0.202816\n",
              "7          0.904971    0.342602             0.934211   0.205810\n",
              "8          0.921053    0.245381             0.934211   0.200004\n",
              "9          0.937865    0.237763             0.936404   0.194928\n",
              "10         0.937865    0.220147             0.947368   0.176048\n",
              "11         0.930556    0.199508             0.951754   0.166419\n",
              "12         0.945906    0.188427             0.953947   0.159581\n",
              "13         0.948099    0.183503             0.947368   0.158475\n",
              "14         0.942982    0.207030             0.951754   0.152860\n",
              "15         0.940789    0.191859             0.951754   0.147143\n",
              "16         0.945906    0.181651             0.953947   0.143817\n",
              "17         0.943713    0.166546             0.951754   0.138188\n",
              "18         0.956140    0.144268             0.953947   0.131515\n",
              "19         0.955409    0.156179             0.956140   0.127605\n",
              "20         0.953216    0.156073             0.956140   0.131820\n",
              "21         0.963450    0.153029             0.958333   0.129980\n",
              "22         0.956140    0.148726             0.958333   0.125915\n",
              "23         0.958333    0.131525             0.958333   0.122511\n",
              "24         0.965643    0.129368             0.958333   0.122279\n",
              "25         0.964181    0.113280             0.960526   0.122405\n",
              "26         0.954678    0.121659             0.958333   0.116720\n",
              "27         0.953947    0.130623             0.958333   0.112723\n",
              "28         0.957602    0.123302             0.958333   0.111056\n",
              "29         0.963450    0.118863             0.960526   0.105224\n",
              "30         0.965643    0.113366             0.962719   0.105212\n",
              "31         0.964181    0.119736             0.960526   0.105329\n",
              "32         0.964181    0.107089             0.953947   0.105836\n",
              "33         0.945906    0.135923             0.958333   0.100851\n",
              "34         0.968567    0.089954             0.962719   0.099309\n",
              "35         0.963450    0.096691             0.960526   0.102076\n",
              "36         0.965643    0.099541             0.962719   0.098330\n",
              "37         0.970029    0.093105             0.962719   0.097701\n",
              "38         0.960526    0.116937             0.962719   0.095621\n",
              "39         0.963450    0.094075             0.962719   0.093536"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dr2 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,)),\n",
        "                     Dropout(0.1),\n",
        "                    Dense(48, activation='relu'), \n",
        "                    Dropout(0.1),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_dr2.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_dr2 = model_dr2.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_dr2 = pd.DataFrame(history_dr2.history)\n",
        "df_dr2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history_dr2.history['loss'])\n",
        "plt.plot(history_dr2.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.987\n"
          ]
        }
      ],
      "source": [
        "get_test_accuracy(model_dr2, test_set_x.T, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DROP OUT REGULARIZATION - Dropout Rate 0.2 first hidden layer, 0.1 second hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 328ms/step - binary_accuracy: 0.4876 - loss: 201.8347 - val_binary_accuracy: 0.5373 - val_loss: 35.7204\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.6367 - loss: 16.5480 - val_binary_accuracy: 0.4715 - val_loss: 0.7283\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5607 - loss: 1.9749 - val_binary_accuracy: 0.4715 - val_loss: 0.6573\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.5746 - loss: 0.7128 - val_binary_accuracy: 0.7610 - val_loss: 0.6029\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.7039 - loss: 0.7477 - val_binary_accuracy: 0.7105 - val_loss: 0.6093\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7135 - loss: 0.6420 - val_binary_accuracy: 0.6974 - val_loss: 0.6017\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7325 - loss: 0.5943 - val_binary_accuracy: 0.7610 - val_loss: 0.5539\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7478 - loss: 0.5551 - val_binary_accuracy: 0.7917 - val_loss: 0.5178\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7617 - loss: 0.5326 - val_binary_accuracy: 0.7939 - val_loss: 0.5068\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7683 - loss: 0.5172 - val_binary_accuracy: 0.7939 - val_loss: 0.4992\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7734 - loss: 0.4963 - val_binary_accuracy: 0.7917 - val_loss: 0.4962\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7712 - loss: 0.5255 - val_binary_accuracy: 0.7873 - val_loss: 0.4916\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7749 - loss: 0.4904 - val_binary_accuracy: 0.7939 - val_loss: 0.4838\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7690 - loss: 0.5245 - val_binary_accuracy: 0.7939 - val_loss: 0.4791\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7800 - loss: 0.5124 - val_binary_accuracy: 0.7982 - val_loss: 0.4696\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7770 - loss: 0.4966 - val_binary_accuracy: 0.8092 - val_loss: 0.4596\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.7865 - loss: 0.4569 - val_binary_accuracy: 0.8136 - val_loss: 0.4533\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7858 - loss: 0.4716 - val_binary_accuracy: 0.7982 - val_loss: 0.4609\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.7858 - loss: 0.4574 - val_binary_accuracy: 0.7829 - val_loss: 0.4751\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.7829 - loss: 0.4624 - val_binary_accuracy: 0.7851 - val_loss: 0.4713\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.7822 - loss: 0.4632 - val_binary_accuracy: 0.7982 - val_loss: 0.4567\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7873 - loss: 0.4861 - val_binary_accuracy: 0.8070 - val_loss: 0.4472\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.7895 - loss: 0.4582 - val_binary_accuracy: 0.8136 - val_loss: 0.4417\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7946 - loss: 0.4596 - val_binary_accuracy: 0.8070 - val_loss: 0.4467\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7909 - loss: 0.4581 - val_binary_accuracy: 0.7982 - val_loss: 0.4561\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7917 - loss: 0.4488 - val_binary_accuracy: 0.8070 - val_loss: 0.4495\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7887 - loss: 0.4757 - val_binary_accuracy: 0.8136 - val_loss: 0.4382\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7924 - loss: 0.4668 - val_binary_accuracy: 0.8158 - val_loss: 0.4350\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.7982 - loss: 0.4645 - val_binary_accuracy: 0.8136 - val_loss: 0.4375\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.7880 - loss: 0.4589 - val_binary_accuracy: 0.8070 - val_loss: 0.4432\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7917 - loss: 0.4424 - val_binary_accuracy: 0.8092 - val_loss: 0.4401\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8048 - loss: 0.4273 - val_binary_accuracy: 0.8158 - val_loss: 0.4331\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8019 - loss: 0.4430 - val_binary_accuracy: 0.8224 - val_loss: 0.4243\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8173 - loss: 0.4292 - val_binary_accuracy: 0.8246 - val_loss: 0.4156\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8143 - loss: 0.4304 - val_binary_accuracy: 0.8246 - val_loss: 0.4111\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8158 - loss: 0.4410 - val_binary_accuracy: 0.8246 - val_loss: 0.4080\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8224 - loss: 0.4476 - val_binary_accuracy: 0.8399 - val_loss: 0.4023\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8304 - loss: 0.4282 - val_binary_accuracy: 0.8289 - val_loss: 0.4068\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.8216 - loss: 0.4151 - val_binary_accuracy: 0.8268 - val_loss: 0.4126\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8355 - loss: 0.4177 - val_binary_accuracy: 0.8443 - val_loss: 0.3997\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.487573</td>\n",
              "      <td>201.834747</td>\n",
              "      <td>0.537281</td>\n",
              "      <td>35.720417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.636696</td>\n",
              "      <td>16.548033</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.728337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.560673</td>\n",
              "      <td>1.974916</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.657321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.574561</td>\n",
              "      <td>0.712768</td>\n",
              "      <td>0.760965</td>\n",
              "      <td>0.602927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.703947</td>\n",
              "      <td>0.747703</td>\n",
              "      <td>0.710526</td>\n",
              "      <td>0.609317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.713450</td>\n",
              "      <td>0.642028</td>\n",
              "      <td>0.697368</td>\n",
              "      <td>0.601737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.732456</td>\n",
              "      <td>0.594303</td>\n",
              "      <td>0.760965</td>\n",
              "      <td>0.553923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.747807</td>\n",
              "      <td>0.555052</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.517840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.761696</td>\n",
              "      <td>0.532650</td>\n",
              "      <td>0.793860</td>\n",
              "      <td>0.506762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.768275</td>\n",
              "      <td>0.517174</td>\n",
              "      <td>0.793860</td>\n",
              "      <td>0.499206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.773392</td>\n",
              "      <td>0.496260</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.496225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.771199</td>\n",
              "      <td>0.525521</td>\n",
              "      <td>0.787281</td>\n",
              "      <td>0.491598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.774854</td>\n",
              "      <td>0.490416</td>\n",
              "      <td>0.793860</td>\n",
              "      <td>0.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.769006</td>\n",
              "      <td>0.524537</td>\n",
              "      <td>0.793860</td>\n",
              "      <td>0.479086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.779971</td>\n",
              "      <td>0.512374</td>\n",
              "      <td>0.798246</td>\n",
              "      <td>0.469607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.777047</td>\n",
              "      <td>0.496560</td>\n",
              "      <td>0.809211</td>\n",
              "      <td>0.459614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.786550</td>\n",
              "      <td>0.456862</td>\n",
              "      <td>0.813596</td>\n",
              "      <td>0.453319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.785819</td>\n",
              "      <td>0.471647</td>\n",
              "      <td>0.798246</td>\n",
              "      <td>0.460871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.785819</td>\n",
              "      <td>0.457429</td>\n",
              "      <td>0.782895</td>\n",
              "      <td>0.475115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.782895</td>\n",
              "      <td>0.462419</td>\n",
              "      <td>0.785088</td>\n",
              "      <td>0.471288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.782164</td>\n",
              "      <td>0.463226</td>\n",
              "      <td>0.798246</td>\n",
              "      <td>0.456700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.787281</td>\n",
              "      <td>0.486150</td>\n",
              "      <td>0.807018</td>\n",
              "      <td>0.447226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.789474</td>\n",
              "      <td>0.458245</td>\n",
              "      <td>0.813596</td>\n",
              "      <td>0.441684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.794591</td>\n",
              "      <td>0.459578</td>\n",
              "      <td>0.807018</td>\n",
              "      <td>0.446687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.790936</td>\n",
              "      <td>0.458060</td>\n",
              "      <td>0.798246</td>\n",
              "      <td>0.456109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.448811</td>\n",
              "      <td>0.807018</td>\n",
              "      <td>0.449541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.788743</td>\n",
              "      <td>0.475722</td>\n",
              "      <td>0.813596</td>\n",
              "      <td>0.438216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.792398</td>\n",
              "      <td>0.466806</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.435011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.798246</td>\n",
              "      <td>0.464519</td>\n",
              "      <td>0.813596</td>\n",
              "      <td>0.437455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.788012</td>\n",
              "      <td>0.458876</td>\n",
              "      <td>0.807018</td>\n",
              "      <td>0.443185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.442395</td>\n",
              "      <td>0.809211</td>\n",
              "      <td>0.440117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.804825</td>\n",
              "      <td>0.427254</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.433111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.801901</td>\n",
              "      <td>0.443043</td>\n",
              "      <td>0.822368</td>\n",
              "      <td>0.424267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.817251</td>\n",
              "      <td>0.429223</td>\n",
              "      <td>0.824561</td>\n",
              "      <td>0.415559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.814327</td>\n",
              "      <td>0.430365</td>\n",
              "      <td>0.824561</td>\n",
              "      <td>0.411058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.441050</td>\n",
              "      <td>0.824561</td>\n",
              "      <td>0.408039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.822368</td>\n",
              "      <td>0.447557</td>\n",
              "      <td>0.839912</td>\n",
              "      <td>0.402264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.830409</td>\n",
              "      <td>0.428242</td>\n",
              "      <td>0.828947</td>\n",
              "      <td>0.406830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.821637</td>\n",
              "      <td>0.415140</td>\n",
              "      <td>0.826754</td>\n",
              "      <td>0.412618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.835526</td>\n",
              "      <td>0.417668</td>\n",
              "      <td>0.844298</td>\n",
              "      <td>0.399653</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.487573  201.834747             0.537281  35.720417\n",
              "1          0.636696   16.548033             0.471491   0.728337\n",
              "2          0.560673    1.974916             0.471491   0.657321\n",
              "3          0.574561    0.712768             0.760965   0.602927\n",
              "4          0.703947    0.747703             0.710526   0.609317\n",
              "5          0.713450    0.642028             0.697368   0.601737\n",
              "6          0.732456    0.594303             0.760965   0.553923\n",
              "7          0.747807    0.555052             0.791667   0.517840\n",
              "8          0.761696    0.532650             0.793860   0.506762\n",
              "9          0.768275    0.517174             0.793860   0.499206\n",
              "10         0.773392    0.496260             0.791667   0.496225\n",
              "11         0.771199    0.525521             0.787281   0.491598\n",
              "12         0.774854    0.490416             0.793860   0.483800\n",
              "13         0.769006    0.524537             0.793860   0.479086\n",
              "14         0.779971    0.512374             0.798246   0.469607\n",
              "15         0.777047    0.496560             0.809211   0.459614\n",
              "16         0.786550    0.456862             0.813596   0.453319\n",
              "17         0.785819    0.471647             0.798246   0.460871\n",
              "18         0.785819    0.457429             0.782895   0.475115\n",
              "19         0.782895    0.462419             0.785088   0.471288\n",
              "20         0.782164    0.463226             0.798246   0.456700\n",
              "21         0.787281    0.486150             0.807018   0.447226\n",
              "22         0.789474    0.458245             0.813596   0.441684\n",
              "23         0.794591    0.459578             0.807018   0.446687\n",
              "24         0.790936    0.458060             0.798246   0.456109\n",
              "25         0.791667    0.448811             0.807018   0.449541\n",
              "26         0.788743    0.475722             0.813596   0.438216\n",
              "27         0.792398    0.466806             0.815789   0.435011\n",
              "28         0.798246    0.464519             0.813596   0.437455\n",
              "29         0.788012    0.458876             0.807018   0.443185\n",
              "30         0.791667    0.442395             0.809211   0.440117\n",
              "31         0.804825    0.427254             0.815789   0.433111\n",
              "32         0.801901    0.443043             0.822368   0.424267\n",
              "33         0.817251    0.429223             0.824561   0.415559\n",
              "34         0.814327    0.430365             0.824561   0.411058\n",
              "35         0.815789    0.441050             0.824561   0.408039\n",
              "36         0.822368    0.447557             0.839912   0.402264\n",
              "37         0.830409    0.428242             0.828947   0.406830\n",
              "38         0.821637    0.415140             0.826754   0.412618\n",
              "39         0.835526    0.417668             0.844298   0.399653"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dr3 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,)),\n",
        "                     Dropout(0.2),\n",
        "                    Dense(48, activation='relu'), \n",
        "                    Dropout(0.1),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_dr3.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_dr3 = model_dr3.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_dr3 = pd.DataFrame(history_dr3.history)\n",
        "df_dr3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DROP OUT REGULARIZATION - Dropout Rate 0.1 first hidden layer, 0.2 second hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 323ms/step - binary_accuracy: 0.5080 - loss: 175.8882 - val_binary_accuracy: 0.5285 - val_loss: 26.0923\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 36ms/step - binary_accuracy: 0.6652 - loss: 14.4306 - val_binary_accuracy: 0.8465 - val_loss: 4.7855\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8238 - loss: 5.6251 - val_binary_accuracy: 0.8882 - val_loss: 3.0066\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8538 - loss: 3.1080 - val_binary_accuracy: 0.9430 - val_loss: 0.7758\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8933 - loss: 1.2839 - val_binary_accuracy: 0.9430 - val_loss: 0.2778\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9057 - loss: 0.5522 - val_binary_accuracy: 0.9079 - val_loss: 0.2615\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8889 - loss: 0.4184 - val_binary_accuracy: 0.9211 - val_loss: 0.2408\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9028 - loss: 0.2707 - val_binary_accuracy: 0.9123 - val_loss: 0.2351\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9057 - loss: 0.2967 - val_binary_accuracy: 0.9211 - val_loss: 0.2192\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8867 - loss: 0.3020 - val_binary_accuracy: 0.9276 - val_loss: 0.2152\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9028 - loss: 0.2609 - val_binary_accuracy: 0.9320 - val_loss: 0.2214\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9013 - loss: 0.2587 - val_binary_accuracy: 0.9342 - val_loss: 0.2152\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9072 - loss: 0.2784 - val_binary_accuracy: 0.9342 - val_loss: 0.2021\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9035 - loss: 0.2750 - val_binary_accuracy: 0.9386 - val_loss: 0.1973\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9072 - loss: 0.2429 - val_binary_accuracy: 0.9364 - val_loss: 0.2002\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9159 - loss: 0.2526 - val_binary_accuracy: 0.9364 - val_loss: 0.1961\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9108 - loss: 0.2739 - val_binary_accuracy: 0.9408 - val_loss: 0.1904\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9211 - loss: 0.2550 - val_binary_accuracy: 0.9408 - val_loss: 0.1874\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9108 - loss: 0.2359 - val_binary_accuracy: 0.9408 - val_loss: 0.1828\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9262 - loss: 0.1935 - val_binary_accuracy: 0.9408 - val_loss: 0.1728\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9232 - loss: 0.2355 - val_binary_accuracy: 0.9408 - val_loss: 0.1734\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9313 - loss: 0.2042 - val_binary_accuracy: 0.9430 - val_loss: 0.1762\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9298 - loss: 0.2120 - val_binary_accuracy: 0.9408 - val_loss: 0.1703\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9291 - loss: 0.2058 - val_binary_accuracy: 0.9364 - val_loss: 0.1698\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9247 - loss: 0.1924 - val_binary_accuracy: 0.9386 - val_loss: 0.1683\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9254 - loss: 0.2298 - val_binary_accuracy: 0.9364 - val_loss: 0.1713\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9291 - loss: 0.1945 - val_binary_accuracy: 0.9364 - val_loss: 0.1691\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9240 - loss: 0.1944 - val_binary_accuracy: 0.9364 - val_loss: 0.1714\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9240 - loss: 0.2131 - val_binary_accuracy: 0.9364 - val_loss: 0.1694\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9269 - loss: 0.1918 - val_binary_accuracy: 0.9408 - val_loss: 0.1667\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9320 - loss: 0.1818 - val_binary_accuracy: 0.9386 - val_loss: 0.1763\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9349 - loss: 0.1872 - val_binary_accuracy: 0.9386 - val_loss: 0.1692\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9371 - loss: 0.1861 - val_binary_accuracy: 0.9386 - val_loss: 0.1616\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9408 - loss: 0.1657 - val_binary_accuracy: 0.9430 - val_loss: 0.1618\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9386 - loss: 0.1752 - val_binary_accuracy: 0.9430 - val_loss: 0.1727\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9393 - loss: 0.1848 - val_binary_accuracy: 0.9430 - val_loss: 0.1695\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9349 - loss: 0.1819 - val_binary_accuracy: 0.9386 - val_loss: 0.1658\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9371 - loss: 0.1851 - val_binary_accuracy: 0.9408 - val_loss: 0.1630\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9437 - loss: 0.1597 - val_binary_accuracy: 0.9430 - val_loss: 0.1650\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9393 - loss: 0.1587 - val_binary_accuracy: 0.9408 - val_loss: 0.1615\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.508041</td>\n",
              "      <td>175.888229</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>26.092272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.665205</td>\n",
              "      <td>14.430641</td>\n",
              "      <td>0.846491</td>\n",
              "      <td>4.785539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.823830</td>\n",
              "      <td>5.625150</td>\n",
              "      <td>0.888158</td>\n",
              "      <td>3.006613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.853801</td>\n",
              "      <td>3.107956</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.775812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.893275</td>\n",
              "      <td>1.283931</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.277750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.905702</td>\n",
              "      <td>0.552183</td>\n",
              "      <td>0.907895</td>\n",
              "      <td>0.261469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.418410</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.240787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.270689</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.235132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.905702</td>\n",
              "      <td>0.296672</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.219223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.886696</td>\n",
              "      <td>0.301956</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.215159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.260938</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.221364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.901316</td>\n",
              "      <td>0.258698</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.215246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.907164</td>\n",
              "      <td>0.278389</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.202076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.903509</td>\n",
              "      <td>0.274982</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.197292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.907164</td>\n",
              "      <td>0.242871</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.200228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.915936</td>\n",
              "      <td>0.252585</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.196089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.910819</td>\n",
              "      <td>0.273915</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.190444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.254989</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.187359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.910819</td>\n",
              "      <td>0.235866</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.182840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.926170</td>\n",
              "      <td>0.193547</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.172788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.923246</td>\n",
              "      <td>0.235486</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.173414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.931287</td>\n",
              "      <td>0.204246</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.176180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.929825</td>\n",
              "      <td>0.211969</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.170268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.929094</td>\n",
              "      <td>0.205764</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.169766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.924708</td>\n",
              "      <td>0.192426</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.168251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.925439</td>\n",
              "      <td>0.229810</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.171265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.929094</td>\n",
              "      <td>0.194497</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.169084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.923977</td>\n",
              "      <td>0.194387</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.171384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.923977</td>\n",
              "      <td>0.213124</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.169381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.926901</td>\n",
              "      <td>0.191820</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.166669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.181806</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.176252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.934942</td>\n",
              "      <td>0.187187</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.169235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.937135</td>\n",
              "      <td>0.186148</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.161601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.165700</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.161841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.175217</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.172660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.939327</td>\n",
              "      <td>0.184778</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.169522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.934942</td>\n",
              "      <td>0.181910</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.165757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.937135</td>\n",
              "      <td>0.185067</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.162957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.943713</td>\n",
              "      <td>0.159735</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.164983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.939327</td>\n",
              "      <td>0.158740</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.161532</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.508041  175.888229             0.528509  26.092272\n",
              "1          0.665205   14.430641             0.846491   4.785539\n",
              "2          0.823830    5.625150             0.888158   3.006613\n",
              "3          0.853801    3.107956             0.942982   0.775812\n",
              "4          0.893275    1.283931             0.942982   0.277750\n",
              "5          0.905702    0.552183             0.907895   0.261469\n",
              "6          0.888889    0.418410             0.921053   0.240787\n",
              "7          0.902778    0.270689             0.912281   0.235132\n",
              "8          0.905702    0.296672             0.921053   0.219223\n",
              "9          0.886696    0.301956             0.927632   0.215159\n",
              "10         0.902778    0.260938             0.932018   0.221364\n",
              "11         0.901316    0.258698             0.934211   0.215246\n",
              "12         0.907164    0.278389             0.934211   0.202076\n",
              "13         0.903509    0.274982             0.938596   0.197292\n",
              "14         0.907164    0.242871             0.936404   0.200228\n",
              "15         0.915936    0.252585             0.936404   0.196089\n",
              "16         0.910819    0.273915             0.940789   0.190444\n",
              "17         0.921053    0.254989             0.940789   0.187359\n",
              "18         0.910819    0.235866             0.940789   0.182840\n",
              "19         0.926170    0.193547             0.940789   0.172788\n",
              "20         0.923246    0.235486             0.940789   0.173414\n",
              "21         0.931287    0.204246             0.942982   0.176180\n",
              "22         0.929825    0.211969             0.940789   0.170268\n",
              "23         0.929094    0.205764             0.936404   0.169766\n",
              "24         0.924708    0.192426             0.938596   0.168251\n",
              "25         0.925439    0.229810             0.936404   0.171265\n",
              "26         0.929094    0.194497             0.936404   0.169084\n",
              "27         0.923977    0.194387             0.936404   0.171384\n",
              "28         0.923977    0.213124             0.936404   0.169381\n",
              "29         0.926901    0.191820             0.940789   0.166669\n",
              "30         0.932018    0.181806             0.938596   0.176252\n",
              "31         0.934942    0.187187             0.938596   0.169235\n",
              "32         0.937135    0.186148             0.938596   0.161601\n",
              "33         0.940789    0.165700             0.942982   0.161841\n",
              "34         0.938596    0.175217             0.942982   0.172660\n",
              "35         0.939327    0.184778             0.942982   0.169522\n",
              "36         0.934942    0.181910             0.938596   0.165757\n",
              "37         0.937135    0.185067             0.940789   0.162957\n",
              "38         0.943713    0.159735             0.942982   0.164983\n",
              "39         0.939327    0.158740             0.940789   0.161532"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dr4 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,)),\n",
        "                     Dropout(0.1),\n",
        "                    Dense(48, activation='relu'), \n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_dr4.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_dr4 = model_dr4.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_dr4 = pd.DataFrame(history_dr4.history)\n",
        "df_dr4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DROP OUT REGULARIZATION - Dropout Rate 0.25 all layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 322ms/step - binary_accuracy: 0.5636 - loss: 153.5634 - val_binary_accuracy: 0.5285 - val_loss: 32.1222\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.5548 - loss: 18.0567 - val_binary_accuracy: 0.4715 - val_loss: 5.3567\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5950 - loss: 2.3105 - val_binary_accuracy: 0.4737 - val_loss: 0.6950\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.5782 - loss: 0.7723 - val_binary_accuracy: 0.4715 - val_loss: 0.6938\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5731 - loss: 0.7377 - val_binary_accuracy: 0.4715 - val_loss: 0.6927\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5738 - loss: 0.7300 - val_binary_accuracy: 0.4737 - val_loss: 0.6726\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5716 - loss: 0.6569 - val_binary_accuracy: 0.4825 - val_loss: 0.5969\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.6016 - loss: 0.6264 - val_binary_accuracy: 0.5154 - val_loss: 0.6155\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.5819 - loss: 0.6573 - val_binary_accuracy: 0.5768 - val_loss: 0.5253\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.6045 - loss: 0.5686 - val_binary_accuracy: 0.7127 - val_loss: 0.4761\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7266 - loss: 0.5331 - val_binary_accuracy: 0.8816 - val_loss: 0.4132\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.7719 - loss: 0.5354 - val_binary_accuracy: 0.8750 - val_loss: 0.3899\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.7683 - loss: 0.4740 - val_binary_accuracy: 0.9057 - val_loss: 0.3442\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7880 - loss: 0.4727 - val_binary_accuracy: 0.9035 - val_loss: 0.3156\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7712 - loss: 0.5067 - val_binary_accuracy: 0.8333 - val_loss: 0.4018\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7507 - loss: 0.4766 - val_binary_accuracy: 0.8947 - val_loss: 0.3388\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.7705 - loss: 0.4748 - val_binary_accuracy: 0.8969 - val_loss: 0.3044\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.7719 - loss: 0.4423 - val_binary_accuracy: 0.9057 - val_loss: 0.2846\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8099 - loss: 0.4081 - val_binary_accuracy: 0.9167 - val_loss: 0.2813\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8012 - loss: 0.4234 - val_binary_accuracy: 0.9035 - val_loss: 0.2759\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8063 - loss: 0.4282 - val_binary_accuracy: 0.9211 - val_loss: 0.2591\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8114 - loss: 0.4213 - val_binary_accuracy: 0.9211 - val_loss: 0.2441\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8121 - loss: 0.4040 - val_binary_accuracy: 0.9232 - val_loss: 0.2650\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8180 - loss: 0.4015 - val_binary_accuracy: 0.9189 - val_loss: 0.2563\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8004 - loss: 0.4033 - val_binary_accuracy: 0.9386 - val_loss: 0.2233\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8275 - loss: 0.3927 - val_binary_accuracy: 0.9364 - val_loss: 0.2524\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8268 - loss: 0.3791 - val_binary_accuracy: 0.9430 - val_loss: 0.2369\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8509 - loss: 0.3439 - val_binary_accuracy: 0.9430 - val_loss: 0.1977\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8326 - loss: 0.3704 - val_binary_accuracy: 0.9452 - val_loss: 0.1979\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8494 - loss: 0.3478 - val_binary_accuracy: 0.9496 - val_loss: 0.2188\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8370 - loss: 0.3596 - val_binary_accuracy: 0.9452 - val_loss: 0.2208\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8385 - loss: 0.3544 - val_binary_accuracy: 0.9539 - val_loss: 0.2201\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8465 - loss: 0.3552 - val_binary_accuracy: 0.9539 - val_loss: 0.1856\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8472 - loss: 0.3590 - val_binary_accuracy: 0.9452 - val_loss: 0.1869\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8640 - loss: 0.3207 - val_binary_accuracy: 0.9539 - val_loss: 0.1879\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8545 - loss: 0.3314 - val_binary_accuracy: 0.9496 - val_loss: 0.1966\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8765 - loss: 0.3090 - val_binary_accuracy: 0.9627 - val_loss: 0.1656\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8692 - loss: 0.2997 - val_binary_accuracy: 0.9518 - val_loss: 0.1699\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.8743 - loss: 0.3161 - val_binary_accuracy: 0.9671 - val_loss: 0.1650\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9006 - loss: 0.2988 - val_binary_accuracy: 0.9605 - val_loss: 0.1603\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.563596</td>\n",
              "      <td>153.563400</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>32.122181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.554825</td>\n",
              "      <td>18.056665</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>5.356693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.595029</td>\n",
              "      <td>2.310505</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.694989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.578216</td>\n",
              "      <td>0.772271</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.693774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.573099</td>\n",
              "      <td>0.737654</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>0.692745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.573830</td>\n",
              "      <td>0.729986</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.672647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.571637</td>\n",
              "      <td>0.656891</td>\n",
              "      <td>0.482456</td>\n",
              "      <td>0.596867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.601608</td>\n",
              "      <td>0.626371</td>\n",
              "      <td>0.515351</td>\n",
              "      <td>0.615522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.581871</td>\n",
              "      <td>0.657303</td>\n",
              "      <td>0.576754</td>\n",
              "      <td>0.525262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.604532</td>\n",
              "      <td>0.568567</td>\n",
              "      <td>0.712719</td>\n",
              "      <td>0.476091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.726608</td>\n",
              "      <td>0.533129</td>\n",
              "      <td>0.881579</td>\n",
              "      <td>0.413170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.771930</td>\n",
              "      <td>0.535408</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.389871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.768275</td>\n",
              "      <td>0.474000</td>\n",
              "      <td>0.905702</td>\n",
              "      <td>0.344185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.788012</td>\n",
              "      <td>0.472729</td>\n",
              "      <td>0.903509</td>\n",
              "      <td>0.315565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.771199</td>\n",
              "      <td>0.506658</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.401769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.750731</td>\n",
              "      <td>0.476594</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0.338830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.770468</td>\n",
              "      <td>0.474835</td>\n",
              "      <td>0.896930</td>\n",
              "      <td>0.304421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.771930</td>\n",
              "      <td>0.442251</td>\n",
              "      <td>0.905702</td>\n",
              "      <td>0.284593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.809942</td>\n",
              "      <td>0.408072</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.281302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.801170</td>\n",
              "      <td>0.423413</td>\n",
              "      <td>0.903509</td>\n",
              "      <td>0.275881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.806287</td>\n",
              "      <td>0.428166</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.259107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.811404</td>\n",
              "      <td>0.421345</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.244076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.812135</td>\n",
              "      <td>0.403996</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>0.264976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.817982</td>\n",
              "      <td>0.401468</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>0.256280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.800439</td>\n",
              "      <td>0.403334</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.223285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.827485</td>\n",
              "      <td>0.392695</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.252351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.826754</td>\n",
              "      <td>0.379110</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.236919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.850877</td>\n",
              "      <td>0.343856</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.197655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.832602</td>\n",
              "      <td>0.370352</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.197914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.849415</td>\n",
              "      <td>0.347797</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.218839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.836988</td>\n",
              "      <td>0.359630</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.220817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.838450</td>\n",
              "      <td>0.354446</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.220113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.846491</td>\n",
              "      <td>0.355204</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.185634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.359044</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.186904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.864035</td>\n",
              "      <td>0.320743</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.187939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.854532</td>\n",
              "      <td>0.331383</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.196609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.876462</td>\n",
              "      <td>0.308980</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.165626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.869152</td>\n",
              "      <td>0.299650</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.169890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.874269</td>\n",
              "      <td>0.316091</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.164984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.900585</td>\n",
              "      <td>0.298777</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.160281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.563596  153.563400             0.528509  32.122181\n",
              "1          0.554825   18.056665             0.471491   5.356693\n",
              "2          0.595029    2.310505             0.473684   0.694989\n",
              "3          0.578216    0.772271             0.471491   0.693774\n",
              "4          0.573099    0.737654             0.471491   0.692745\n",
              "5          0.573830    0.729986             0.473684   0.672647\n",
              "6          0.571637    0.656891             0.482456   0.596867\n",
              "7          0.601608    0.626371             0.515351   0.615522\n",
              "8          0.581871    0.657303             0.576754   0.525262\n",
              "9          0.604532    0.568567             0.712719   0.476091\n",
              "10         0.726608    0.533129             0.881579   0.413170\n",
              "11         0.771930    0.535408             0.875000   0.389871\n",
              "12         0.768275    0.474000             0.905702   0.344185\n",
              "13         0.788012    0.472729             0.903509   0.315565\n",
              "14         0.771199    0.506658             0.833333   0.401769\n",
              "15         0.750731    0.476594             0.894737   0.338830\n",
              "16         0.770468    0.474835             0.896930   0.304421\n",
              "17         0.771930    0.442251             0.905702   0.284593\n",
              "18         0.809942    0.408072             0.916667   0.281302\n",
              "19         0.801170    0.423413             0.903509   0.275881\n",
              "20         0.806287    0.428166             0.921053   0.259107\n",
              "21         0.811404    0.421345             0.921053   0.244076\n",
              "22         0.812135    0.403996             0.923246   0.264976\n",
              "23         0.817982    0.401468             0.918860   0.256280\n",
              "24         0.800439    0.403334             0.938596   0.223285\n",
              "25         0.827485    0.392695             0.936404   0.252351\n",
              "26         0.826754    0.379110             0.942982   0.236919\n",
              "27         0.850877    0.343856             0.942982   0.197655\n",
              "28         0.832602    0.370352             0.945175   0.197914\n",
              "29         0.849415    0.347797             0.949561   0.218839\n",
              "30         0.836988    0.359630             0.945175   0.220817\n",
              "31         0.838450    0.354446             0.953947   0.220113\n",
              "32         0.846491    0.355204             0.953947   0.185634\n",
              "33         0.847222    0.359044             0.945175   0.186904\n",
              "34         0.864035    0.320743             0.953947   0.187939\n",
              "35         0.854532    0.331383             0.949561   0.196609\n",
              "36         0.876462    0.308980             0.962719   0.165626\n",
              "37         0.869152    0.299650             0.951754   0.169890\n",
              "38         0.874269    0.316091             0.967105   0.164984\n",
              "39         0.900585    0.298777             0.960526   0.160281"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dr5 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,)),\n",
        "                     Dropout(0.25),\n",
        "                    Dense(48, activation='relu'), \n",
        "                    Dropout(0.25),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_dr5.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_dr5 = model_dr5.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_dr5 = pd.DataFrame(history_dr5.history)\n",
        "df_dr5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Combination L2 & Dropout "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combination L2 & Dropout - 0.002 Penalty Rate and 0.1 Dropout Rate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 346ms/step - binary_accuracy: 0.6023 - loss: 127.0240 - val_binary_accuracy: 0.8904 - val_loss: 10.0177\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 31ms/step - binary_accuracy: 0.8428 - loss: 11.2996 - val_binary_accuracy: 0.8947 - val_loss: 9.8793\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8604 - loss: 10.0753 - val_binary_accuracy: 0.8991 - val_loss: 9.4925\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9101 - loss: 8.8136 - val_binary_accuracy: 0.9386 - val_loss: 7.9969\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9276 - loss: 7.6501 - val_binary_accuracy: 0.9430 - val_loss: 7.0043\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9488 - loss: 6.6022 - val_binary_accuracy: 0.9408 - val_loss: 6.1268\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9525 - loss: 5.8439 - val_binary_accuracy: 0.9430 - val_loss: 5.4746\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9503 - loss: 5.2373 - val_binary_accuracy: 0.9496 - val_loss: 4.9520\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9474 - loss: 4.7266 - val_binary_accuracy: 0.9452 - val_loss: 4.4648\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9620 - loss: 4.2861 - val_binary_accuracy: 0.9561 - val_loss: 4.1112\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9423 - loss: 3.9553 - val_binary_accuracy: 0.9232 - val_loss: 3.8620\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9481 - loss: 3.6422 - val_binary_accuracy: 0.9496 - val_loss: 3.5294\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9532 - loss: 3.3859 - val_binary_accuracy: 0.9518 - val_loss: 3.2629\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9598 - loss: 3.1373 - val_binary_accuracy: 0.9496 - val_loss: 3.0716\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9605 - loss: 2.9266 - val_binary_accuracy: 0.9518 - val_loss: 2.8550\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9642 - loss: 2.7324 - val_binary_accuracy: 0.9539 - val_loss: 2.6524\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9678 - loss: 2.5538 - val_binary_accuracy: 0.9583 - val_loss: 2.4968\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9620 - loss: 2.4104 - val_binary_accuracy: 0.9583 - val_loss: 2.3370\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9737 - loss: 2.2576 - val_binary_accuracy: 0.9627 - val_loss: 2.1976\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9773 - loss: 2.1183 - val_binary_accuracy: 0.9496 - val_loss: 2.1091\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9700 - loss: 2.0157 - val_binary_accuracy: 0.9561 - val_loss: 1.9760\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9635 - loss: 1.9211 - val_binary_accuracy: 0.9539 - val_loss: 1.9061\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9759 - loss: 1.8057 - val_binary_accuracy: 0.9583 - val_loss: 1.8039\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9642 - loss: 1.7369 - val_binary_accuracy: 0.9583 - val_loss: 1.7095\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9693 - loss: 1.6423 - val_binary_accuracy: 0.9518 - val_loss: 1.6336\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9803 - loss: 1.5524 - val_binary_accuracy: 0.9583 - val_loss: 1.5505\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9773 - loss: 1.4871 - val_binary_accuracy: 0.9627 - val_loss: 1.4791\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9708 - loss: 1.4246 - val_binary_accuracy: 0.9605 - val_loss: 1.4051\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9744 - loss: 1.3479 - val_binary_accuracy: 0.9627 - val_loss: 1.3653\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9781 - loss: 1.2897 - val_binary_accuracy: 0.9474 - val_loss: 1.3389\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9620 - loss: 1.2775 - val_binary_accuracy: 0.9320 - val_loss: 1.3513\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9539 - loss: 1.2666 - val_binary_accuracy: 0.9496 - val_loss: 1.2530\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9730 - loss: 1.1897 - val_binary_accuracy: 0.9539 - val_loss: 1.2064\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9715 - loss: 1.1494 - val_binary_accuracy: 0.9627 - val_loss: 1.1448\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9766 - loss: 1.0830 - val_binary_accuracy: 0.9605 - val_loss: 1.0918\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9744 - loss: 1.0330 - val_binary_accuracy: 0.9583 - val_loss: 1.0246\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9759 - loss: 0.9856 - val_binary_accuracy: 0.9671 - val_loss: 0.9876\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9744 - loss: 0.9496 - val_binary_accuracy: 0.9539 - val_loss: 0.9714\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9598 - loss: 0.9515 - val_binary_accuracy: 0.9474 - val_loss: 0.9496\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9539 - loss: 0.9453 - val_binary_accuracy: 0.9561 - val_loss: 0.9369\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.602339</td>\n",
              "      <td>127.024017</td>\n",
              "      <td>0.890351</td>\n",
              "      <td>10.017708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.842836</td>\n",
              "      <td>11.299568</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>9.879310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.860380</td>\n",
              "      <td>10.075313</td>\n",
              "      <td>0.899123</td>\n",
              "      <td>9.492467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.910088</td>\n",
              "      <td>8.813607</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>7.996880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.927632</td>\n",
              "      <td>7.650107</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>7.004307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.948830</td>\n",
              "      <td>6.602189</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>6.126779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.952485</td>\n",
              "      <td>5.843946</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>5.474557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.950292</td>\n",
              "      <td>5.237322</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>4.951972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.947368</td>\n",
              "      <td>4.726565</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>4.464812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>4.286139</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>4.111164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.942251</td>\n",
              "      <td>3.955263</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>3.862015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.948099</td>\n",
              "      <td>3.642151</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>3.529397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.953216</td>\n",
              "      <td>3.385904</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>3.262883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.959795</td>\n",
              "      <td>3.137315</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>3.071558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.960526</td>\n",
              "      <td>2.926646</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>2.854959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>2.732425</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>2.652437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.967836</td>\n",
              "      <td>2.553761</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>2.496765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>2.410387</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>2.337028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>2.257557</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>2.197619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>2.118315</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>2.109055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>2.015685</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>1.976029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>1.921138</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.906065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.975877</td>\n",
              "      <td>1.805737</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.803877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>1.736876</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.709461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>1.642288</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.633555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>1.552386</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.550504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>1.487124</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>1.479054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>1.424557</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>1.405072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>1.347851</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>1.365253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.978070</td>\n",
              "      <td>1.289707</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.338853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>1.277524</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>1.351311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.266644</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.252984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>1.189652</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.206434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.971491</td>\n",
              "      <td>1.149405</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>1.144841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>1.083009</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>1.091772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>1.033048</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.024618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.985601</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.987563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.949630</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.971428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.959795</td>\n",
              "      <td>0.951535</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.949580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.945285</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.936938</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.602339  127.024017             0.890351  10.017708\n",
              "1          0.842836   11.299568             0.894737   9.879310\n",
              "2          0.860380   10.075313             0.899123   9.492467\n",
              "3          0.910088    8.813607             0.938596   7.996880\n",
              "4          0.927632    7.650107             0.942982   7.004307\n",
              "5          0.948830    6.602189             0.940789   6.126779\n",
              "6          0.952485    5.843946             0.942982   5.474557\n",
              "7          0.950292    5.237322             0.949561   4.951972\n",
              "8          0.947368    4.726565             0.945175   4.464812\n",
              "9          0.961988    4.286139             0.956140   4.111164\n",
              "10         0.942251    3.955263             0.923246   3.862015\n",
              "11         0.948099    3.642151             0.949561   3.529397\n",
              "12         0.953216    3.385904             0.951754   3.262883\n",
              "13         0.959795    3.137315             0.949561   3.071558\n",
              "14         0.960526    2.926646             0.951754   2.854959\n",
              "15         0.964181    2.732425             0.953947   2.652437\n",
              "16         0.967836    2.553761             0.958333   2.496765\n",
              "17         0.961988    2.410387             0.958333   2.337028\n",
              "18         0.973684    2.257557             0.962719   2.197619\n",
              "19         0.977339    2.118315             0.949561   2.109055\n",
              "20         0.970029    2.015685             0.956140   1.976029\n",
              "21         0.963450    1.921138             0.953947   1.906065\n",
              "22         0.975877    1.805737             0.958333   1.803877\n",
              "23         0.964181    1.736876             0.958333   1.709461\n",
              "24         0.969298    1.642288             0.951754   1.633555\n",
              "25         0.980263    1.552386             0.958333   1.550504\n",
              "26         0.977339    1.487124             0.962719   1.479054\n",
              "27         0.970760    1.424557             0.960526   1.405072\n",
              "28         0.974415    1.347851             0.962719   1.365253\n",
              "29         0.978070    1.289707             0.947368   1.338853\n",
              "30         0.961988    1.277524             0.932018   1.351311\n",
              "31         0.953947    1.266644             0.949561   1.252984\n",
              "32         0.972953    1.189652             0.953947   1.206434\n",
              "33         0.971491    1.149405             0.962719   1.144841\n",
              "34         0.976608    1.083009             0.960526   1.091772\n",
              "35         0.974415    1.033048             0.958333   1.024618\n",
              "36         0.975877    0.985601             0.967105   0.987563\n",
              "37         0.974415    0.949630             0.953947   0.971428\n",
              "38         0.959795    0.951535             0.947368   0.949580\n",
              "39         0.953947    0.945285             0.956140   0.936938"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_LD = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,),kernel_regularizer=tf.keras.regularizers.l2(0.002)),\n",
        "                     Dropout(0.1),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002)), \n",
        "                    Dropout(0.1),\n",
        "                    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.002))\n",
        "])\n",
        "\n",
        "model_LD.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_LD = model_LD.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_LD = pd.DataFrame(history_LD.history)\n",
        "df_LD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combination L2 & Dropout - 0.002 Penalty Rate and 0.2 Dropout Rate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 351ms/step - binary_accuracy: 0.5848 - loss: 62.1250 - val_binary_accuracy: 0.7368 - val_loss: 33.8089\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.7822 - loss: 22.7809 - val_binary_accuracy: 0.9145 - val_loss: 12.8939\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8494 - loss: 13.6454 - val_binary_accuracy: 0.9123 - val_loss: 10.9042\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8838 - loss: 10.7828 - val_binary_accuracy: 0.9189 - val_loss: 9.0724\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8596 - loss: 8.6986 - val_binary_accuracy: 0.9057 - val_loss: 7.7507\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8977 - loss: 7.4906 - val_binary_accuracy: 0.9189 - val_loss: 6.7093\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 32ms/step - binary_accuracy: 0.9284 - loss: 6.3318 - val_binary_accuracy: 0.9167 - val_loss: 5.8318\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9196 - loss: 5.5414 - val_binary_accuracy: 0.9320 - val_loss: 5.1042\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9284 - loss: 4.8729 - val_binary_accuracy: 0.9342 - val_loss: 4.5379\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9320 - loss: 4.3823 - val_binary_accuracy: 0.9452 - val_loss: 4.0965\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9240 - loss: 3.9459 - val_binary_accuracy: 0.9386 - val_loss: 3.7047\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9225 - loss: 3.6129 - val_binary_accuracy: 0.9474 - val_loss: 3.3755\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9225 - loss: 3.2978 - val_binary_accuracy: 0.9430 - val_loss: 3.1207\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9393 - loss: 3.0016 - val_binary_accuracy: 0.9474 - val_loss: 2.8664\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9364 - loss: 2.7936 - val_binary_accuracy: 0.9452 - val_loss: 2.6575\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9415 - loss: 2.5588 - val_binary_accuracy: 0.9452 - val_loss: 2.5141\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9444 - loss: 2.4081 - val_binary_accuracy: 0.9342 - val_loss: 2.3420\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9415 - loss: 2.2468 - val_binary_accuracy: 0.9408 - val_loss: 2.1964\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9459 - loss: 2.1094 - val_binary_accuracy: 0.9430 - val_loss: 2.0647\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9481 - loss: 1.9733 - val_binary_accuracy: 0.9408 - val_loss: 1.9119\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9474 - loss: 1.8670 - val_binary_accuracy: 0.9408 - val_loss: 1.8124\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9518 - loss: 1.7406 - val_binary_accuracy: 0.9474 - val_loss: 1.7396\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9547 - loss: 1.6507 - val_binary_accuracy: 0.9320 - val_loss: 1.6349\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9539 - loss: 1.5638 - val_binary_accuracy: 0.9518 - val_loss: 1.5724\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9554 - loss: 1.4975 - val_binary_accuracy: 0.9298 - val_loss: 1.4932\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9583 - loss: 1.4026 - val_binary_accuracy: 0.9474 - val_loss: 1.4022\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9554 - loss: 1.3425 - val_binary_accuracy: 0.9474 - val_loss: 1.3434\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9496 - loss: 1.3041 - val_binary_accuracy: 0.9474 - val_loss: 1.3043\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9532 - loss: 1.2625 - val_binary_accuracy: 0.9430 - val_loss: 1.2144\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9503 - loss: 1.1849 - val_binary_accuracy: 0.9496 - val_loss: 1.1695\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9554 - loss: 1.1254 - val_binary_accuracy: 0.9518 - val_loss: 1.1282\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 27ms/step - binary_accuracy: 0.9583 - loss: 1.0780 - val_binary_accuracy: 0.9298 - val_loss: 1.0639\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9518 - loss: 1.0555 - val_binary_accuracy: 0.9496 - val_loss: 1.0139\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9576 - loss: 0.9714 - val_binary_accuracy: 0.9518 - val_loss: 0.9690\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9598 - loss: 0.9302 - val_binary_accuracy: 0.9539 - val_loss: 0.9177\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9569 - loss: 0.9003 - val_binary_accuracy: 0.9539 - val_loss: 0.9235\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9591 - loss: 0.8787 - val_binary_accuracy: 0.9342 - val_loss: 0.9146\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9620 - loss: 0.8548 - val_binary_accuracy: 0.9561 - val_loss: 0.8311\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9605 - loss: 0.8011 - val_binary_accuracy: 0.9474 - val_loss: 0.8452\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9532 - loss: 0.7982 - val_binary_accuracy: 0.9583 - val_loss: 0.7766\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.584795</td>\n",
              "      <td>62.125050</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>33.808918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.782164</td>\n",
              "      <td>22.780945</td>\n",
              "      <td>0.914474</td>\n",
              "      <td>12.893876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.849415</td>\n",
              "      <td>13.645364</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>10.904244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.883772</td>\n",
              "      <td>10.782759</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>9.072450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.859649</td>\n",
              "      <td>8.698621</td>\n",
              "      <td>0.905702</td>\n",
              "      <td>7.750740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.897661</td>\n",
              "      <td>7.490571</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>6.709260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.928363</td>\n",
              "      <td>6.331816</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>5.831845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.919591</td>\n",
              "      <td>5.541379</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>5.104226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.928363</td>\n",
              "      <td>4.872943</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>4.537908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.932018</td>\n",
              "      <td>4.382263</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>4.096534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.923977</td>\n",
              "      <td>3.945885</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>3.704700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.922515</td>\n",
              "      <td>3.612887</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>3.375455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.922515</td>\n",
              "      <td>3.297826</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>3.120740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.939327</td>\n",
              "      <td>3.001634</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>2.866405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.936404</td>\n",
              "      <td>2.793616</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>2.657468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.941520</td>\n",
              "      <td>2.558807</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>2.514140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.944444</td>\n",
              "      <td>2.408104</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>2.341955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.941520</td>\n",
              "      <td>2.246799</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>2.196381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.945906</td>\n",
              "      <td>2.109380</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>2.064694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.948099</td>\n",
              "      <td>1.973313</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>1.911872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.866995</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>1.812438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.740595</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.739650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>1.650697</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>1.634855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.563800</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.572436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.955409</td>\n",
              "      <td>1.497483</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>1.493240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.402624</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.402168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.955409</td>\n",
              "      <td>1.342457</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.343411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.304140</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.304327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.953216</td>\n",
              "      <td>1.262537</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>1.214431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.950292</td>\n",
              "      <td>1.184886</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.169543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.955409</td>\n",
              "      <td>1.125392</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.128241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.078015</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>1.063942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.055454</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.013920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.957602</td>\n",
              "      <td>0.971397</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.968992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.959795</td>\n",
              "      <td>0.930188</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.917745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.956871</td>\n",
              "      <td>0.900252</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.923540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.959064</td>\n",
              "      <td>0.878663</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.914611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>0.854847</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.831100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.801122</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.845158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.953216</td>\n",
              "      <td>0.798194</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.776627</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy       loss  val_binary_accuracy   val_loss\n",
              "0          0.584795  62.125050             0.736842  33.808918\n",
              "1          0.782164  22.780945             0.914474  12.893876\n",
              "2          0.849415  13.645364             0.912281  10.904244\n",
              "3          0.883772  10.782759             0.918860   9.072450\n",
              "4          0.859649   8.698621             0.905702   7.750740\n",
              "5          0.897661   7.490571             0.918860   6.709260\n",
              "6          0.928363   6.331816             0.916667   5.831845\n",
              "7          0.919591   5.541379             0.932018   5.104226\n",
              "8          0.928363   4.872943             0.934211   4.537908\n",
              "9          0.932018   4.382263             0.945175   4.096534\n",
              "10         0.923977   3.945885             0.938596   3.704700\n",
              "11         0.922515   3.612887             0.947368   3.375455\n",
              "12         0.922515   3.297826             0.942982   3.120740\n",
              "13         0.939327   3.001634             0.947368   2.866405\n",
              "14         0.936404   2.793616             0.945175   2.657468\n",
              "15         0.941520   2.558807             0.945175   2.514140\n",
              "16         0.944444   2.408104             0.934211   2.341955\n",
              "17         0.941520   2.246799             0.940789   2.196381\n",
              "18         0.945906   2.109380             0.942982   2.064694\n",
              "19         0.948099   1.973313             0.940789   1.911872\n",
              "20         0.947368   1.866995             0.940789   1.812438\n",
              "21         0.951754   1.740595             0.947368   1.739650\n",
              "22         0.954678   1.650697             0.932018   1.634855\n",
              "23         0.953947   1.563800             0.951754   1.572436\n",
              "24         0.955409   1.497483             0.929825   1.493240\n",
              "25         0.958333   1.402624             0.947368   1.402168\n",
              "26         0.955409   1.342457             0.947368   1.343411\n",
              "27         0.949561   1.304140             0.947368   1.304327\n",
              "28         0.953216   1.262537             0.942982   1.214431\n",
              "29         0.950292   1.184886             0.949561   1.169543\n",
              "30         0.955409   1.125392             0.951754   1.128241\n",
              "31         0.958333   1.078015             0.929825   1.063942\n",
              "32         0.951754   1.055454             0.949561   1.013920\n",
              "33         0.957602   0.971397             0.951754   0.968992\n",
              "34         0.959795   0.930188             0.953947   0.917745\n",
              "35         0.956871   0.900252             0.953947   0.923540\n",
              "36         0.959064   0.878663             0.934211   0.914611\n",
              "37         0.961988   0.854847             0.956140   0.831100\n",
              "38         0.960526   0.801122             0.947368   0.845158\n",
              "39         0.953216   0.798194             0.958333   0.776627"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_LD2 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,),kernel_regularizer=tf.keras.regularizers.l2(0.002)),\n",
        "                     Dropout(0.2),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002)), \n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.002))\n",
        "])\n",
        "\n",
        "model_LD2.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_LD2 = model_LD2.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_LD2 = pd.DataFrame(history_LD2.history)\n",
        "df_LD2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history_LD2.history['loss'])\n",
        "plt.plot(history_LD2.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.974\n"
          ]
        }
      ],
      "source": [
        "get_test_accuracy(model_LD2, test_set_x.T, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combination L2 & Dropout - 0.001 Penalty Rate and 0.2 Dropout Rate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 349ms/step - binary_accuracy: 0.6082 - loss: 68.1177 - val_binary_accuracy: 0.8816 - val_loss: 9.0624\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7558 - loss: 19.3887 - val_binary_accuracy: 0.8816 - val_loss: 8.7240\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8874 - loss: 8.7874 - val_binary_accuracy: 0.9167 - val_loss: 6.0630\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8260 - loss: 7.1184 - val_binary_accuracy: 0.9342 - val_loss: 5.5028\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8816 - loss: 5.5131 - val_binary_accuracy: 0.9386 - val_loss: 5.0404\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8904 - loss: 4.9738 - val_binary_accuracy: 0.9298 - val_loss: 4.6421\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8918 - loss: 4.5791 - val_binary_accuracy: 0.9386 - val_loss: 4.2769\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9072 - loss: 4.1981 - val_binary_accuracy: 0.9342 - val_loss: 3.9744\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9218 - loss: 3.9043 - val_binary_accuracy: 0.9408 - val_loss: 3.6676\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9284 - loss: 3.5888 - val_binary_accuracy: 0.9430 - val_loss: 3.4437\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9291 - loss: 3.3849 - val_binary_accuracy: 0.9452 - val_loss: 3.1938\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9386 - loss: 3.1122 - val_binary_accuracy: 0.9408 - val_loss: 2.9720\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9401 - loss: 2.9220 - val_binary_accuracy: 0.9496 - val_loss: 2.7970\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9386 - loss: 2.7534 - val_binary_accuracy: 0.9474 - val_loss: 2.6347\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9539 - loss: 2.5699 - val_binary_accuracy: 0.9518 - val_loss: 2.4950\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9547 - loss: 2.4262 - val_binary_accuracy: 0.9452 - val_loss: 2.3599\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9539 - loss: 2.3078 - val_binary_accuracy: 0.9539 - val_loss: 2.2399\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9613 - loss: 2.1829 - val_binary_accuracy: 0.9583 - val_loss: 2.1474\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9503 - loss: 2.1089 - val_binary_accuracy: 0.9430 - val_loss: 2.0667\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9459 - loss: 1.9997 - val_binary_accuracy: 0.9605 - val_loss: 1.9671\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9525 - loss: 1.9573 - val_binary_accuracy: 0.9518 - val_loss: 1.8990\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9510 - loss: 1.8321 - val_binary_accuracy: 0.9298 - val_loss: 1.8082\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9518 - loss: 1.7497 - val_binary_accuracy: 0.9452 - val_loss: 1.7283\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9591 - loss: 1.6574 - val_binary_accuracy: 0.9496 - val_loss: 1.6466\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9576 - loss: 1.5924 - val_binary_accuracy: 0.9518 - val_loss: 1.5513\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9686 - loss: 1.5077 - val_binary_accuracy: 0.9539 - val_loss: 1.4966\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9627 - loss: 1.4342 - val_binary_accuracy: 0.9518 - val_loss: 1.4239\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9722 - loss: 1.3570 - val_binary_accuracy: 0.9561 - val_loss: 1.3599\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9686 - loss: 1.3146 - val_binary_accuracy: 0.9561 - val_loss: 1.3074\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9751 - loss: 1.2606 - val_binary_accuracy: 0.9539 - val_loss: 1.2645\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9686 - loss: 1.2076 - val_binary_accuracy: 0.9539 - val_loss: 1.2141\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 1.1776 - val_binary_accuracy: 0.9561 - val_loss: 1.1700\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9788 - loss: 1.1141 - val_binary_accuracy: 0.9605 - val_loss: 1.1335\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9715 - loss: 1.0785 - val_binary_accuracy: 0.9561 - val_loss: 1.0827\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9708 - loss: 1.0520 - val_binary_accuracy: 0.9583 - val_loss: 1.0479\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9722 - loss: 1.0129 - val_binary_accuracy: 0.9474 - val_loss: 1.0229\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9693 - loss: 0.9733 - val_binary_accuracy: 0.9583 - val_loss: 1.0008\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9700 - loss: 0.9358 - val_binary_accuracy: 0.9474 - val_loss: 0.9455\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9700 - loss: 0.9174 - val_binary_accuracy: 0.9452 - val_loss: 0.9347\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9715 - loss: 0.8652 - val_binary_accuracy: 0.9605 - val_loss: 0.8771\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.608187</td>\n",
              "      <td>68.117683</td>\n",
              "      <td>0.881579</td>\n",
              "      <td>9.062382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.755848</td>\n",
              "      <td>19.388733</td>\n",
              "      <td>0.881579</td>\n",
              "      <td>8.724027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.887427</td>\n",
              "      <td>8.787446</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>6.063004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.826023</td>\n",
              "      <td>7.118361</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>5.502759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.881579</td>\n",
              "      <td>5.513076</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>5.040387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.890351</td>\n",
              "      <td>4.973844</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>4.642086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.891813</td>\n",
              "      <td>4.579123</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>4.276861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.907164</td>\n",
              "      <td>4.198088</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>3.974408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.921784</td>\n",
              "      <td>3.904315</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>3.667560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.928363</td>\n",
              "      <td>3.588788</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>3.443723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.929094</td>\n",
              "      <td>3.384899</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>3.193792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.938596</td>\n",
              "      <td>3.112217</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>2.972024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.940058</td>\n",
              "      <td>2.921989</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>2.797028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.938596</td>\n",
              "      <td>2.753416</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>2.634738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>2.569884</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>2.495002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>2.426161</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>2.359857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>2.307777</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>2.239896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.961257</td>\n",
              "      <td>2.182924</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>2.147356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.950292</td>\n",
              "      <td>2.108921</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>2.066672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.945906</td>\n",
              "      <td>1.999717</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>1.967053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.952485</td>\n",
              "      <td>1.957295</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.899040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.951023</td>\n",
              "      <td>1.832057</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>1.808192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.749653</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.728265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.959064</td>\n",
              "      <td>1.657386</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.646628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.957602</td>\n",
              "      <td>1.592416</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.551316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>1.507681</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.496632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>1.434150</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.423926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>1.357033</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>1.359909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>1.314556</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>1.307393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>1.260625</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.264502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>1.207576</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.214097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>1.177641</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>1.170033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>1.114059</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>1.133550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.971491</td>\n",
              "      <td>1.078452</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>1.082728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>1.051972</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.047852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>1.012858</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.022886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.973347</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.000752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.935795</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.945476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.917367</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.934704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.865232</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.877074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy       loss  val_binary_accuracy  val_loss\n",
              "0          0.608187  68.117683             0.881579  9.062382\n",
              "1          0.755848  19.388733             0.881579  8.724027\n",
              "2          0.887427   8.787446             0.916667  6.063004\n",
              "3          0.826023   7.118361             0.934211  5.502759\n",
              "4          0.881579   5.513076             0.938596  5.040387\n",
              "5          0.890351   4.973844             0.929825  4.642086\n",
              "6          0.891813   4.579123             0.938596  4.276861\n",
              "7          0.907164   4.198088             0.934211  3.974408\n",
              "8          0.921784   3.904315             0.940789  3.667560\n",
              "9          0.928363   3.588788             0.942982  3.443723\n",
              "10         0.929094   3.384899             0.945175  3.193792\n",
              "11         0.938596   3.112217             0.940789  2.972024\n",
              "12         0.940058   2.921989             0.949561  2.797028\n",
              "13         0.938596   2.753416             0.947368  2.634738\n",
              "14         0.953947   2.569884             0.951754  2.495002\n",
              "15         0.954678   2.426161             0.945175  2.359857\n",
              "16         0.953947   2.307777             0.953947  2.239896\n",
              "17         0.961257   2.182924             0.958333  2.147356\n",
              "18         0.950292   2.108921             0.942982  2.066672\n",
              "19         0.945906   1.999717             0.960526  1.967053\n",
              "20         0.952485   1.957295             0.951754  1.899040\n",
              "21         0.951023   1.832057             0.929825  1.808192\n",
              "22         0.951754   1.749653             0.945175  1.728265\n",
              "23         0.959064   1.657386             0.949561  1.646628\n",
              "24         0.957602   1.592416             0.951754  1.551316\n",
              "25         0.968567   1.507681             0.953947  1.496632\n",
              "26         0.962719   1.434150             0.951754  1.423926\n",
              "27         0.972222   1.357033             0.956140  1.359909\n",
              "28         0.968567   1.314556             0.956140  1.307393\n",
              "29         0.975146   1.260625             0.953947  1.264502\n",
              "30         0.968567   1.207576             0.953947  1.214097\n",
              "31         0.969298   1.177641             0.956140  1.170033\n",
              "32         0.978801   1.114059             0.960526  1.133550\n",
              "33         0.971491   1.078452             0.956140  1.082728\n",
              "34         0.970760   1.051972             0.958333  1.047852\n",
              "35         0.972222   1.012858             0.947368  1.022886\n",
              "36         0.969298   0.973347             0.958333  1.000752\n",
              "37         0.970029   0.935795             0.947368  0.945476\n",
              "38         0.970029   0.917367             0.945175  0.934704\n",
              "39         0.971491   0.865232             0.960526  0.877074"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_LD3 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,),kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "                     Dropout(0.2),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), \n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "])\n",
        "\n",
        "model_LD3.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_LD3 = model_LD3.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_LD3 = pd.DataFrame(history_LD3.history)\n",
        "df_LD3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combination L2 & Dropout - 0.002 Penalty Rate and 0.2 Dropout Rate for first hidden layer, then 0.001 l2 and 0.15 dropout rate for second hidden layer, output layer with 0.001 l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 349ms/step - binary_accuracy: 0.5980 - loss: 155.0935 - val_binary_accuracy: 0.4715 - val_loss: 99.0121\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.6623 - loss: 36.8401 - val_binary_accuracy: 0.5746 - val_loss: 12.2744\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.6981 - loss: 12.8046 - val_binary_accuracy: 0.8882 - val_loss: 10.6343\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7924 - loss: 11.2218 - val_binary_accuracy: 0.7018 - val_loss: 10.3661\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 33ms/step - binary_accuracy: 0.7690 - loss: 10.2074 - val_binary_accuracy: 0.8377 - val_loss: 9.5737\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8129 - loss: 9.3344 - val_binary_accuracy: 0.8772 - val_loss: 8.7631\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8107 - loss: 8.5908 - val_binary_accuracy: 0.8772 - val_loss: 8.0713\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8253 - loss: 7.9044 - val_binary_accuracy: 0.9364 - val_loss: 7.4690\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8545 - loss: 7.2717 - val_binary_accuracy: 0.9452 - val_loss: 6.8862\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8706 - loss: 6.7255 - val_binary_accuracy: 0.9430 - val_loss: 6.3717\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9020 - loss: 6.2425 - val_binary_accuracy: 0.9364 - val_loss: 5.9643\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 27ms/step - binary_accuracy: 0.9057 - loss: 5.8424 - val_binary_accuracy: 0.9452 - val_loss: 5.5511\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8962 - loss: 5.4750 - val_binary_accuracy: 0.9013 - val_loss: 5.2804\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.8940 - loss: 5.1415 - val_binary_accuracy: 0.9123 - val_loss: 4.9325\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.8962 - loss: 4.8354 - val_binary_accuracy: 0.9408 - val_loss: 4.6039\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8911 - loss: 4.5830 - val_binary_accuracy: 0.9452 - val_loss: 4.3247\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9094 - loss: 4.2928 - val_binary_accuracy: 0.9430 - val_loss: 4.0667\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9254 - loss: 4.0255 - val_binary_accuracy: 0.9452 - val_loss: 3.8649\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9108 - loss: 3.8239 - val_binary_accuracy: 0.9474 - val_loss: 3.6258\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9181 - loss: 3.5861 - val_binary_accuracy: 0.9474 - val_loss: 3.4485\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9159 - loss: 3.4047 - val_binary_accuracy: 0.9561 - val_loss: 3.2210\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9306 - loss: 3.1902 - val_binary_accuracy: 0.9452 - val_loss: 3.0859\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9430 - loss: 3.0380 - val_binary_accuracy: 0.9474 - val_loss: 2.9121\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9335 - loss: 2.8758 - val_binary_accuracy: 0.9276 - val_loss: 2.7529\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9364 - loss: 2.7244 - val_binary_accuracy: 0.9539 - val_loss: 2.6027\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9386 - loss: 2.5877 - val_binary_accuracy: 0.9298 - val_loss: 2.5015\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9408 - loss: 2.4471 - val_binary_accuracy: 0.9583 - val_loss: 2.3541\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9452 - loss: 2.3343 - val_binary_accuracy: 0.9496 - val_loss: 2.2803\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9444 - loss: 2.2208 - val_binary_accuracy: 0.9364 - val_loss: 2.1232\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9452 - loss: 2.1011 - val_binary_accuracy: 0.9561 - val_loss: 2.0217\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9474 - loss: 1.9928 - val_binary_accuracy: 0.9496 - val_loss: 1.9232\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9371 - loss: 1.9029 - val_binary_accuracy: 0.9101 - val_loss: 1.8983\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.8808 - loss: 1.9588 - val_binary_accuracy: 0.9386 - val_loss: 1.8234\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9357 - loss: 1.7943 - val_binary_accuracy: 0.9320 - val_loss: 1.7579\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9444 - loss: 1.7264 - val_binary_accuracy: 0.9320 - val_loss: 1.6510\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9444 - loss: 1.6057 - val_binary_accuracy: 0.9496 - val_loss: 1.5354\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9466 - loss: 1.5261 - val_binary_accuracy: 0.9452 - val_loss: 1.4535\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9444 - loss: 1.4349 - val_binary_accuracy: 0.9496 - val_loss: 1.3778\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9415 - loss: 1.3619 - val_binary_accuracy: 0.9539 - val_loss: 1.3140\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9466 - loss: 1.2951 - val_binary_accuracy: 0.9518 - val_loss: 1.2450\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.597953</td>\n",
              "      <td>155.093460</td>\n",
              "      <td>0.471491</td>\n",
              "      <td>99.012093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.662281</td>\n",
              "      <td>36.840134</td>\n",
              "      <td>0.574561</td>\n",
              "      <td>12.274354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.698099</td>\n",
              "      <td>12.804601</td>\n",
              "      <td>0.888158</td>\n",
              "      <td>10.634262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.792398</td>\n",
              "      <td>11.221802</td>\n",
              "      <td>0.701754</td>\n",
              "      <td>10.366093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.769006</td>\n",
              "      <td>10.207428</td>\n",
              "      <td>0.837719</td>\n",
              "      <td>9.573722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.812865</td>\n",
              "      <td>9.334358</td>\n",
              "      <td>0.877193</td>\n",
              "      <td>8.763104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.810673</td>\n",
              "      <td>8.590835</td>\n",
              "      <td>0.877193</td>\n",
              "      <td>8.071285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.825292</td>\n",
              "      <td>7.904430</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>7.469026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.854532</td>\n",
              "      <td>7.271656</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>6.886240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.870614</td>\n",
              "      <td>6.725547</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>6.371681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.902047</td>\n",
              "      <td>6.242494</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>5.964310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.905702</td>\n",
              "      <td>5.842376</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>5.551113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.896199</td>\n",
              "      <td>5.475021</td>\n",
              "      <td>0.901316</td>\n",
              "      <td>5.280440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.894006</td>\n",
              "      <td>5.141516</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>4.932478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.896199</td>\n",
              "      <td>4.835387</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>4.603926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.891082</td>\n",
              "      <td>4.582963</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>4.324660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.909357</td>\n",
              "      <td>4.292802</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>4.066711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.925439</td>\n",
              "      <td>4.025467</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>3.864913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.910819</td>\n",
              "      <td>3.823930</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>3.625754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.918129</td>\n",
              "      <td>3.586099</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>3.448469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.915936</td>\n",
              "      <td>3.404683</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>3.221010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.930556</td>\n",
              "      <td>3.190194</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>3.085895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.942982</td>\n",
              "      <td>3.038028</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>2.912103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.933480</td>\n",
              "      <td>2.875775</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>2.752852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.936404</td>\n",
              "      <td>2.724376</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>2.602657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.938596</td>\n",
              "      <td>2.587670</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>2.501480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>2.447054</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>2.354149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.945175</td>\n",
              "      <td>2.334264</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>2.280330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.944444</td>\n",
              "      <td>2.220780</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>2.123197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.945175</td>\n",
              "      <td>2.101140</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>2.021699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.992766</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.923231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.937135</td>\n",
              "      <td>1.902877</td>\n",
              "      <td>0.910088</td>\n",
              "      <td>1.898255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.880848</td>\n",
              "      <td>1.958771</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>1.823362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.935673</td>\n",
              "      <td>1.794318</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>1.757937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.944444</td>\n",
              "      <td>1.726416</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>1.651047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.944444</td>\n",
              "      <td>1.605701</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.535371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.946637</td>\n",
              "      <td>1.526130</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.453469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.944444</td>\n",
              "      <td>1.434891</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.377832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.941520</td>\n",
              "      <td>1.361864</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.313958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.946637</td>\n",
              "      <td>1.295097</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.245019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.597953  155.093460             0.471491  99.012093\n",
              "1          0.662281   36.840134             0.574561  12.274354\n",
              "2          0.698099   12.804601             0.888158  10.634262\n",
              "3          0.792398   11.221802             0.701754  10.366093\n",
              "4          0.769006   10.207428             0.837719   9.573722\n",
              "5          0.812865    9.334358             0.877193   8.763104\n",
              "6          0.810673    8.590835             0.877193   8.071285\n",
              "7          0.825292    7.904430             0.936404   7.469026\n",
              "8          0.854532    7.271656             0.945175   6.886240\n",
              "9          0.870614    6.725547             0.942982   6.371681\n",
              "10         0.902047    6.242494             0.936404   5.964310\n",
              "11         0.905702    5.842376             0.945175   5.551113\n",
              "12         0.896199    5.475021             0.901316   5.280440\n",
              "13         0.894006    5.141516             0.912281   4.932478\n",
              "14         0.896199    4.835387             0.940789   4.603926\n",
              "15         0.891082    4.582963             0.945175   4.324660\n",
              "16         0.909357    4.292802             0.942982   4.066711\n",
              "17         0.925439    4.025467             0.945175   3.864913\n",
              "18         0.910819    3.823930             0.947368   3.625754\n",
              "19         0.918129    3.586099             0.947368   3.448469\n",
              "20         0.915936    3.404683             0.956140   3.221010\n",
              "21         0.930556    3.190194             0.945175   3.085895\n",
              "22         0.942982    3.038028             0.947368   2.912103\n",
              "23         0.933480    2.875775             0.927632   2.752852\n",
              "24         0.936404    2.724376             0.953947   2.602657\n",
              "25         0.938596    2.587670             0.929825   2.501480\n",
              "26         0.940789    2.447054             0.958333   2.354149\n",
              "27         0.945175    2.334264             0.949561   2.280330\n",
              "28         0.944444    2.220780             0.936404   2.123197\n",
              "29         0.945175    2.101140             0.956140   2.021699\n",
              "30         0.947368    1.992766             0.949561   1.923231\n",
              "31         0.937135    1.902877             0.910088   1.898255\n",
              "32         0.880848    1.958771             0.938596   1.823362\n",
              "33         0.935673    1.794318             0.932018   1.757937\n",
              "34         0.944444    1.726416             0.932018   1.651047\n",
              "35         0.944444    1.605701             0.949561   1.535371\n",
              "36         0.946637    1.526130             0.945175   1.453469\n",
              "37         0.944444    1.434891             0.949561   1.377832\n",
              "38         0.941520    1.361864             0.953947   1.313958\n",
              "39         0.946637    1.295097             0.951754   1.245019"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_LD4 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,),kernel_regularizer=tf.keras.regularizers.l2(0.002)),\n",
        "                     Dropout(0.2),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), \n",
        "                    Dropout(0.15),\n",
        "                    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "])\n",
        "\n",
        "model_LD4.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_LD4 = model_LD4.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_LD4 = pd.DataFrame(history_LD4.history)\n",
        "df_LD4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combination L2 & Dropout - 0.001 Penalty Rate and 0.2 Dropout Rate for first hidden layer, then 0.002 l2 and 0.15 dropout rate for second hidden layer, output layer with 0.002 l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 2s - 399ms/step - binary_accuracy: 0.5197 - loss: 150.4365 - val_binary_accuracy: 0.5285 - val_loss: 35.9684\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.5570 - loss: 21.4672 - val_binary_accuracy: 0.7478 - val_loss: 6.9567\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7054 - loss: 8.5585 - val_binary_accuracy: 0.8531 - val_loss: 7.3264\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7193 - loss: 7.5967 - val_binary_accuracy: 0.5943 - val_loss: 7.4032\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.6930 - loss: 7.3459 - val_binary_accuracy: 0.8706 - val_loss: 6.9472\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7654 - loss: 6.8723 - val_binary_accuracy: 0.9013 - val_loss: 6.4238\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.7953 - loss: 6.4337 - val_binary_accuracy: 0.9057 - val_loss: 5.9595\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.7887 - loss: 6.0301 - val_binary_accuracy: 0.9254 - val_loss: 5.5943\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8246 - loss: 5.5668 - val_binary_accuracy: 0.9298 - val_loss: 5.1910\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8392 - loss: 5.2084 - val_binary_accuracy: 0.9145 - val_loss: 4.9014\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8494 - loss: 4.8962 - val_binary_accuracy: 0.9430 - val_loss: 4.5825\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8750 - loss: 4.5695 - val_binary_accuracy: 0.9298 - val_loss: 4.3035\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8750 - loss: 4.3215 - val_binary_accuracy: 0.9408 - val_loss: 4.0601\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.8933 - loss: 4.0563 - val_binary_accuracy: 0.9408 - val_loss: 3.8384\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9108 - loss: 3.8165 - val_binary_accuracy: 0.9430 - val_loss: 3.6472\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9152 - loss: 3.6183 - val_binary_accuracy: 0.9496 - val_loss: 3.4471\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9137 - loss: 3.4403 - val_binary_accuracy: 0.9474 - val_loss: 3.2789\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9130 - loss: 3.3020 - val_binary_accuracy: 0.9561 - val_loss: 3.1224\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9181 - loss: 3.1352 - val_binary_accuracy: 0.9496 - val_loss: 2.9844\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9218 - loss: 2.9888 - val_binary_accuracy: 0.9474 - val_loss: 2.8568\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9247 - loss: 2.8560 - val_binary_accuracy: 0.9539 - val_loss: 2.7182\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9225 - loss: 2.7223 - val_binary_accuracy: 0.9232 - val_loss: 2.7013\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.8969 - loss: 2.6882 - val_binary_accuracy: 0.9496 - val_loss: 2.5128\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9050 - loss: 2.5381 - val_binary_accuracy: 0.9430 - val_loss: 2.4217\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9327 - loss: 2.4327 - val_binary_accuracy: 0.9496 - val_loss: 2.3175\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9342 - loss: 2.3166 - val_binary_accuracy: 0.9474 - val_loss: 2.2208\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9306 - loss: 2.2255 - val_binary_accuracy: 0.9518 - val_loss: 2.1308\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9371 - loss: 2.1343 - val_binary_accuracy: 0.9518 - val_loss: 2.0492\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9327 - loss: 2.0565 - val_binary_accuracy: 0.9496 - val_loss: 1.9842\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9291 - loss: 1.9902 - val_binary_accuracy: 0.9474 - val_loss: 1.9145\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9386 - loss: 1.9085 - val_binary_accuracy: 0.9452 - val_loss: 1.8394\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9225 - loss: 1.8537 - val_binary_accuracy: 0.9232 - val_loss: 1.8631\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9306 - loss: 1.8039 - val_binary_accuracy: 0.9518 - val_loss: 1.7288\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9415 - loss: 1.7304 - val_binary_accuracy: 0.9474 - val_loss: 1.6655\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9488 - loss: 1.6474 - val_binary_accuracy: 0.9496 - val_loss: 1.6004\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9532 - loss: 1.5852 - val_binary_accuracy: 0.9496 - val_loss: 1.5428\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9444 - loss: 1.5301 - val_binary_accuracy: 0.9496 - val_loss: 1.4856\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9466 - loss: 1.4971 - val_binary_accuracy: 0.9583 - val_loss: 1.4327\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9408 - loss: 1.4327 - val_binary_accuracy: 0.9474 - val_loss: 1.3913\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9539 - loss: 1.3846 - val_binary_accuracy: 0.9452 - val_loss: 1.3495\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.519737</td>\n",
              "      <td>150.436493</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>35.968422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.557018</td>\n",
              "      <td>21.467232</td>\n",
              "      <td>0.747807</td>\n",
              "      <td>6.956728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.705409</td>\n",
              "      <td>8.558504</td>\n",
              "      <td>0.853070</td>\n",
              "      <td>7.326421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.719298</td>\n",
              "      <td>7.596695</td>\n",
              "      <td>0.594298</td>\n",
              "      <td>7.403170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.692982</td>\n",
              "      <td>7.345872</td>\n",
              "      <td>0.870614</td>\n",
              "      <td>6.947236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.765351</td>\n",
              "      <td>6.872253</td>\n",
              "      <td>0.901316</td>\n",
              "      <td>6.423792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.795322</td>\n",
              "      <td>6.433726</td>\n",
              "      <td>0.905702</td>\n",
              "      <td>5.959491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.788743</td>\n",
              "      <td>6.030115</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>5.594294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.824561</td>\n",
              "      <td>5.566766</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>5.190992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.839181</td>\n",
              "      <td>5.208367</td>\n",
              "      <td>0.914474</td>\n",
              "      <td>4.901390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.849415</td>\n",
              "      <td>4.896192</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>4.582520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.875000</td>\n",
              "      <td>4.569514</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>4.303533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.875000</td>\n",
              "      <td>4.321473</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>4.060148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.893275</td>\n",
              "      <td>4.056275</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>3.838444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.910819</td>\n",
              "      <td>3.816504</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>3.647221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.915205</td>\n",
              "      <td>3.618293</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>3.447086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.913743</td>\n",
              "      <td>3.440255</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>3.278913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.913012</td>\n",
              "      <td>3.301976</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>3.122402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.918129</td>\n",
              "      <td>3.135153</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>2.984357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.921784</td>\n",
              "      <td>2.988795</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>2.856820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.924708</td>\n",
              "      <td>2.855961</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>2.718235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.922515</td>\n",
              "      <td>2.722282</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>2.701254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.896930</td>\n",
              "      <td>2.688160</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>2.512826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.904971</td>\n",
              "      <td>2.538096</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>2.421712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.932749</td>\n",
              "      <td>2.432726</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>2.317471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.934211</td>\n",
              "      <td>2.316594</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>2.220798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.930556</td>\n",
              "      <td>2.225451</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>2.130815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.937135</td>\n",
              "      <td>2.134305</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>2.049190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.932749</td>\n",
              "      <td>2.056479</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.984156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.929094</td>\n",
              "      <td>1.990154</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.914475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.938596</td>\n",
              "      <td>1.908517</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.839362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.922515</td>\n",
              "      <td>1.853743</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>1.863130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.930556</td>\n",
              "      <td>1.803880</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>1.728822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.941520</td>\n",
              "      <td>1.730447</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.665465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.948830</td>\n",
              "      <td>1.647416</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.600434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.953216</td>\n",
              "      <td>1.585207</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.542782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.944444</td>\n",
              "      <td>1.530132</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>1.485609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.946637</td>\n",
              "      <td>1.497057</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.432672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>1.432679</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.391335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.384589</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.349480</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy        loss  val_binary_accuracy   val_loss\n",
              "0          0.519737  150.436493             0.528509  35.968422\n",
              "1          0.557018   21.467232             0.747807   6.956728\n",
              "2          0.705409    8.558504             0.853070   7.326421\n",
              "3          0.719298    7.596695             0.594298   7.403170\n",
              "4          0.692982    7.345872             0.870614   6.947236\n",
              "5          0.765351    6.872253             0.901316   6.423792\n",
              "6          0.795322    6.433726             0.905702   5.959491\n",
              "7          0.788743    6.030115             0.925439   5.594294\n",
              "8          0.824561    5.566766             0.929825   5.190992\n",
              "9          0.839181    5.208367             0.914474   4.901390\n",
              "10         0.849415    4.896192             0.942982   4.582520\n",
              "11         0.875000    4.569514             0.929825   4.303533\n",
              "12         0.875000    4.321473             0.940789   4.060148\n",
              "13         0.893275    4.056275             0.940789   3.838444\n",
              "14         0.910819    3.816504             0.942982   3.647221\n",
              "15         0.915205    3.618293             0.949561   3.447086\n",
              "16         0.913743    3.440255             0.947368   3.278913\n",
              "17         0.913012    3.301976             0.956140   3.122402\n",
              "18         0.918129    3.135153             0.949561   2.984357\n",
              "19         0.921784    2.988795             0.947368   2.856820\n",
              "20         0.924708    2.855961             0.953947   2.718235\n",
              "21         0.922515    2.722282             0.923246   2.701254\n",
              "22         0.896930    2.688160             0.949561   2.512826\n",
              "23         0.904971    2.538096             0.942982   2.421712\n",
              "24         0.932749    2.432726             0.949561   2.317471\n",
              "25         0.934211    2.316594             0.947368   2.220798\n",
              "26         0.930556    2.225451             0.951754   2.130815\n",
              "27         0.937135    2.134305             0.951754   2.049190\n",
              "28         0.932749    2.056479             0.949561   1.984156\n",
              "29         0.929094    1.990154             0.947368   1.914475\n",
              "30         0.938596    1.908517             0.945175   1.839362\n",
              "31         0.922515    1.853743             0.923246   1.863130\n",
              "32         0.930556    1.803880             0.951754   1.728822\n",
              "33         0.941520    1.730447             0.947368   1.665465\n",
              "34         0.948830    1.647416             0.949561   1.600434\n",
              "35         0.953216    1.585207             0.949561   1.542782\n",
              "36         0.944444    1.530132             0.949561   1.485609\n",
              "37         0.946637    1.497057             0.958333   1.432672\n",
              "38         0.940789    1.432679             0.947368   1.391335\n",
              "39         0.953947    1.384589             0.945175   1.349480"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_LD5 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,),kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "                     Dropout(0.2),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002)), \n",
        "                    Dropout(0.15),\n",
        "                    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.002))\n",
        "])\n",
        "\n",
        "model_LD5.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_LD5 = model_LD5.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_LD5 = pd.DataFrame(history_LD5.history)\n",
        "df_LD5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vCsr6vPGCd"
      },
      "source": [
        "#### BATCH NORMALIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Default Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "ziDEjU-cPHwu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 442ms/step - binary_accuracy: 0.8238 - loss: 0.3508 - val_binary_accuracy: 0.7018 - val_loss: 4.1833\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9627 - loss: 0.1269 - val_binary_accuracy: 0.7018 - val_loss: 2.6264\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9605 - loss: 0.1091 - val_binary_accuracy: 0.7763 - val_loss: 0.9038\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9627 - loss: 0.1063 - val_binary_accuracy: 0.8816 - val_loss: 0.4452\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 0.0910 - val_binary_accuracy: 0.8596 - val_loss: 0.4956\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9671 - loss: 0.0935 - val_binary_accuracy: 0.8969 - val_loss: 0.3262\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9759 - loss: 0.0770 - val_binary_accuracy: 0.9101 - val_loss: 0.2557\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9773 - loss: 0.0739 - val_binary_accuracy: 0.9211 - val_loss: 0.2364\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9803 - loss: 0.0674 - val_binary_accuracy: 0.9189 - val_loss: 0.2419\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9810 - loss: 0.0633 - val_binary_accuracy: 0.9298 - val_loss: 0.2176\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9825 - loss: 0.0632 - val_binary_accuracy: 0.9474 - val_loss: 0.1788\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9861 - loss: 0.0571 - val_binary_accuracy: 0.9496 - val_loss: 0.1607\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9854 - loss: 0.0548 - val_binary_accuracy: 0.9496 - val_loss: 0.1548\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9832 - loss: 0.0533 - val_binary_accuracy: 0.9496 - val_loss: 0.1603\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9890 - loss: 0.0507 - val_binary_accuracy: 0.9561 - val_loss: 0.1340\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9912 - loss: 0.0476 - val_binary_accuracy: 0.9627 - val_loss: 0.1239\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9861 - loss: 0.0475 - val_binary_accuracy: 0.9627 - val_loss: 0.1228\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9898 - loss: 0.0438 - val_binary_accuracy: 0.9605 - val_loss: 0.1311\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9890 - loss: 0.0418 - val_binary_accuracy: 0.9715 - val_loss: 0.1112\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9890 - loss: 0.0401 - val_binary_accuracy: 0.9671 - val_loss: 0.1218\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9912 - loss: 0.0389 - val_binary_accuracy: 0.9671 - val_loss: 0.1188\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9927 - loss: 0.0333 - val_binary_accuracy: 0.9737 - val_loss: 0.1007\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9912 - loss: 0.0347 - val_binary_accuracy: 0.9737 - val_loss: 0.1087\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9905 - loss: 0.0289 - val_binary_accuracy: 0.9671 - val_loss: 0.1052\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9927 - loss: 0.0289 - val_binary_accuracy: 0.9715 - val_loss: 0.1100\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9942 - loss: 0.0244 - val_binary_accuracy: 0.9803 - val_loss: 0.0848\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9949 - loss: 0.0251 - val_binary_accuracy: 0.9715 - val_loss: 0.0746\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9949 - loss: 0.0200 - val_binary_accuracy: 0.9737 - val_loss: 0.0780\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9956 - loss: 0.0179 - val_binary_accuracy: 0.9759 - val_loss: 0.0746\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9971 - loss: 0.0171 - val_binary_accuracy: 0.9759 - val_loss: 0.0671\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0183 - val_binary_accuracy: 0.9715 - val_loss: 0.0675\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9978 - loss: 0.0148 - val_binary_accuracy: 0.9781 - val_loss: 0.0575\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9971 - loss: 0.0200 - val_binary_accuracy: 0.9715 - val_loss: 0.0659\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9942 - loss: 0.0187 - val_binary_accuracy: 0.9737 - val_loss: 0.0716\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0168 - val_binary_accuracy: 0.9759 - val_loss: 0.0721\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9978 - loss: 0.0131 - val_binary_accuracy: 0.9715 - val_loss: 0.0854\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9971 - loss: 0.0133 - val_binary_accuracy: 0.9781 - val_loss: 0.0671\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9971 - loss: 0.0108 - val_binary_accuracy: 0.9737 - val_loss: 0.0607\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9971 - loss: 0.0105 - val_binary_accuracy: 0.9803 - val_loss: 0.0493\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9985 - loss: 0.0085 - val_binary_accuracy: 0.9825 - val_loss: 0.0400\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.823830</td>\n",
              "      <td>0.350822</td>\n",
              "      <td>0.701754</td>\n",
              "      <td>4.183346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.126942</td>\n",
              "      <td>0.701754</td>\n",
              "      <td>2.626352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.109118</td>\n",
              "      <td>0.776316</td>\n",
              "      <td>0.903778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.106328</td>\n",
              "      <td>0.881579</td>\n",
              "      <td>0.445248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.090961</td>\n",
              "      <td>0.859649</td>\n",
              "      <td>0.495590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.093471</td>\n",
              "      <td>0.896930</td>\n",
              "      <td>0.326196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.077006</td>\n",
              "      <td>0.910088</td>\n",
              "      <td>0.255730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>0.073893</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.236364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.067415</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>0.241914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>0.063281</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>0.217647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.063200</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.178809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.057130</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.160720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.054826</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.154822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.983187</td>\n",
              "      <td>0.053252</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.160315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.050658</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.134018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.047643</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.123929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.047503</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.122791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.043764</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.131112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.041778</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.111221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.040137</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.121831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.038910</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.118760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.033298</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.100718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.034705</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.108684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.028881</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.105178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.028857</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.110030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.024407</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.084836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.994883</td>\n",
              "      <td>0.025077</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.074631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.994883</td>\n",
              "      <td>0.020028</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.078049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.017885</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.074570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.017095</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.067061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.996345</td>\n",
              "      <td>0.018347</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.067542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.014840</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.057520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.020022</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.065853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.018691</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.071623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.996345</td>\n",
              "      <td>0.016820</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.072132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.085414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.013251</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.067110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.010822</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.060688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.010545</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.049283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.008476</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.040047</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.823830  0.350822             0.701754  4.183346\n",
              "1          0.962719  0.126942             0.701754  2.626352\n",
              "2          0.960526  0.109118             0.776316  0.903778\n",
              "3          0.962719  0.106328             0.881579  0.445248\n",
              "4          0.969298  0.090961             0.859649  0.495590\n",
              "5          0.967105  0.093471             0.896930  0.326196\n",
              "6          0.975877  0.077006             0.910088  0.255730\n",
              "7          0.977339  0.073893             0.921053  0.236364\n",
              "8          0.980263  0.067415             0.918860  0.241914\n",
              "9          0.980994  0.063281             0.929825  0.217647\n",
              "10         0.982456  0.063200             0.947368  0.178809\n",
              "11         0.986111  0.057130             0.949561  0.160720\n",
              "12         0.985380  0.054826             0.949561  0.154822\n",
              "13         0.983187  0.053252             0.949561  0.160315\n",
              "14         0.989035  0.050658             0.956140  0.134018\n",
              "15         0.991228  0.047643             0.962719  0.123929\n",
              "16         0.986111  0.047503             0.962719  0.122791\n",
              "17         0.989766  0.043764             0.960526  0.131112\n",
              "18         0.989035  0.041778             0.971491  0.111221\n",
              "19         0.989035  0.040137             0.967105  0.121831\n",
              "20         0.991228  0.038910             0.967105  0.118760\n",
              "21         0.992690  0.033298             0.973684  0.100718\n",
              "22         0.991228  0.034705             0.973684  0.108684\n",
              "23         0.990497  0.028881             0.967105  0.105178\n",
              "24         0.992690  0.028857             0.971491  0.110030\n",
              "25         0.994152  0.024407             0.980263  0.084836\n",
              "26         0.994883  0.025077             0.971491  0.074631\n",
              "27         0.994883  0.020028             0.973684  0.078049\n",
              "28         0.995614  0.017885             0.975877  0.074570\n",
              "29         0.997076  0.017095             0.975877  0.067061\n",
              "30         0.996345  0.018347             0.971491  0.067542\n",
              "31         0.997807  0.014840             0.978070  0.057520\n",
              "32         0.997076  0.020022             0.971491  0.065853\n",
              "33         0.994152  0.018691             0.973684  0.071623\n",
              "34         0.996345  0.016820             0.975877  0.072132\n",
              "35         0.997807  0.013125             0.971491  0.085414\n",
              "36         0.997076  0.013251             0.978070  0.067110\n",
              "37         0.997076  0.010822             0.973684  0.060688\n",
              "38         0.997076  0.010545             0.980263  0.049283\n",
              "39         0.998538  0.008476             0.982456  0.040047"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "model_bn1 = Sequential([#Flatten(input_shape = (64, 64, 3)),\n",
        "                     Dense(64, activation='relu',input_shape = (12288,)),\n",
        "                     BatchNormalization(momentum=0.99, # default is 0.99\n",
        "                                    epsilon=0.001, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer='zeros', # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer='ones'), # default is gamma_initializer='ones'),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.99, # default is 0.99\n",
        "                                    epsilon=0.001, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer='zeros', # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer='ones'), # default is gamma_initializer='ones'),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "\n",
        "])\n",
        "\n",
        "model_bn1.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bn1 = model_bn1.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bn1 = pd.DataFrame(history_bn1.history)\n",
        "df_bn1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "gtUy8oxjPLGH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 440ms/step - binary_accuracy: 0.7785 - loss: 0.4460 - val_binary_accuracy: 0.5943 - val_loss: 2.1472\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9496 - loss: 0.1481 - val_binary_accuracy: 0.8399 - val_loss: 0.4159\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9576 - loss: 0.1185 - val_binary_accuracy: 0.9452 - val_loss: 0.1692\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9730 - loss: 0.0967 - val_binary_accuracy: 0.9496 - val_loss: 0.1294\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9722 - loss: 0.0866 - val_binary_accuracy: 0.9518 - val_loss: 0.1189\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 32ms/step - binary_accuracy: 0.9766 - loss: 0.0713 - val_binary_accuracy: 0.9539 - val_loss: 0.1187\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9773 - loss: 0.0639 - val_binary_accuracy: 0.9583 - val_loss: 0.0988\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9817 - loss: 0.0603 - val_binary_accuracy: 0.9627 - val_loss: 0.0882\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9803 - loss: 0.0518 - val_binary_accuracy: 0.9627 - val_loss: 0.0883\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9846 - loss: 0.0515 - val_binary_accuracy: 0.9715 - val_loss: 0.0801\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9876 - loss: 0.0447 - val_binary_accuracy: 0.9715 - val_loss: 0.0765\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9876 - loss: 0.0434 - val_binary_accuracy: 0.9649 - val_loss: 0.0757\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9905 - loss: 0.0371 - val_binary_accuracy: 0.9715 - val_loss: 0.0657\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9920 - loss: 0.0349 - val_binary_accuracy: 0.9715 - val_loss: 0.0625\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9890 - loss: 0.0367 - val_binary_accuracy: 0.9715 - val_loss: 0.0651\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9890 - loss: 0.0309 - val_binary_accuracy: 0.9781 - val_loss: 0.0580\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9927 - loss: 0.0305 - val_binary_accuracy: 0.9759 - val_loss: 0.0548\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9934 - loss: 0.0278 - val_binary_accuracy: 0.9781 - val_loss: 0.0498\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9905 - loss: 0.0246 - val_binary_accuracy: 0.9781 - val_loss: 0.0466\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9920 - loss: 0.0241 - val_binary_accuracy: 0.9803 - val_loss: 0.0448\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9942 - loss: 0.0221 - val_binary_accuracy: 0.9737 - val_loss: 0.0487\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9920 - loss: 0.0218 - val_binary_accuracy: 0.9846 - val_loss: 0.0368\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9949 - loss: 0.0209 - val_binary_accuracy: 0.9846 - val_loss: 0.0353\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9963 - loss: 0.0164 - val_binary_accuracy: 0.9781 - val_loss: 0.0412\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9942 - loss: 0.0199 - val_binary_accuracy: 0.9846 - val_loss: 0.0423\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9956 - loss: 0.0178 - val_binary_accuracy: 0.9890 - val_loss: 0.0336\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9956 - loss: 0.0128 - val_binary_accuracy: 0.9890 - val_loss: 0.0229\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9956 - loss: 0.0119 - val_binary_accuracy: 0.9890 - val_loss: 0.0267\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9978 - loss: 0.0108 - val_binary_accuracy: 0.9890 - val_loss: 0.0252\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9971 - loss: 0.0086 - val_binary_accuracy: 0.9868 - val_loss: 0.0277\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9978 - loss: 0.0099 - val_binary_accuracy: 0.9934 - val_loss: 0.0210\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9978 - loss: 0.0082 - val_binary_accuracy: 0.9934 - val_loss: 0.0255\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9985 - loss: 0.0065 - val_binary_accuracy: 0.9934 - val_loss: 0.0189\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9971 - loss: 0.0070 - val_binary_accuracy: 0.9934 - val_loss: 0.0197\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9978 - loss: 0.0069 - val_binary_accuracy: 0.9934 - val_loss: 0.0194\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9971 - loss: 0.0067 - val_binary_accuracy: 0.9912 - val_loss: 0.0184\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9993 - loss: 0.0058 - val_binary_accuracy: 0.9912 - val_loss: 0.0211\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9985 - loss: 0.0057 - val_binary_accuracy: 0.9890 - val_loss: 0.0191\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9993 - loss: 0.0051 - val_binary_accuracy: 0.9956 - val_loss: 0.0179\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0039 - val_binary_accuracy: 0.9890 - val_loss: 0.0223\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.778509</td>\n",
              "      <td>0.446019</td>\n",
              "      <td>0.594298</td>\n",
              "      <td>2.147167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.148066</td>\n",
              "      <td>0.839912</td>\n",
              "      <td>0.415932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.957602</td>\n",
              "      <td>0.118495</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.169224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.096691</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.129439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.086569</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.118949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.071267</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.118745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>0.063922</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.098784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.060306</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.088247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.051763</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.088324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.051509</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.080095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.044685</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.076521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.043374</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.075659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.037085</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.065656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.034930</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.062532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.036687</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.065149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.030949</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.058006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.030470</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.054850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.027775</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.049786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.024627</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.046644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.024145</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.044815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.022116</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.048739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.021757</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.036831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.994883</td>\n",
              "      <td>0.020901</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.035250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.996345</td>\n",
              "      <td>0.016404</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.041239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.019878</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.042254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.017847</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.033640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.012791</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.022935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.011881</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.026746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.010762</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.025225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.008618</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.027739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.009945</td>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.020990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.008182</td>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.025474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.006469</td>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.018864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.006994</td>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.019666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.006882</td>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.019365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.006694</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.018405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.005820</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.021109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.005724</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.019118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.005108</td>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.017930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003883</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.022312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.778509  0.446019             0.594298  2.147167\n",
              "1          0.949561  0.148066             0.839912  0.415932\n",
              "2          0.957602  0.118495             0.945175  0.169224\n",
              "3          0.972953  0.096691             0.949561  0.129439\n",
              "4          0.972222  0.086569             0.951754  0.118949\n",
              "5          0.976608  0.071267             0.953947  0.118745\n",
              "6          0.977339  0.063922             0.958333  0.098784\n",
              "7          0.981725  0.060306             0.962719  0.088247\n",
              "8          0.980263  0.051763             0.962719  0.088324\n",
              "9          0.984649  0.051509             0.971491  0.080095\n",
              "10         0.987573  0.044685             0.971491  0.076521\n",
              "11         0.987573  0.043374             0.964912  0.075659\n",
              "12         0.990497  0.037085             0.971491  0.065656\n",
              "13         0.991959  0.034930             0.971491  0.062532\n",
              "14         0.989035  0.036687             0.971491  0.065149\n",
              "15         0.989035  0.030949             0.978070  0.058006\n",
              "16         0.992690  0.030470             0.975877  0.054850\n",
              "17         0.993421  0.027775             0.978070  0.049786\n",
              "18         0.990497  0.024627             0.978070  0.046644\n",
              "19         0.991959  0.024145             0.980263  0.044815\n",
              "20         0.994152  0.022116             0.973684  0.048739\n",
              "21         0.991959  0.021757             0.984649  0.036831\n",
              "22         0.994883  0.020901             0.984649  0.035250\n",
              "23         0.996345  0.016404             0.978070  0.041239\n",
              "24         0.994152  0.019878             0.984649  0.042254\n",
              "25         0.995614  0.017847             0.989035  0.033640\n",
              "26         0.995614  0.012791             0.989035  0.022935\n",
              "27         0.995614  0.011881             0.989035  0.026746\n",
              "28         0.997807  0.010762             0.989035  0.025225\n",
              "29         0.997076  0.008618             0.986842  0.027739\n",
              "30         0.997807  0.009945             0.993421  0.020990\n",
              "31         0.997807  0.008182             0.993421  0.025474\n",
              "32         0.998538  0.006469             0.993421  0.018864\n",
              "33         0.997076  0.006994             0.993421  0.019666\n",
              "34         0.997807  0.006882             0.993421  0.019365\n",
              "35         0.997076  0.006694             0.991228  0.018405\n",
              "36         0.999269  0.005820             0.991228  0.021109\n",
              "37         0.998538  0.005724             0.989035  0.019118\n",
              "38         0.999269  0.005108             0.995614  0.017930\n",
              "39         1.000000  0.003883             0.989035  0.022312"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bn2 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.001, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer='zeros', # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer='ones'), # default is gamma_initializer='ones'),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.001, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer='zeros', # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer='ones'), # default is gamma_initializer='ones'),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_bn2.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bn2 = model_bn2.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bn2 = pd.DataFrame(history_bn2.history)\n",
        "df_bn2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "AVWz2yt4PNat"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 457ms/step - binary_accuracy: 0.8692 - loss: 0.2968 - val_binary_accuracy: 0.8772 - val_loss: 0.6257\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9510 - loss: 0.1177 - val_binary_accuracy: 0.9386 - val_loss: 0.2446\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9664 - loss: 0.0883 - val_binary_accuracy: 0.9496 - val_loss: 0.1755\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9781 - loss: 0.0675 - val_binary_accuracy: 0.9583 - val_loss: 0.1436\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9795 - loss: 0.0570 - val_binary_accuracy: 0.9561 - val_loss: 0.1305\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 34ms/step - binary_accuracy: 0.9817 - loss: 0.0525 - val_binary_accuracy: 0.9605 - val_loss: 0.1175\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9825 - loss: 0.0465 - val_binary_accuracy: 0.9649 - val_loss: 0.1062\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9861 - loss: 0.0450 - val_binary_accuracy: 0.9649 - val_loss: 0.0977\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9868 - loss: 0.0422 - val_binary_accuracy: 0.9693 - val_loss: 0.1002\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9839 - loss: 0.0609 - val_binary_accuracy: 0.9693 - val_loss: 0.1400\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9825 - loss: 0.0606 - val_binary_accuracy: 0.9715 - val_loss: 0.1282\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9839 - loss: 0.0520 - val_binary_accuracy: 0.9737 - val_loss: 0.1279\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9890 - loss: 0.0493 - val_binary_accuracy: 0.9671 - val_loss: 0.1209\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0452 - val_binary_accuracy: 0.9715 - val_loss: 0.1075\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9883 - loss: 0.0434 - val_binary_accuracy: 0.9737 - val_loss: 0.1054\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9905 - loss: 0.0400 - val_binary_accuracy: 0.9671 - val_loss: 0.1160\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9912 - loss: 0.0404 - val_binary_accuracy: 0.9781 - val_loss: 0.0954\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9934 - loss: 0.0320 - val_binary_accuracy: 0.9781 - val_loss: 0.0935\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9942 - loss: 0.0310 - val_binary_accuracy: 0.9759 - val_loss: 0.0978\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9920 - loss: 0.0285 - val_binary_accuracy: 0.9803 - val_loss: 0.0888\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0341 - val_binary_accuracy: 0.9781 - val_loss: 0.0873\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9934 - loss: 0.0240 - val_binary_accuracy: 0.9781 - val_loss: 0.0891\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9956 - loss: 0.0192 - val_binary_accuracy: 0.9825 - val_loss: 0.0891\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9956 - loss: 0.0162 - val_binary_accuracy: 0.9825 - val_loss: 0.0828\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9978 - loss: 0.0169 - val_binary_accuracy: 0.9781 - val_loss: 0.0925\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9956 - loss: 0.0168 - val_binary_accuracy: 0.9781 - val_loss: 0.0757\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9949 - loss: 0.0154 - val_binary_accuracy: 0.9781 - val_loss: 0.0774\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9956 - loss: 0.0166 - val_binary_accuracy: 0.9715 - val_loss: 0.1067\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9971 - loss: 0.0101 - val_binary_accuracy: 0.9759 - val_loss: 0.0988\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9978 - loss: 0.0106 - val_binary_accuracy: 0.9759 - val_loss: 0.0891\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9971 - loss: 0.0093 - val_binary_accuracy: 0.9803 - val_loss: 0.0814\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9978 - loss: 0.0086 - val_binary_accuracy: 0.9781 - val_loss: 0.0905\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9985 - loss: 0.0079 - val_binary_accuracy: 0.9759 - val_loss: 0.0978\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9985 - loss: 0.0074 - val_binary_accuracy: 0.9759 - val_loss: 0.0750\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9985 - loss: 0.0059 - val_binary_accuracy: 0.9825 - val_loss: 0.0738\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0046 - val_binary_accuracy: 0.9803 - val_loss: 0.0884\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9971 - loss: 0.0089 - val_binary_accuracy: 0.9803 - val_loss: 0.0869\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9985 - loss: 0.0077 - val_binary_accuracy: 0.9803 - val_loss: 0.0783\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9985 - loss: 0.0052 - val_binary_accuracy: 0.9846 - val_loss: 0.0889\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0050 - val_binary_accuracy: 0.9781 - val_loss: 0.1004\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.869152</td>\n",
              "      <td>0.296813</td>\n",
              "      <td>0.877193</td>\n",
              "      <td>0.625681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.951023</td>\n",
              "      <td>0.117672</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.244559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>0.088269</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.175464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.067453</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.143577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.057014</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.130549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.052462</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.117519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.046510</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.106174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.044970</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.097682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.042216</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.100199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.060900</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.140001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.060634</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.128200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.051985</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.127940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.049322</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.120860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.045181</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.107542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.043449</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.105386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.039966</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.116024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.040384</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.095409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.031971</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.093497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.031006</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.097813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.028478</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.088776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.034076</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.087326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.024010</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.089121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.019154</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.089090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.016227</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.082763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.016896</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.092508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.016837</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.075686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.994883</td>\n",
              "      <td>0.015412</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.077411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.016633</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.106673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.010102</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.098799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.010559</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.089055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.009260</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.081420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.008565</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.090549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.007943</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.097807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.007355</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.075047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.005858</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.073756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.004559</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.088392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.008933</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.086926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.007720</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.078279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.005221</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.088889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.004997</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.100397</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.869152  0.296813             0.877193  0.625681\n",
              "1          0.951023  0.117672             0.938596  0.244559\n",
              "2          0.966374  0.088269             0.949561  0.175464\n",
              "3          0.978070  0.067453             0.958333  0.143577\n",
              "4          0.979532  0.057014             0.956140  0.130549\n",
              "5          0.981725  0.052462             0.960526  0.117519\n",
              "6          0.982456  0.046510             0.964912  0.106174\n",
              "7          0.986111  0.044970             0.964912  0.097682\n",
              "8          0.986842  0.042216             0.969298  0.100199\n",
              "9          0.983918  0.060900             0.969298  0.140001\n",
              "10         0.982456  0.060634             0.971491  0.128200\n",
              "11         0.983918  0.051985             0.973684  0.127940\n",
              "12         0.989035  0.049322             0.967105  0.120860\n",
              "13         0.988304  0.045181             0.971491  0.107542\n",
              "14         0.988304  0.043449             0.973684  0.105386\n",
              "15         0.990497  0.039966             0.967105  0.116024\n",
              "16         0.991228  0.040384             0.978070  0.095409\n",
              "17         0.993421  0.031971             0.978070  0.093497\n",
              "18         0.994152  0.031006             0.975877  0.097813\n",
              "19         0.991959  0.028478             0.980263  0.088776\n",
              "20         0.988304  0.034076             0.978070  0.087326\n",
              "21         0.993421  0.024010             0.978070  0.089121\n",
              "22         0.995614  0.019154             0.982456  0.089090\n",
              "23         0.995614  0.016227             0.982456  0.082763\n",
              "24         0.997807  0.016896             0.978070  0.092508\n",
              "25         0.995614  0.016837             0.978070  0.075686\n",
              "26         0.994883  0.015412             0.978070  0.077411\n",
              "27         0.995614  0.016633             0.971491  0.106673\n",
              "28         0.997076  0.010102             0.975877  0.098799\n",
              "29         0.997807  0.010559             0.975877  0.089055\n",
              "30         0.997076  0.009260             0.980263  0.081420\n",
              "31         0.997807  0.008565             0.978070  0.090549\n",
              "32         0.998538  0.007943             0.975877  0.097807\n",
              "33         0.998538  0.007355             0.975877  0.075047\n",
              "34         0.998538  0.005858             0.982456  0.073756\n",
              "35         0.999269  0.004559             0.980263  0.088392\n",
              "36         0.997076  0.008933             0.980263  0.086926\n",
              "37         0.998538  0.007720             0.980263  0.078279\n",
              "38         0.998538  0.005221             0.984649  0.088889\n",
              "39         0.999269  0.004997             0.978070  0.100397"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bn3 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer='zeros', # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer='ones'), # default is gamma_initializer='ones'),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer='zeros', # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer='ones'), # default is gamma_initializer='ones'),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_bn3.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bn3 = model_bn3.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bn3 = pd.DataFrame(history_bn3.history)\n",
        "df_bn3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnz0lEQVR4nO3dd3hUZcLG4d9MyqQnhJAGgdCLNKVERAEXNFjBsqKiICqsCq4un6uigoqr2JdVVGzI6qoU17YWFBBQAaULKEUQSShJCJDeZ873x0kGIiEkYQoZnvu65pozZ86ceU8mME/eajEMw0BERETER1i9XQARERERV1K4EREREZ+icCMiIiI+ReFGREREfIrCjYiIiPgUhRsRERHxKQo3IiIi4lMUbkRERMSnKNyIiIiIT1G4ERE5RTzyyCNYLBays7O9XRSRRk3hRsTHzJ49G4vFwpo1a7xdFBERr1C4EREREZ+icCMiIiI+ReFG5DS1fv16LrroIiIiIggLC2Pw4MH88MMP1Y4pLy/n0UcfpX379gQFBdG0aVPOPfdcFi5c6DwmIyODMWPG0KJFC2w2GwkJCQwbNozff//9uO/97LPPYrFY2L179zHPTZo0icDAQA4fPgzAr7/+ylVXXUV8fDxBQUG0aNGCa6+9ltzc3AZd9969e7n55puJi4vDZrNxxhlnMGvWrGrHLF26FIvFwty5c3nggQeIj48nNDSUyy+/nPT09GPOOX/+fHr16kVwcDAxMTHccMMN7N2795jjtm7dyjXXXEOzZs0IDg6mY8eOPPjgg8ccl5OTw0033URUVBSRkZGMGTOGoqKiascsXLiQc889l6ioKMLCwujYsSMPPPBAg34mIr7G39sFEBHP+/nnnznvvPOIiIjg3nvvJSAggFdffZVBgwaxbNkyUlJSALOD67Rp07j11lvp27cveXl5rFmzhnXr1nHBBRcAcNVVV/Hzzz9z5513kpycTFZWFgsXLiQtLY3k5OQa3/+aa67h3nvvZd68efz973+v9ty8efO48MILadKkCWVlZaSmplJaWsqdd95JfHw8e/fu5bPPPiMnJ4fIyMh6XXdmZiZnn302FouFCRMm0KxZM7788ktuueUW8vLyuPvuu6sd//jjj2OxWLjvvvvIyspi+vTpDBkyhA0bNhAcHAyYfZzGjBlDnz59mDZtGpmZmfzrX/9i+fLlrF+/nqioKAA2btzIeeedR0BAAOPGjSM5OZmdO3fyv//9j8cff/yYn0/r1q2ZNm0a69at44033iA2NpannnrK+fldeumldO/enalTp2Kz2dixYwfLly+v189DxGcZIuJT3nrrLQMwVq9efdxjhg8fbgQGBho7d+507tu3b58RHh5uDBgwwLmvR48exiWXXHLc8xw+fNgAjGeeeabe5ezXr5/Rq1evavtWrVplAMbbb79tGIZhrF+/3gCM+fPn1/v8NbnllluMhIQEIzs7u9r+a6+91oiMjDSKiooMwzCMJUuWGIDRvHlzIy8vz3ncvHnzDMD417/+ZRiGYZSVlRmxsbFG165djeLiYudxn332mQEYU6ZMce4bMGCAER4ebuzevbvaezscDuf2ww8/bADGzTffXO2YK664wmjatKnz8T//+U8DMA4cONDQH4WIT1OzlMhpxm638/XXXzN8+HDatGnj3J+QkMD111/P999/T15eHgBRUVH8/PPP/PrrrzWeKzg4mMDAQJYuXepsRqqrESNGsHbtWnbu3OncN3fuXGw2G8OGDQNw1sx89dVXxzTL1JdhGPz3v//lsssuwzAMsrOznbfU1FRyc3NZt25dtdeMGjWK8PBw5+Orr76ahIQEvvjiCwDWrFlDVlYWd9xxB0FBQc7jLrnkEjp16sTnn38OwIEDB/j222+5+eabadmyZbX3sFgsx5T1tttuq/b4vPPO4+DBg9U+F4BPPvkEh8PRwJ+IiO9SuBE5zRw4cICioiI6dux4zHOdO3fG4XA4+5VMnTqVnJwcOnToQLdu3fj73//Oxo0bncfbbDaeeuopvvzyS+Li4hgwYABPP/00GRkZJyzHn//8Z6xWK3PnzgXM8DF//nxnPyCA1q1bM3HiRN544w1iYmJITU3lpZdealB/mwMHDpCTk8Nrr71Gs2bNqt3GjBkDQFZWVrXXtG/fvtpji8VCu3btnP2JqvoM1fSz7NSpk/P53377DYCuXbvWqax/DEBNmjQBcAbIESNG0L9/f2699Vbi4uK49tprmTdvnoKOSCWFGxE5rgEDBrBz505mzZpF165deeONNzjrrLN44403nMfcfffdbN++nWnTphEUFMTkyZPp3Lkz69evr/XciYmJnHfeecybNw+AH374gbS0NEaMGFHtuOeee46NGzfywAMPUFxczF//+lfOOOMM9uzZU69rqfriv+GGG1i4cGGNt/79+9frnO7i5+dX437DMACzxuzbb79l0aJF3HjjjWzcuJERI0ZwwQUXYLfbPVlUkVOSwo3IaaZZs2aEhISwbdu2Y57bunUrVquVpKQk577o6GjGjBnD+++/T3p6Ot27d+eRRx6p9rq2bdvyf//3f3z99dds3ryZsrIynnvuuROWZcSIEfz0009s27aNuXPnEhISwmWXXXbMcd26deOhhx7i22+/5bvvvmPv3r3MnDmz3tcdHh6O3W5nyJAhNd5iY2OrveaPzXGGYbBjxw5nR+lWrVoB1Piz3LZtm/P5qua/zZs316vMtbFarQwePJjnn3+eX375hccff5xvvvmGJUuWuOw9RBorhRuR04yfnx8XXnghn3zySbXh2pmZmbz33nuce+65zmahgwcPVnttWFgY7dq1o7S0FICioiJKSkqqHdO2bVvCw8Odx9Tmqquuws/Pj/fff5/58+dz6aWXEhoa6nw+Ly+PioqKaq/p1q0bVqu12vnT0tLYunXrCa/7qquu4r///W+NIePAgQPH7Hv77bfJz893Pv7ggw/Yv38/F110EQC9e/cmNjaWmTNnVivPl19+yZYtW7jkkksAM1gNGDCAWbNmkZaWVu09qmpj6uPQoUPH7OvZsydAnX7uIr5OQ8FFfNSsWbNYsGDBMfvvuusu/vGPfzjnSbnjjjvw9/fn1VdfpbS0lKefftp5bJcuXRg0aBC9evUiOjqaNWvW8MEHHzBhwgQAtm/fzuDBg7nmmmvo0qUL/v7+fPTRR2RmZnLttdeesIyxsbGcf/75PP/88+Tn5x/TJPXNN98wYcIE/vznP9OhQwcqKip45513nEGlyqhRo1i2bNkJg8KTTz7JkiVLSElJYezYsXTp0oVDhw6xbt06Fi1adExoiI6O5txzz2XMmDFkZmYyffp02rVrx9ixYwEICAjgqaeeYsyYMQwcOJDrrrvOORQ8OTmZv/3tb85zvfDCC5x77rmcddZZjBs3jtatW/P777/z+eefs2HDhhP+rI42depUvv32Wy655BJatWpFVlYWL7/8Mi1atODcc8+t17lEfJIXR2qJiBtUDQU/3i09Pd0wDMNYt26dkZqaaoSFhRkhISHG+eefb6xYsaLauf7xj38Yffv2NaKioozg4GCjU6dOxuOPP26UlZUZhmEY2dnZxvjx441OnToZoaGhRmRkpJGSkmLMmzevzuV9/fXXDcAIDw+vNpzaMAzjt99+M26++Wajbdu2RlBQkBEdHW2cf/75xqJFi6odN3DgQKOu/51lZmYa48ePN5KSkoyAgAAjPj7eGDx4sPHaa685j6kaCv7+++8bkyZNMmJjY43g4GDjkksuOWYot2EYxty5c40zzzzTsNlsRnR0tDFy5Ehjz549xxy3efNm44orrjCioqKMoKAgo2PHjsbkyZOdz1cNBf/jEO+qz3TXrl2GYRjG4sWLjWHDhhmJiYlGYGCgkZiYaFx33XXG9u3b6/QzEPF1FsNoQJ2oiIgPW7p0Keeffz7z58/n6quv9nZxRKSe1OdGREREfIrCjYiIiPgUhRsRERHxKepzIyIiIj5FNTciIiLiUxRuRERExKecdpP4ORwO9u3bR3h4eI2r8YqIiMipxzAM8vPzSUxMxGqtvW7mtAs3+/btq7ZujoiIiDQe6enptGjRotZjTrtwEx4eDpg/nKr1c0REROTUlpeXR1JSkvN7vDanXbipaoqKiIhQuBEREWlk6tKlRB2KRURExKco3IiIiIhPUbgRERERn3La9bkRERHfYbfbKS8v93YxxEUCAwNPOMy7LhRuRESk0TEMg4yMDHJycrxdFHEhq9VK69atCQwMPKnzKNyIiEijUxVsYmNjCQkJ0aSsPqBqkt39+/fTsmXLk/pMFW5ERKRRsdvtzmDTtGlTbxdHXKhZs2bs27ePiooKAgICGnwedSgWEZFGpaqPTUhIiJdLIq5W1Rxlt9tP6jwKNyIi0iipKcr3uOozVbgRERERn6JwIyIi0oglJyczffr0Oh+/dOlSLBaLT480U7gRERHxAIvFUuvtkUceadB5V69ezbhx4+p8/DnnnMP+/fuJjIxs0Ps1Bhot5Sr2cig8YN43aeXt0oiIyClm//79zu25c+cyZcoUtm3b5twXFhbm3DYMA7vdjr//ib+mmzVrVq9yBAYGEh8fX6/XNDaquXGV9B/h+c7w7tXeLomIiJyC4uPjnbfIyEgsFovz8datWwkPD+fLL7+kV69e2Gw2vv/+e3bu3MmwYcOIi4sjLCyMPn36sGjRomrn/WOzlMVi4Y033uCKK64gJCSE9u3b8+mnnzqf/2Oz1OzZs4mKiuKrr76ic+fOhIWFMXTo0GphrKKigr/+9a9ERUXRtGlT7rvvPkaPHs3w4cPd+SNrMIUbV7FFmPcled4th4jIacgwDIrKKrxyMwzDZddx//338+STT7Jlyxa6d+9OQUEBF198MYsXL2b9+vUMHTqUyy67jLS0tFrP8+ijj3LNNdewceNGLr74YkaOHMmhQ4eOe3xRURHPPvss77zzDt9++y1paWncc889zuefeuop3n33Xd566y2WL19OXl4eH3/8sasu2+XULOUqtnDzvjTfu+UQETkNFZfb6TLlK6+89y9TUwkJdM3X6dSpU7ngggucj6Ojo+nRo4fz8WOPPcZHH33Ep59+yoQJE457nptuuonrrrsOgCeeeIIXXniBVatWMXTo0BqPLy8vZ+bMmbRt2xaACRMmMHXqVOfzL774IpMmTeKKK64AYMaMGXzxxRcNv1A3U82Nq1TV3JQXguPkJh8SEZHTU+/evas9Ligo4J577qFz585ERUURFhbGli1bTlhz0717d+d2aGgoERERZGVlHff4kJAQZ7ABSEhIcB6fm5tLZmYmffv2dT7v5+dHr1696nVtnqSaG1epqrkBKM2D4CbeK4uIyGkmOMCPX6ameu29XSU0NLTa43vuuYeFCxfy7LPP0q5dO4KDg7n66qspKyur9Tx/XLrAYrHgcDjqdbwrm9s8TeHGVfwDwT8IKkrMpimFGxERj7FYLC5rGjqVLF++nJtuusnZHFRQUMDvv//u0TJERkYSFxfH6tWrGTBgAGAuj7Bu3Tp69uzp0bLUle/9JniTLdwMN+pULCIiLtC+fXs+/PBDLrvsMiwWC5MnT661BsZd7rzzTqZNm0a7du3o1KkTL774IocPHz5ll8BQnxtXqup3o07FIiLiAs8//zxNmjThnHPO4bLLLiM1NZWzzjrL4+W47777uO666xg1ahT9+vUjLCyM1NRUgoKCPF6WurAYjblRrQHy8vKIjIwkNzeXiIgI15781YGwfwNcPx86XOjac4uICAAlJSXs2rWL1q1bn7Jfrr7O4XDQuXNnrrnmGh577DGXnbe2z7Y+399qlnKloKqaGzVLiYiI79i9ezdff/01AwcOpLS0lBkzZrBr1y6uv/56bxetRmqWciWbwo2IiPgeq9XK7Nmz6dOnD/3792fTpk0sWrSIzp07e7toNfJ6uHnppZdITk4mKCiIlJQUVq1aVevxOTk5jB8/noSEBGw2Gx06dDh1JhLSRH4iIuKDkpKSWL58Obm5ueTl5bFixQrnyKlTkVebpebOncvEiROZOXMmKSkpTJ8+ndTUVLZt20ZsbOwxx5eVlXHBBRcQGxvLBx98QPPmzdm9ezdRUVGeL3xNtASDiIiI13k13Dz//POMHTuWMWPGADBz5kw+//xzZs2axf3333/M8bNmzeLQoUOsWLHCOeFQcnKyJ4tcO9XciIiIeJ3XmqXKyspYu3YtQ4YMOVIYq5UhQ4awcuXKGl/z6aef0q9fP8aPH09cXBxdu3bliSeewG4//nIHpaWl5OXlVbu5jcKNiIiI13kt3GRnZ2O324mLi6u2Py4ujoyMjBpf89tvv/HBBx9gt9v54osvmDx5Ms899xz/+Mc/jvs+06ZNIzIy0nlLSkpy6XVUo9FSIiIiXuf1DsX14XA4iI2N5bXXXqNXr16MGDGCBx98kJkzZx73NZMmTSI3N9d5S09Pd18BNVpKRETE67zW5yYmJgY/Pz8yMzOr7c/MzCQ+Pr7G1yQkJBAQEICf35FFyjp37kxGRgZlZWUEBgYe8xqbzYbNZnNt4Y9HzVIiIiJe57Wam8DAQHr16sXixYud+xwOB4sXL6Zfv341vqZ///7s2LGj2roa27dvJyEhocZg43EaLSUiIm40aNAg7r77bufj5ORkpk+fXutrLBYLH3/88Um/t6vO4wlebZaaOHEir7/+Ov/+97/ZsmULt99+O4WFhc7RU6NGjWLSpEnO42+//XYOHTrEXXfdxfbt2/n888954oknGD9+vLcuoTrV3IiIyHFcdtllDB06tMbnvvvuOywWCxs3bqzXOVevXs24ceNcUTynRx55pMbVvvfv389FF13k0vdyF68OBR8xYgQHDhxgypQpZGRk0LNnTxYsWODsZJyWlobVeiR/JSUl8dVXX/G3v/2N7t2707x5c+666y7uu+8+b11Cdc5wo5obERGp7pZbbuGqq65iz549tGjRotpzb731Fr1796Z79+71OmezZs1cWcRaHa/LyKnI6x2KJ0yYwO7duyktLeXHH38kJSXF+dzSpUuZPXt2teP79evHDz/8QElJCTt37uSBBx6o1gfHq6pGS1WUQEWZd8siIiKnlEsvvZRmzZod871WUFDA/PnzGT58ONdddx3NmzcnJCSEbt268f7779d6zj82S/36668MGDCAoKAgunTpwsKFC495zX333UeHDh0ICQmhTZs2TJ48mfLycgBmz57No48+yk8//YTFYsFisTjL+8dmqU2bNvGnP/2J4OBgmjZtyrhx4ygoKHA+f9NNNzF8+HCeffZZEhISaNq0KePHj3e+lztp4UxXCgw/sl1WAP7R3iuLiMjpxDCgvMg77x0QAhbLCQ/z9/dn1KhRzJ49mwcffBBL5Wvmz5+P3W7nhhtuYP78+dx3331ERETw+eefc+ONN9K2bVv69u17wvM7HA6uvPJK4uLi+PHHH8nNza3WP6dKeHg4s2fPJjExkU2bNjF27FjCw8O59957GTFiBJs3b2bBggUsWrQIgMjIyGPOUVhYSGpqKv369WP16tVkZWVx6623MmHChGrhbcmSJSQkJLBkyRJ27NjBiBEj6NmzJ2PHjj3h9ZwMhRtX8vM3f8nLi6AkF0IUbkREPKK8CJ5I9M57P7APAkPrdOjNN9/MM888w7Jlyxg0aBBgNkldddVVtGrVinvuucd57J133slXX33FvHnz6hRuFi1axNatW/nqq69ITDR/Fk888cQx/WQeeugh53ZycjL33HMPc+bM4d577yU4OJiwsDD8/f1rbYZ67733KCkp4e233yY01Lz2GTNmcNlll/HUU085u5c0adKEGTNm4OfnR6dOnbjkkktYvHix28ON15ulfI5zrht1KhYRkeo6derEOeecw6xZswDYsWMH3333Hbfccgt2u53HHnuMbt26ER0dTVhYGF999RVpaWl1OveWLVtISkpyBhugxtHHc+fOpX///sTHxxMWFsZDDz1U5/c4+r169OjhDDZgjmh2OBxs27bNue+MM86o1nUkISGBrKyser1XQ6jmxtVs4VCQoXAjIuJJASFmDYq33rsebrnlFu68805eeukl3nrrLdq2bcvAgQN56qmn+Ne//sX06dPp1q0boaGh3H333ZSVua4P58qVKxk5ciSPPvooqampREZGMmfOHJ577jmXvcfRqtaBrGKxWKpN5+IuCjeupiUYREQ8z2Kpc9OQt11zzTXcddddvPfee7z99tvcfvvtWCwWli9fzrBhw7jhhhsAsw/N9u3b6dKlS53O27lzZ9LT09m/fz8JCQkA/PDDD9WOWbFiBa1ateLBBx907tu9e3e1YwIDA2tds7HqvWbPnk1hYaGz9mb58uVYrVY6duxYp/K6k5qlXE1z3YiISC3CwsIYMWIEkyZNYv/+/dx0000AtG/fnoULF7JixQq2bNnCX/7yl2Nm8a/NkCFD6NChA6NHj+ann37iu+++qxZiqt4jLS2NOXPmsHPnTl544QU++uijasckJyeza9cuNmzYQHZ2NqWlpce818iRIwkKCmL06NFs3ryZJUuWcOedd3LjjTces2akNyjcuJrmuhERkRO45ZZbOHz4MKmpqc4+Mg899BBnnXUWqampDBo0iPj4eIYPH17nc1qtVj766COKi4vp27cvt956K48//ni1Yy6//HL+9re/MWHCBHr27MmKFSuYPHlytWOuuuoqhg4dyvnnn0+zZs1qHI4eEhLCV199xaFDh+jTpw9XX301gwcPZsaMGfX/YbiBxTAMw9uF8KS8vDwiIyPJzc0lIiLC9W/w8XjY8B8Y/DCcN9H15xcROc2VlJSwa9cuWrduTVBQkLeLIy5U22dbn+9v1dy4mpqlREREvErhxtUUbkRERLxK4cbVNFpKRETEqxRuXE01NyIiIl6lcONqVeGmRDU3IiLudJqNhzktuOozVbhxNVvlAmNqlhIRcYuqWW+Liry0UKa4TdVszEcv2dAQmqHY1dQsJSLiVn5+fkRFRTnXKAoJCXGusC2Nl8Ph4MCBA4SEhODvf3LxROHG1TSJn4iI21WtWO2JRRjFc6xWKy1btjzpsKpw42pBWhVcRMTdLBYLCQkJxMbGUl5e7u3iiIsEBgZitZ58jxmFG1erqrmxl0FFKfjbvFseEREf5ufnd9L9M8T3qEOxqwWGHdnWiCkRERGPU7hxNasfBKrfjYiIiLco3LiDRkyJiIh4jcKNO2jElIiIiNco3LiDRkyJiIh4jcKNO6hZSkRExGsUbtzBVllzo9FSIiIiHqdw4w7qcyMiIuI1CjfuYFOfGxEREW9RuHEHZ4di1dyIiIh4msKNO6hDsYiIiNco3LhDVbhRh2IRERGPU7hxB/W5ERER8RqFG3dQs5SIiIjXKNy4g7PmJte75RARETkNKdy4g5ZfEBER8RqFG3c4ulnKMLxbFhERkdOMwo07VIUbRwWUF3u3LCIiIqcZhRt3CAwDLOa2mqZEREQ8SuHGHSwWDQcXERHxEoUbd3H2u9GIKREREU9SuHEXjZgSERHxCoUbd9FEfiIiIl6hcOMuVX1utL6UiIiIRyncuItqbkRERLzilAg3L730EsnJyQQFBZGSksKqVauOe+zs2bOxWCzVbkFBQR4sbR05w41qbkRERDzJ6+Fm7ty5TJw4kYcffph169bRo0cPUlNTycrKOu5rIiIi2L9/v/O2e/duD5a4jpwdihVuREREPMnr4eb5559n7NixjBkzhi5dujBz5kxCQkKYNWvWcV9jsViIj4933uLi4jxY4jrSPDciIiJe4dVwU1ZWxtq1axkyZIhzn9VqZciQIaxcufK4rysoKKBVq1YkJSUxbNgwfv755+MeW1paSl5eXrWbR1Q1S6lDsYiIiEd5NdxkZ2djt9uPqXmJi4sjIyOjxtd07NiRWbNm8cknn/Cf//wHh8PBOeecw549e2o8ftq0aURGRjpvSUlJLr+OGqnmRkRExCu83ixVX/369WPUqFH07NmTgQMH8uGHH9KsWTNeffXVGo+fNGkSubm5zlt6erpnCqrRUiIiIl7h7803j4mJwc/Pj8zMzGr7MzMziY+Pr9M5AgICOPPMM9mxY0eNz9tsNmw220mXtd40WkpERMQrvFpzExgYSK9evVi8eLFzn8PhYPHixfTr169O57Db7WzatImEhAR3FbNhtPyCiIiIV3i15gZg4sSJjB49mt69e9O3b1+mT59OYWEhY8aMAWDUqFE0b96cadOmATB16lTOPvts2rVrR05ODs888wy7d+/m1ltv9eZlHMumoeAiIiLe4PVwM2LECA4cOMCUKVPIyMigZ8+eLFiwwNnJOC0tDav1SAXT4cOHGTt2LBkZGTRp0oRevXqxYsUKunTp4q1LqNnRfW4MAywW75ZHRETkNGExDMPwdiE8KS8vj8jISHJzc4mIiHDfG5UVwROVTWWT9oItzH3vJSIi4uPq8/3d6EZLNRoBwWDxM7fV70ZERMRjFG7cxWLRiCkREREvULhxJ42YEhER8TiFG3eqGjFVkuvdcoiIiJxGFG7cSUswiIiIeJzCjTtpCQYRERGPU7hxJ3UoFhER8TiFG3dSh2IRERGPU7hxJzVLiYiIeJzCjTtVhRuNlhIREfEYhRt3skWa96q5ERER8RiFG3dSs5SIiIjHKdy4k0ZLiYiIeJzCjTtptJSIiIjHKdy4k5qlREREPE7hxp2ca0upWUpERMRTFG7cqSrclOWDw+HdsoiIiJwmFG7cqapZCsyAIyIiIm6ncONO/jawBpjb6ncjIiLiEQo37mSxaMSUiIiIhyncuJtzCQZ1KhYREfEEhRt3s6nmRkRExJMUbtzNGW5UcyMiIuIJCjfupiUYREREPErhxt3UoVhERMSjFG7cTUswiIiIeJTCjbtptJSIiIhHKdy4m0ZLiYiIeJTCjbupQ7GIiIhHKdy4m4aCi4iIeJTCjbtptJSIiIhHKdy4mzoUi4iIeJTCjbtpKLiIiIhHKdy4m0ZLiYiIeJTCjbtVhZvyQrBXeLcsIiIipwGFG3erapYCKFPtjYiIiLsp3LibfyD4B5nbapoSERFxO4UbT9CIKREREY9RuPEEdSoWERHxGIUbT9BwcBEREY9RuPEErS8lIiLiMQo3nhAUad4r3IiIiLidwo0nqFlKRETEY06JcPPSSy+RnJxMUFAQKSkprFq1qk6vmzNnDhaLheHDh7u3gCdLo6VEREQ8xuvhZu7cuUycOJGHH36YdevW0aNHD1JTU8nKyqr1db///jv33HMP5513nodKehI0WkpERMRjvB5unn/+ecaOHcuYMWPo0qULM2fOJCQkhFmzZh33NXa7nZEjR/Loo4/Spk0bD5a2gdShWERExGO8Gm7KyspYu3YtQ4YMce6zWq0MGTKElStXHvd1U6dOJTY2lltuueWE71FaWkpeXl61m8epz42IiIjHeDXcZGdnY7fbiYuLq7Y/Li6OjIyMGl/z/fff8+abb/L666/X6T2mTZtGZGSk85aUlHTS5a43jZYSERHxGK83S9VHfn4+N954I6+//joxMTF1es2kSZPIzc113tLT091cyhqoQ7GIiIjH+HvzzWNiYvDz8yMzM7Pa/szMTOLj4485fufOnfz+++9cdtllzn0OhwMAf39/tm3bRtu2bau9xmazYbPZ3FD6elCzlIiIiMd4teYmMDCQXr16sXjxYuc+h8PB4sWL6dev3zHHd+rUiU2bNrFhwwbn7fLLL+f8889nw4YN3mlyqguNlhIREfEYr9bcAEycOJHRo0fTu3dv+vbty/Tp0yksLGTMmDEAjBo1iubNmzNt2jSCgoLo2rVrtddHRUUBHLP/lKLRUiIiIh7j9XAzYsQIDhw4wJQpU8jIyKBnz54sWLDA2ck4LS0Nq7VRdQ06VlW4qSiBijLwD/RueURERHyYxTAMw9uF8KS8vDwiIyPJzc0lIiLCM29qr4DHmprb9+6CkGjPvK+IiIiPqM/3dyOvEmkk/PwhIMTcLsn1bllERER8nMKNp6hTsYiIiEco3HiKhoOLiIh4hMKNp2jElIiIiEco3HhKkJqlREREPEHhxlOcSzCoQ7GIiIg7Kdx4ijoUi4iIeITCjaco3IiIiHiEwo2nqEOxiIiIRyjceIqGgouIiHiEwo2naLSUiIiIRyjceIpztJSapURERNxJ4cZTnB2KFW5ERETcSeHGUzRaSkRExCMUbjxFo6VEREQ8QuHGUzRaSkRExCMUbjylarSUvQwqSr1bFhERER+mcOMpgWFHtjViSkRExG0UbjzF6geB6ncjIiLibgo3nqROxSIiIm6ncONJ6lQsIiLidgo3nqQlGERERNxO4caTtASDiIiI2ynceJKapURERNxO4caTtL6UiIiI2ynceJLCjYiIiNsp3HiSmqVERETcrkHhJj09nT179jgfr1q1irvvvpvXXnvNZQXzSRotJSIi4nYNCjfXX389S5YsASAjI4MLLriAVatW8eCDDzJ16lSXFtCnaLSUiIiI2zUo3GzevJm+ffsCMG/ePLp27cqKFSt49913mT17tivL51vULCUiIuJ2DQo35eXl2Gw2ABYtWsTll18OQKdOndi/f7/rSudrbGqWEhERcbcGhZszzjiDmTNn8t1337Fw4UKGDh0KwL59+2jatKlLC+hTnOEm17vlEBER8WENCjdPPfUUr776KoMGDeK6666jR48eAHz66afO5iqpgZqlRERE3M6/IS8aNGgQ2dnZ5OXl0aRJE+f+cePGERIS4rLC+Zyq0VIleWAYYLF4tzwiIiI+qEE1N8XFxZSWljqDze7du5k+fTrbtm0jNjbWpQX0KVU1N4Ydyou9WxYREREf1aBwM2zYMN5++20AcnJySElJ4bnnnmP48OG88sorLi2gTwkMAypra9Q0JSIi4hYNCjfr1q3jvPPOA+CDDz4gLi6O3bt38/bbb/PCCy+4tIA+xWLREgwiIiJu1qBwU1RURHi42cTy9ddfc+WVV2K1Wjn77LPZvXu3Swvoc5ydihVuRERE3KFB4aZdu3Z8/PHHpKen89VXX3HhhRcCkJWVRUREhEsL6HO0BIOIiIhbNSjcTJkyhXvuuYfk5GT69u1Lv379ALMW58wzz3RpAX2OlmAQERFxqwYNBb/66qs599xz2b9/v3OOG4DBgwdzxRVXuKxwPklz3YiIiLhVg8INQHx8PPHx8c7VwVu0aKEJ/OpCSzCIiIi4VYOapRwOB1OnTiUyMpJWrVrRqlUroqKieOyxx3A4HK4uo29Rh2IRERG3alC4efDBB5kxYwZPPvkk69evZ/369TzxxBO8+OKLTJ48ud7ne+mll0hOTiYoKIiUlBRWrVp13GM//PBDevfuTVRUFKGhofTs2ZN33nmnIZfhHQo3IiIibtWgZql///vfvPHGG87VwAG6d+9O8+bNueOOO3j88cfrfK65c+cyceJEZs6cSUpKCtOnTyc1NfW4sx1HR0fz4IMP0qlTJwIDA/nss88YM2YMsbGxpKamNuRyPCso0rxXs5SIiIhbNKjm5tChQ3Tq1OmY/Z06deLQoUP1Otfzzz/P2LFjGTNmDF26dGHmzJmEhIQwa9asGo8fNGgQV1xxBZ07d6Zt27bcdddddO/ene+//74hl+J5Gi0lIiLiVg0KNz169GDGjBnH7J8xYwbdu3ev83nKyspYu3YtQ4YMOVIgq5UhQ4awcuXKE77eMAwWL17Mtm3bGDBgQI3HlJaWkpeXV+3mVRotJSIi4lYNapZ6+umnueSSS1i0aJFzjpuVK1eSnp7OF198UefzZGdnY7fbiYuLq7Y/Li6OrVu3Hvd1ubm5NG/enNLSUvz8/Hj55Ze54IILajx22rRpPProo3UuU0MVl9n5LbuAknIHvVo1Of6BWn5BRETErRpUczNw4EC2b9/OFVdcQU5ODjk5OVx55ZX8/PPPHuncGx4ezoYNG1i9ejWPP/44EydOZOnSpTUeO2nSJHJzc5239PR0t5Rpze5DXPLC99z/3421H6iaGxEREbdq8Dw3iYmJx3Qc/umnn3jzzTd57bXX6nSOmJgY/Pz8yMzMrLY/MzOT+Pj4477OarXSrl07AHr27MmWLVuYNm0agwYNOuZYm82GzWarU3lORvOoYAD25hRjGAYWi6XmAzXPjYiIiFs1qObGVQIDA+nVqxeLFy927nM4HCxevNjZ3FUXDoeD0tJSdxSxzhIrw01RmZ3c4vLjH1i1tpQ6FIuIiLhFg2tuXGXixImMHj2a3r1707dvX6ZPn05hYSFjxowBYNSoUTRv3pxp06YBZh+a3r1707ZtW0pLS/niiy945513eOWVV7x5GQQF+BETFkh2QRl7DhcTFRJY84FHz3NjGHC8Gh4RERFpEK+HmxEjRnDgwAGmTJlCRkYGPXv2ZMGCBc5OxmlpaVitRyqYCgsLueOOO9izZw/BwcF06tSJ//znP4wYMcJbl+CUGBVMdkEZ+3KK6do8suaDqpqlMKCsEGxhHiufiIjI6cBiGIZR14OvvPLKWp/Pyclh2bJl2O32ky6Yu+Tl5REZGUlubi4REREnfkE93P6ftXy5OYOHL+vCmP6taz7IMGBqUzDsMHELRCS6tAwiIiK+qD7f3/WquYmMPE5txFHPjxo1qj6n9ClV/W725RQf/yCLxWyaKslRp2IRERE3qFe4eeutt9xVDp9w9IipWgVFKNyIiIi4iVdHS/ma5k0qw83hE4Sbqn43JbluLpGIiMjpR+HGhY7U3JTUfqAm8hMREXEbhRsXqgo32QWllJTX0qlaE/mJiIi4jcKNC0WFBBAc4AfA/txaam+OnutGREREXErhxoUsFkvd+t2oWUpERMRtFG5crE7DwYPULCUiIuIuCjcuVtXvZk9t4aaq5kajpURERFxO4cbFmkcFASeouVGHYhEREbdRuHGxuvW5qQo36lAsIiLiago3LpYYWdnnJlcdikVERLxB4cbFqmpu9ueU4HAcZ01ShRsRERG3UbhxsfiIIKwWKLM7yC4orfmgqtFSJWqWEhERcTWFGxfz97MSH2F2Kj7uiCl1KBYREXEbhRs3OGGn4qpmqbJ8cDg8VCoREZHTg8KNG5xwIr+qmhswA46IiIi4jMKNGxxZHfw44cbfBtYAc1tNUyIiIi6lcOMGJ6y5sVi0BIOIiIibKNy4QVWfmz11WTxTI6ZERERcSuHGDZrXZfFMzXUjIiLiFgo3blDVLJVXUkF+SXnNB9kizXstwSAiIuJSCjduEGbzJzLY7DC8L6ek5oOcNTcKNyIiIq6kcOMmR0ZMFdV8QFRL837vOg+VSERE5PSgcOMmzon8jldz0yHVvN++QBP5iYiIuJDCjZs4a26ON2Iq+TxzMr+CTNi71oMlExER8W0KN25ywhFT/oHQ/gJze+tnHiqViIiI71O4cZPEE81SDNDxYvN+2xceKJGIiMjpQeHGTU64eCaYNTfWAMjeDtk7PFQyERER36Zw4yaJUUEAZOaXUG4/TofhoEhIPtfc3va5h0omIiLi2xRu3CQm1EagvxXDgIzc44yYAuh0iXm/VU1TIiIirqBw4yZWq4XESLP2pvZ+NxeZ9+k/QkGWB0omIiLi2xRu3KhO/W4iW0BCT8Aw57wRERGRk6Jw40Z1WkAT1DQlIiLiQgo3blSn4eBwZEj4b0ugrNDNpRIREfFtCjdu1Lyu4SbuDHOtqYoS2LnEAyUTERHxXQo3blTncGOxQKdLze2tGhIuIiJyMhRu3KiqQ/G+nGIMw6j94Kqmqe0LwF7h5pKJiIj4LoUbN4qvHApeUu7gUGFZ7Qe37AfBTaD4kDksXERERBpE4caNbP5+xIbbgDo0Tfn5Q/tUc1trTYmIiDSYwo2bJdZ1ODhAp8qmqa2fw4masURERKRGCjduVtXvZk9tE/lVaTsY/GxweBcc2OrmkomIiPgmhRs3OzKRXy3rS1WxhUGbQeb21s/cVygREREfdkqEm5deeonk5GSCgoJISUlh1apVxz329ddf57zzzqNJkyY0adKEIUOG1Hq8tx0ZDl5Utxc4m6bU70ZERKQhvB5u5s6dy8SJE3n44YdZt24dPXr0IDU1laysmheRXLp0Kddddx1Llixh5cqVJCUlceGFF7J3714Pl7xu6lVzA9DhIsAC+9ZB3n73FUxERMRHeT3cPP/884wdO5YxY8bQpUsXZs6cSUhICLNmzarx+HfffZc77riDnj170qlTJ9544w0cDgeLFy/2cMnrps5LMFQJj4MWvc1tjZoSERGpN6+Gm7KyMtauXcuQIUOc+6xWK0OGDGHlypV1OkdRURHl5eVER0fX+HxpaSl5eXnVbp5U1aH4UGEZxWX2ur2oakI/hRsREZF682q4yc7Oxm63ExcXV21/XFwcGRkZdTrHfffdR2JiYrWAdLRp06YRGRnpvCUlJZ10uesjIsifMJs/UI/am6qlGH5bBiWeDWMiIiKNndebpU7Gk08+yZw5c/joo48ICgqq8ZhJkyaRm5vrvKWnp3u0jBaL5ah+N3UMN806QNN24CiHHYvcWDoRERHf49VwExMTg5+fH5mZmdX2Z2ZmEh8fX+trn332WZ588km+/vprunfvftzjbDYbERER1W6elhhlBq8619yAmqZEREQayKvhJjAwkF69elXrDFzVObhfv37Hfd3TTz/NY489xoIFC+jdu7cninpSjl5As846XWLe//o12MvdUCoRERHf5PVmqYkTJ/L666/z73//my1btnD77bdTWFjImDFjABg1ahSTJk1yHv/UU08xefJkZs2aRXJyMhkZGWRkZFBQUOCtSzgh54ipusxSXKVFHwiJgZJc2L3cTSUTERHxPV4PNyNGjODZZ59lypQp9OzZkw0bNrBgwQJnJ+O0tDT27z8y38srr7xCWVkZV199NQkJCc7bs88+661LOKGqPjd76lNzY/WDjheZ21s/d0OpREREfJPFME6vFRrz8vKIjIwkNzfXY/1v1u4+xFWvrKRFk2C+v+9PdX/hti/h/WshogX8bTNYLO4rpIiIyCmsPt/fXq+5OR1UNUtl5JZgd9QjS7YZBAEhkLcHMja6p3AiIiI+RuHGA2LDg/C3WqhwGGTl13EZBoCAYGhbWdOjtaZERETqROHGA/ysFuIjK4eD16dTMRw1JFz9bkREROpC4cZDmtd3jakqHYaCxQoZm+DwbjeUTERExLco3HhIg8NNaFNoWTnnz7YvXVwqERER36Nw4yENmsivipqmRERE6kzhxkMaNJFflU6V4WbXd7C7bquli4iInK4UbjzkyOKZ9RgtVSW6DfQcCRjwyR1QVuTawomIiPgQhRsPSTyqz02D5k1MfQLCE+HQb7B4qotLJyIi4jsUbjykquamoLSCvOKK+p8gOAouf9Hc/nEm/K71pkRERGqicOMhwYF+NA0NBBowYqpK+yFw5o0caZ4qdF0BRUREfITCjQclNnQ4+NFSHzfXmjr8Oyx6xCXlEhER8SUKNx50pFPxSYSboEi4/AVze9Vr5ggqERERcVK48SCX1NwAtBsMvW4ytz+5A0oLTu58IiIiPkThxoOqJvI76XADcOE/ILIl5KTBoodP/nwiIiI+QuHGg5pHNXDxzJrYwmFY5eip1W/Ab8tO/pwiIiI+QOHGg5pHhQAn2efmaG0GQe9bzO1PJkBpvmvOKyIi0ogp3HhQYmXNTVZ+KaUVdtec9IKpENUSctPg68muOaeIiEgjpnDjQdGhgQQFmD/yjNwGLMNQE1sYDHvJ3F77Fuz8xjXnFRERaaQUbjzIYrGc3AKax9N6APQdZ25/cieU5Lnu3CIiIo2Mwo2HNXfVcPA/GvIINEmGvD3w9YOuPbeIiEgjonDjYS1cORz8aIGhMOxlc3vd27BjkWvPLyIi0kgo3HhYYqQbmqWqJPeHlNvM7U//CsU5rn8PERGRU5zCjYdVTeS3L9cN4QZg8BSIbgN5e+HVAbD9a/e8j4iIyClK4cbD3NKh+GiBoXD1LAhPhJzd8N6fYc5IyEl3z/s1JnvWQtYWb5dCRETcTOHGw5yLZ+aW4HAY7nmTxDNhwio4506w+MHWz+ClvvD9dLCXu+c9T3VpP8Ibg+Hls+G/t8Lh3d4ukYiIuInCjYfFRwZhtUBZhYPswlL3vZEt3Fx/6rbvoGU/KC8y16CaeS78vtx973sqMozK9bcqw+Sm+TCjjznpofoliYj4HIUbDwvwsxIXYc5UvC/HRRP51SbuDBjzJQx/BUKawoGtMPti+Og2KDjg/vc/FWz/CtJWgn8QXD/PnBfIXgorXoAXesIPr0BFmbdLKSIiLqJw4wVu73fzRxYL9LweJqyB3jcDFvjpfZjRy1x00+GipSBORQ47LHrE3E65DTqkwqhP4fr50KwTFB+GBfebzXY/f2zW8oiISKOmcOMFzn43rp7r5kRCouHSf8KtiyGhB5Tkwuf/Z/ZF2bvOs2XxlI1z4cAWCIqCc+8291ks0OFCuG05XPYvCI2Fw7tg/mh480Kzf46IiDRaCjde0NxdE/nVVYteMHYJXPws2CJg33p4/U9m0PGlPijlJfDN4+b2eRMhuEn15/38oddN8Nf1MPB+CAiBPatg1oUw90Y4uNPjRRYRkZPn7+0CnI4S3bUEQ31Y/aDvWOh8OXz9EGyaZzZR/fKJ2RG5+wizhqMxW/2GuRxFRPMja2/VxBYG50+C3mNgyeOw/j+w5VPzBmD1N0edWf0q761/eFx536QVdL7M/JlGJHjmGkVE5BiqufGCFp7uc1Ob8Di46nUY/T+I6QCFB+Cjv8DsSyFrq7dL13AlufDds+b2oEkQEHzi14THw+Uvms1V7S88st9RYXZALi+Csnzz3MWHzJ9VQYY5YWJuGvz+HXx5LzzfGWYNhR9mQt4+91yfiIgcl8UwTq8elHl5eURGRpKbm0tERIRXyrAtI5/U6d8SFRLAhikXnvgFnlJRBitnwLKnoaLYrLHoNwEG3mtODtiYLJ4K3z0HMR3h9hVmE1R9leRCRanZKdlRAYbd3DYclfeV+x2V+/esMjsl71lV/TxJKdBlOHQZBpHNXXF1IiKnnfp8fyvceEF+STndHjGXRfj50VRCbadY62BOGnx5P2z73Hwc0QIuego6XdI4mqry9sMLZ5oB7dr3zHJ7Uu4e2PI/M+ik/1D9uRZ94YzhZtNVVJJnyyUi0ojV5/tbzVJeEB4UQESQGWi82u/meKJawnXvwXVzzO28PTB3JLw3Ag7t8nbpTmzZU2awSUqBjhd7/v0jW8DZt8MtX8HELTD0KWh5DmAxa3W+egCmd4UPx50+cw2JiHiQwo2XNG8SApyi4aZKx4vgjh/hvHvAGgC/fmUuX7DsaXMk0qkoewese9vcHvKI92uaIhLh7Nvg5i/NoHPRM9CqP2Axh6nP6A1r/w0Oh3fLKSLiQxRuvKRqrpsfdh70cklOIDAEBk82+620HgAVJeaIohm9Yf27p94EgN9MNfvCdBgKrc7xdmmqi0iAlHEw5gtzrqH4blCSA//7K7x1kXcW9czaAls/V7gSEZ+icOMlV55ldix9/bvf+PG3UzzgADTrYM7se9WbEJ4AuenwyR3wyjmw5bNTY2bfPWvNoexYYPDD3i5N7Vr0grFLIfUJCAg1++bMPBcWPQplRe5//4zN5lw+L58Nc66H/9156gVVEZEGUrjxkou7JfDnXi1wGHD33A0cLmwEaxtZLNDtanPSuwummrP+Hthq9sd5Ywjs+s57ZXMujgn0uA7iunivLHXl5w/9xsP4H82+QY4K+P55M3D8usg977l/I8wZCTP7V87jYwGL1Zzb56O/gL3CPe8rIuJBGi3lRYWlFVz24vf8ll3IhV3iePXGXli83UekPopzYMWL8MPL5hwwAG3/BIOnQOKZni3LjkXwn6vALxDuXGt2hG5stnwGX/wd8ivnxjnjShg6zZx/52Tt32h2tN76WeUOizlqa8C9kL0N/nurGa46X27WzvkHnvx7ioi4kIaC1+JUCjcAm/fmcuXLKyizO3hseFduPLuVt4tUf/mZ8O0zsPYt8wsS4Iwr4PyHIKad+9/f4YBXB0DmJnNentTH3f+e7lKaD0uegB9nmvPp2CJhyBTodbM5M3J97f8Jlj51ZFg/Fuh6JQz4O8R2PnLc1i/MtbXsZdA+Fa55GwKCXHJJIiKuoHBTi1Mt3AC8+f0uHvvsFwL9rXw6oT+d4k+NctXboV2wdBpsnAcY5pIEZ90IA+8zRw25y8b58OGt5jpZd/1kLhDa2O3bAP+7C/ZvMB9Ht4WY9mYtTniCeR8Wf+RxaIy5DMTRr1/2FGz7onKHBbpeVRlqOtX8njsWmU1WFSXQ5nxzjqDAEPddozQuWVugtACa92pY0BY5SY0q3Lz00ks888wzZGRk0KNHD1588UX69u1b47E///wzU6ZMYe3atezevZt//vOf3H333fV6v1Mx3BiGwc2zV7Nk2wE6xIXxyfhzCQ70O/ELT1UZm+Gbx2D7AvOxX6C55lKvmyD5PNcOz64oM0du5eyGP02GAfe47tze5rCb62MtngplBbUfa/GDsDgz7PgFHjV5YGU/qQF/h2YdT/yeu74z5zMqLzSHrF8/F2zhJ30p0sjt2wBvXmDW7EW1gh7XmuvPNW3r7ZLJaaTRhJu5c+cyatQoZs6cSUpKCtOnT2f+/Pls27aN2NjYY45fvXo18+bNo1evXvztb3/jvvvu84lwA5BdUMpF//qOA/mljExpyeNXdPN2kU7e7pWw+FFIW3lkX3QbOGs09BwJYc1O/j1+fNVczykszuzo3NiWiaiLwmzYsxryM6AgE/L3m9tV94UHzCaso1ms0LUq1HSo3/ul/QjvXg2ledCiD4z8AIKjXHY50siU5sOrA+HQzmOfS0oxO/CfcYV+R8TtGk24SUlJoU+fPsyYMQMAh8NBUlISd955J/fff3+tr01OTubuu+/2mXAD8P2v2dw460cMA2becBZDu/rIytL71psT1W2af6QGwhpgLovQazS0HtSwau7SfPhXTyjKhkv/Cb1vdmGhGxF7hRlwqsJO0UFo2e/k+jvtWw/vXAHFhyG+O9z4MYQ2dVmRpRH56Db46X1zGZZbF8Lvy83Hvy05Eqr9bOaknz2vNwcV+AV4t8ziXSW55v9LLv4/o1GEm7KyMkJCQvjggw8YPny4c//o0aPJycnhk08+qfX1dQ03paWllJaWOh/n5eWRlJR0SoYbgCe/3MrMZTuJCPLny7sHOCf78wmlBfDzh7B2Nuxde2R/VCsz5PS8wVyl/GiGYf5Dyd1TeUuvvO0x+wBk/WL2Rxn/o/5DdbWMzfD2MDM8xnaBUZ9A2LE1quLDfpoLH40zawJv+rz6xJh5+80/WH563/x3WCW0GXT7s9l0ldDD82UW7ykvMZvSv3vWHHl5+QsuPX19wo3XVmzMzs7GbrcTF1f9yywuLo6tW7e67H2mTZvGo48+6rLzudv/XdiBlb8d5Kf0HO6es573x56Nv5+PdN6zhcFZo8xbxiazNmfjXLO/zOKp5iihDkPN/xydYWYPlOXXft4LHlWwcYf4rjDmS3j7cvPL662LYfSnru8cnp9h1hTt32g2VXb7s/r5nAoO7oTPJ5rbA+8/dsbviATo/1c4507I2Ag/zTEHExQeMKeH+OFlc6HYc+40a2mtp3g/QsOAzM1mX8Gks6H1ed4uUePhsJuf/ZLHzT8+AfasgYpS8Ld5pUheq7nZt28fzZs3Z8WKFfTr18+5/95772XZsmX8+OOPtb7eV2tuANIOFnHxC99RUFrBXYPb87cL6tlnojEpKzRXz14721xU8nhCmpoLUkYmVd5amLeYDo1jwr7G7NBv8O/Lzf+0miTDsJfNFc1DY+s/XLwgy+ycum99ZaDZYDanHS0wHHpeB33HmSPExPMqyswOxPs3QKtzzVBbl3BiL4cdi83anG1fmB2Qwaxd7TfebLYKOMVqo7O2mjXKmz+Eg7+a+6wBcP0caDfEu2U71RkG/LoQFj0CWT+b+8IT4fwHzL5Yfq6tP2kUNTcxMTH4+fmRmZlZbX9mZibx8S6YtKySzWbDZvNOcmyolk1DePyKrtw1ZwMvfvMr57RtSkobH+3vEBgKZ440b5m/wOb/mlXgkS3ML9DIJIhoriHJ3hTdxlwP69+Xw+FdMPuoldZtEWZNW1jsUfex5tD0sFhz5FbGxspAs8FcYf6PLFZo1sns27N3rfkFs+o189bmfDPkdEg99f/y9yWLHzWDTXATuPK1uv/s/QKg41Dzlp8Jq16F1W+anZE/n2j+Zd93HPQZ690+XAd3mmHm5w+rN6n52cwAn70N5twAoz6Glmd7q5Sntj1rYOHDsPt783FQJJw7EVL+ckoEWK93KO7bty8vvvgiYHYobtmyJRMmTDgtOxT/0T3zf+KDtXtIiAziy7vOIypEs8aKF+Xth8/uNvviFGYd+au8XixmbVvimZW3nuYColWj3AwDfltqBpttXwKV/z1FtoQ+t5hNmr4wj1FdlBaYy2JkboaB93pu1u1fF5qj5QCufR86XVz78SdSdR0rX4LcNHOff7D5B83Zd3huOPnh3+Hnj8xQk7HxyH5rALQbbM4I3vEi8A8y11vbsdAM7zd9pr5DR8v+1Qy/W/5nPvazwdm3wbl/M8OwGzWKDsVgDgUfPXo0r776Kn379mX69OnMmzePrVu3EhcXx6hRo2jevDnTpk0DzE7Iv/xipuyLL76YkSNHMnLkSMLCwmjXrm4jQxpTuCksreDSF79nV3YhqWfEMfOGRrY8g/iuqo7ehQfMpqbCA0dtZ0HBAfO+rMhsNqwKM/Hd6t6f5vBuWPMmrHvbHLUF5hdPt6vNv/599QsnP8Oc4mDNm+bPGCA4Gq6eBW3Pd/97v9Lf7ETe9y9w8dOuO7e9An75GFa8YM6cDYDFnAOr/13Qorfr3qtKaYHZRPbT+9UHMVj8oM0gc7buTpcc+6VcVmQu55K2AkJi4OYFaiLN2w/LnoR174BhN2tce14PgyaZNe2eKEJjCTcAM2bMcE7i17NnT1544QVSUlIAGDRoEMnJycyePRuA33//ndatWx9zjoEDB7J06dI6vV9jCjdgLs9wxcvLKbcbjXd5BpGTUV4Mmz4wmzgyNh3Z37y32ZwVHFV5a2Iu5hrcxHxctR0UeaRZpaLMHCpfeMD8Ai/MrgxmlffO5w6Z/4E7HfVHRbU/MCq3g5uYf/13SIWEMxs2tcGBbeYX/8Z5R2rFmrYzazkyN5lfJn+abP6F7I4/chwOeGc47FoGcd3g1kXuWYLDMOD372D5C2btSJWkFHOEVZfhJ187d3CnOWpn/X/M+ZrA/Pkln1cZaC47cbNYSS78+zIziEU0NwOOJ9esy8+EXz4xf3erfr+P/h23RXhmpujCbHMNwR9fhYpic1/HS8w1BI8327mbNKpw42mNLdwAvPHdb/zj8y34Wy28eN2ZXNTNR+a/EakPw4D0VWbI+eWTI+uYnZAFgir/rVfVhLhTaCx0uNBco6vt+bXXVBkG7F5hhpqqGb3B/KI/56/mavH2Mvji/8wvajBrOoa9fOSaXOW758xRiwEh8JdvPVNTkfkLrJxhBjpHubnPGgDtL4Tu15ijJ+sasBwO+O0b80v414U4mzSbtoM+t5rLj9R3KoPCg/DWRWYfnOg2cPNXnpkOYfOHZh+lqhrLmlisZnA/OtDHdDSXvIk74+TLUHDA/L1c/aY5YzmYo8gueNRr/ZAUbmrRGMONw2Hwt3kb+GTDPvysFp6/pgfDejb3drFEvCc/A7Z/Zda+FB82V6gvPmyGF+d2Ts3LVlj8zA7PITHmfWiM2Rn66MchTc0vWcD5JQlmGDny4MjmwZ3w61ew45vqUxdYAyD5XPNLusOF5hckmENnt3xq1l7sW1dVMLOJ5Jy/QsuU6mU2DHNE4Rd/N0NATAcY8W79Z58+nvRVMGuoWVs17CU48wbXnLeu8vbDpnnmOnGZR9XO2SKgy+XmUg+tzq25pqIkz2x2WvUaHNxxZH/7C83OrW3+dHI1HHn7YFYq5KRBXFezD467+pYUHYIv7jEHVoA5v1R0m8rf8aN+z6tqUI4nKcWc1LTLsPp37i3IOirUFJn7EnqazU8dUt1Ta1hHCje1aIzhBsDuMPj7Bz/x4bq9WC3wzNU9uKqXZ9o5RRqtirLKwFP5F3BojPmXrruq8yvKzH4a278ya2IO/Vb9+ZgOZtPIzsVmB1cw+xH1vB7OHn/iWaX3rIG5N0L+PggMg+Evm19gJ6M4B2aeZ3b27Xo1XPWGV7/AyPzlSNA5enRdeKLZ36r7NWbIOLjTDDQb3jsSKG0RZjDrc6trOyof3GnW4BRkmkuS3PixOW+XK/26ED6ZAAUZZgA/7//M5VP8axhIUl5ihvejA0/xIfP3buvnR5pUg6LMpW56jzlxTVx+5pFQUxWeEs+CQfebQfEU6O+pcFOLxhpuwKzBeeCjTcxZnY7FAk9e2Y0RfTzYBiwi9ZO9www52xeYa6wd3ZQWHA19x5rDouuzzlrBAfhgjNlvBaD/3WZfnIbMKWIYMP8ms6Nvk2T4y3eub+5qKIfD/JltnGuW7+gmxciWR0ZegRka+44z++y4awLIzF/MgFOSA60HwvXzXNMnqbQAvn7QrJkDaNoerngVWvRq2PnyM2D9O+YkqVUT6oEZqnuPMfsbHR2Y8jNg+b9gzSyoKDH3Ne9thpp2Q06JUFNF4aYWjTncgBlwHv70Z975YTcAjw07gxv7JXu3UCJyYsU5sPMbs49Ns47mX9QNnb/JXgGLHjb7q4D5ZXv1W/WfO2btbPjfXWD1h5u/bvgXqrtVlMKvX5t9c7YvqOxwbTGb+1LGmfMheeJLeM9ac8busgLodCn8+d8nN1Hd7pXw8W1HavFSbochD7tmnhiHHXYsgjVvmU2mVeuAhcSYtVudLzdryNbOPhJqWvSFQfdB28GnVKiponBTi8YebgAMw+Cxz7Ywa/kuACZf2oVbzj12FJmI+LjN/4VP7jQ7fEYmwTVvQ/Ozqh9jGOaIoaKDZgfZomxzuyATlj1jNkFcMNUcjt0YFOeYNTrNOkG0F/7f2/Ut/OdqsJdC92th+Cv1b+YsLzEnNFzxImCYn93wl6H1ALcUmdw95pQK694+dkZwMPvoDLrfcyGxgRRuauEL4QbMgPPUgm3MXLYTgPuGduL2QR6aDEtETh2Zv8DcG8xZgP1s5uzAxTlmgKm61TbhYtvBMPIDzwwr9hXbvoQ5I82+Lb1vNm+2CLNJzxZR+4zO+3+CD/8CB7aYj8+8AVKneaY50F5h1nytmWWu6p6UAgPvM+f8OYVDTRWFm1r4SrgBM+D8c9GvvLDYXA9l4gUd+Ovg03yiKZHTUUkufHSbuZ7T8QSEmqPAQpuaTRMhTc2aj5TbzGHEUj8b58OHY6k2aq5KYHjlMO0I895WeW+xwuYPzL5XobHmqtkdL/J40QGzRq8RBJqjNYq1peTkWSwWJl7QgUA/C89+vZ3nF26nrMLB/13YQTMZi5xOgiLNoeG/fGR2EA2JqR5iQppqfTZX6/5nsx/L8ulm7VhJ7pG+K2X55i3vOK/tMgwu+ad319fy8e8I1dz4iNe+3ckTX2wF4C8D2nD/RZ0UcEREPKmi1Jx3pzTPHFVVkmeGntLK+5I8s09Uh6E+Hy7cQTU3p6FxA9oS4Gfl0f/9wqvf/kaZ3cGUS7so4IiIeIq/zRzWX5+h/eIW6kHmQ8b0b83jV3QF4K3lv3PFyyv4Zmsmp1nlnIiInOYUbnzMyJRWPHN1d2z+Vjak53Dz7DVcNuN7FmzOwOFQyBEREd+nPjc+Kiu/hDe+28U7K3dTXG5Oxd0pPpwJf2rHRV0T8LOquUpERBoPDQWvxekSbqocKizjze9/498rdlNQak793rZZKBP+1I7Luifi73d6VN6V2x38e8Xv/JZdiJ/Fgp/1yM1qseBnpXK/FT8rWK0WQgL8GNIljhZNNMpERMTbFG5qcbqFmyq5ReW8tWIXs77fRV6JGXJaNQ1h/KB2XHFWcwJ8OOTkl5Qz/r31fLv9QL1fa7HA+R1jub5vS87vFKsaLxERL1G4qcXpGm6q5JeU8/bK3bz5/S4OFZqzljaPCmb0Oa3onRxNl4QIggJqmV2zkcnILWHM7NVs2Z9HcIAfN/VPJsBqwW4Y2B3gMAwq7AYOw8DuMLAbBg6HQYXDIP1QET/uOuQ8V2JkECP6tGREnyTiI12wYJ6IiNSZwk0tTvdwU6WorIJ3f0jj1W9/I7ug1Lk/wM9Cl4QIeiRF0TMpih5JUbRuGoq1EdZYbM3IY8xbq9mfW0JMWCCzbupD9xZR9TrHbwcKeH9VGh+s3cPhonIA/KwWhnSO5fqUVpzXLqZR/mxERBobhZtaKNxUV1JuZ/6adJZsO8CG9Bxnbc7RIoL8j4SdFlH0bBlFTJjNC6Wtu+U7srntnbXkl1bQtlkos8f0JSm64X1nSsrtLNicwXs/prHq9yO1OS2jQ7iub0v+3LvFKf8zERFpzBRuaqFwc3yGYbDncDHr03P4qfK2aW8upRWOY45tHxvGgA7NGNChGSmto0+ppqwP1u7h/v9upMJh0Ld1NK/d2IuokECXnX97Zj7v/ZjGf9ftIb+y/1KAn4W+raNpGR1KiybBlbcQkpoEExNmU+2OiMhJUriphcJN/ZTbHWzLyGdDZdjZkJ7DjgMFHP1bY/O3ktKmKQPaxzCwQzPaxYZ5ZWZkwzB4YfEO/rloOwCX9Ujk2T93x+bvnuBVXGbns437ePfHNDak5xz3uEB/Ky2igml+VOhp0SSYTvERtG0WetqMWBMRORkKN7VQuDl5uUXlfL8jm2+3H+DbXw+wP7ek2vOJkUHOWp3+bWOIDAlwe5nK7Q4mfbiJD9buAeD2QW35+4UdPVZjsjUjj017ctlzuLjyVsSew8Xszy2mtrkTbf5WOiVEcEZiBF0TIzkjMYKO8eGnVE2YiMipQOGmFgo3rmUYBjuyCli2/QDLth/gx12HKDuqGctqgY7xEcSEBRIVEkhUcABRIQHO7SahAUQGB9Kkcl9kcEC9h1vnl5Rzx7vr+O7XbKwWmDqsKzec3crVl9og5XYHGbkl1QLPnsPF7D5YyNaMfOfcQ0fzs1poHxvGGYmRdG0ewRmJkbSPDSMyOEDNWyJy2lK4qYXCjXsVl9n5cddBvt2ezbe/HmBHVkG9Xm+xQLMwG0nRZn8V8z6EFtHBJDUJISEyqFozzv7cYsa8tZqtGfkEB/gx4/ozGdw5ztWX5RYOh8HuQ0X8vC+XzXvz+HlfLj/vy6uxUzeYoadJSCDRoQFEhwbSNNRGdGggTUIDaRoaWLkvkOiwQJKahBBq07q4IuI7FG5qoXDjWXtzitmWkUdOUTmHi8rJLSrjcFE5OcXl5BSVVe4vI7eonPwaajH+yN9qISEqiKQmZuhZtv0AGXklxITZmHVT73oP9T7VGIZBRl6JM+xU3f+x6a8umkcF0y42jPaxYbSPC6NdbDjtKmuAREQaG4WbWijcnLrK7Q5yisrZl1NM+uEi0g9V3ZvNOXsPF1NmP3bkVrvYMN66qc9JDfU+1ZVVODhcVMbBgjIOFZZxqKiMQwWlHCos42BhWbXnsgtKnXPy1CQ23GaGnWZhtIsLp0VUMHaHQYXDQbn9qPtq2w4qHOa+LokRnN+xmTpCi4hHKdzUQuGm8XI4DDLzS8zQc6iI9MNF+FksjOqX7JFOy43J4cIydhwoYEdWAb9mFvBrVj47sgoaVANUk9hwG3/u3YIRvVvSsqnvhkoROXUo3NRC4UZOZ/kl5ew8UMivmWbY2ZFVQGZ+CX5WKwFWC/5+FgL8rPhbLfj7WQnws+BvteLvZ8HfasFhwJKtWRw8ql9Q/3ZNubZPSy48I85tw+5FRBRuaqFwI3JyyiocLNqSyfur0vh+R7ZzzqPo0ECuPLM51/ZNol1suHcL+QcVdgdWi0WjzUQaMYWbWijciLhO+qEi5q9JZ96aPWTkHWny6pPchBF9WnJJtwSCAz1fm+NwGPyyP48VO7NZvuMgq3Ydwt/Pwo1nt+Lmc1trqQyRRkjhphYKNyKuV2F3sGz7AeasTuebrVnYK2cuDPS3EhUcQEigHyGB/ua9zZ/QQD+CA/0IrdpXeR8dGkhiVDDNo4KJjwwi0L9unZYNw+D3g0Us35HNip3ZrNx58Lidqm3+Vkb0SWLseW18uhO6iK9RuKmFwo2Ie2XmlfDB2j3MWZ1G+qHiBp/HYjE7LleFneZRwSQ6b0GE2wJYm3aI5TsOsmJHNvv+0Fk6NNCPlDZNOadtU/q3iyHtUBEvL93JT5VLZfhZLVzeI5HbBralY/yp1YwmIsdSuKmFwo2IZzgc5kKsBaUVFJVVUFhmp7isgsJSO0XldopKj9pXZqewtIKDBWXszSlmb05xtZmu6yLQz8qZLaPo3y6G/u2a0r1FFAF/GK5uGAYrdx7k5aU7+X5HtnP/kM6x3D6oHb1aNXHJtYuI6ync1ELhRuTUZxgGBwvL2Hu4mH2VYWdfTgl7c4rYl1PCvpxiDheVcUZiJOe0a0r/tjH0SY6uV/+ejXtyeGXpThb8nOHsFJ3SOprbB7VlYIdmXln8VUSOT+GmFgo3Ir7B4TBcMvpp54ECXl22k4/W76Xcbv532CUhgkt7JNCrZRO6t4jySqdoEalO4aYWCjciUpP9ucW88d0u3l+VRlGZ3bnf32rhjMQIzmrVhF6tmtC7VTTxkUFeLKnI6UnhphYKNyJSm8OFZXy8YS9rfj/Mmt2HyMwrPeaY5lHBZthpGUWvVtF0Sgg/pn+PiLiWwk0tFG5EpK4Mw2BvTjFrdx9m3e7DrNl9mC3783D84X/NAD8LSU1CaNk0hFbRIbRsGkqr6BBaNQ0hKTqEoAA1a4mcLIWbWijciMjJKCyt4Kf0HNZWhp11aYfJL6l9Rfv4iCBaNTXDTsvoEAL8rJRWOCitsFNa7jiyXeGofFy5XeGgvHKxWGfvIovFuW2xmPurOj9bgOBAPyKCA4gMDiCq8v7oW9VzkSEBhAb6U1RWQX6JeSsoLSevpIKCkqp95RSUHnk+JjyQwZ3iOKtllBZOFY9TuKmFwo2IuJLDYdbupB0qYvfBInYfKiTtoLmddqiIgtLag09j1CQkgPM7xXJB5zgGdGhGqM3f20WS04DCTS0UbkTEUwzD4FBhGbsPFTkDT/rhIhyGgc3fD5u/FVuAlSB/P2wB1iP7/K3YAsztAD/LUefDOWzdqDy/cdRzYFBUZie3uJyconJyi8vJKzbv/3grPWoeoQA/C+FBAYQH+RNm8yc8yN98XLkdFuRPmC2A7Zn5fLM1i9ziI7M/B/pZOaddU4Z0jmNI5zh1tha3UbiphcKNiAiUlNspKrMTEuhXrz5BFXYHa3YfZtEvmSzcksnug0XVnu/eIpIhneM4v2Ms4UH+lNsdlNsNyu0OKhwOyioMKhyO6vvtBhYLRAQHEBHkT0RQAOFBAUQE+xMc4Kc5hwRQuKmVwo2IiGsYhsGOrAIWbslk0S+ZrE/PwdXfKP5WC+FB/pXBJ6CyVsmfkEB/Zy1XUIBftdquqnvnfn8rgf7Va8aqHgc6t634Wy2NLkiV2x1sy8hn455ccovLCQvyJ8JZAxdwVE2cua8x95VSuKmFwo2IiHtk5ZewZGsWC3/J4sddBzEMs8nL389KoJ8Vfz8LAX5miAisDBMBflYC/KxUOBzOjst5JeXkl1Q4F2D1FIvFXFg10M9KYGUQCnQ+Pna7KkBVdd6OCgkgMiSw2uOo4EDCg/xdMuGk3WGGyY17cti0N5ef9uSyZX9evZYqCQ7wc4ad+MggEiODneu3Va3blhgVfEqO8Gt04eall17imWeeISMjgx49evDiiy/St2/f4x4/f/58Jk+ezO+//0779u156qmnuPjii+v0Xgo3IiKnPsMw+w8dCTvl5BWb23klFZSWmyPKSsqrRpnZKakcaVZy1IizqufLKkeflVWOTKt6XOGBAGWx4Aw8R9dARVQ2vYUHmc1xZlPckW1/Pwtb9uexcU8uG/fksHlvHsXl9mPOHxHkT/cWUcRG2CgoqXCOcDPvzaBYWs+12qJDA82gUxl+YsIC8bNasVrMRWetFsuRbasFP0vlPqsFPytEh9oY2KGZq36EQP2+v73exX3u3LlMnDiRmTNnkpKSwvTp00lNTWXbtm3ExsYec/yKFSu47rrrmDZtGpdeeinvvfcew4cPZ926dXTt2tULVyAiIq5msVgItfkTavN3aydlu8Ps91Na7qDUbg7NL7ObIaisovp26R8el1XYKS53VHbSLiOnyOzInVNcTm5RGbnF5RSW2TEMnM+drJBAP7o2j6R780i6J0XRvXkkrZqGnLA5razCUS3s5BWXk5FXUrl2m3lfdSsss3OosIxDhWVs3pvXoHKe2TLK5eGmPrxec5OSkkKfPn2YMWMGAA6Hg6SkJO68807uv//+Y44fMWIEhYWFfPbZZ859Z599Nj179mTmzJknfD/V3IiIiKeUVRwJP4eLzNFrRze95RWXO2uj/vhcSbmddrFh9GgRRbfmkXRvEUmbZmH4uaCJ63gMwyCvpKJa2NmbU8LhwjIchoHdMHA4DOwGOKq2HQaOysfmtkG72DAevuwMl5at0dTclJWVsXbtWiZNmuTcZ7VaGTJkCCtXrqzxNStXrmTixInV9qWmpvLxxx/XeHxpaSmlpUemT8/La1gKFRERqa9AfyvNwm00C7d5uyh1YrFYnBM+dk5ovBUAXu02nZ2djd1uJy4urtr+uLg4MjIyanxNRkZGvY6fNm0akZGRzltSUpJrCi8iIiKnpMY7JqyOJk2aRG5urvOWnp7u7SKJiIiIG3m1WSomJgY/Pz8yMzOr7c/MzCQ+Pr7G18THx9freJvNhs3WOKoDRURE5OR5teYmMDCQXr16sXjxYuc+h8PB4sWL6devX42v6devX7XjARYuXHjc40VEROT04vWh4BMnTmT06NH07t2bvn37Mn36dAoLCxkzZgwAo0aNonnz5kybNg2Au+66i4EDB/Lcc89xySWXMGfOHNasWcNrr73mzcsQERGRU4TXw82IESM4cOAAU6ZMISMjg549e7JgwQJnp+G0tDSs1iMVTOeccw7vvfceDz30EA888ADt27fn448/1hw3IiIiApwC89x4mua5ERERaXzq8/3t86OlRERE5PSicCMiIiI+ReFGREREfIrCjYiIiPgUhRsRERHxKQo3IiIi4lMUbkRERMSneH0SP0+rmtYnLy/PyyURERGRuqr63q7L9HynXbjJz88HICkpycslERERkfrKz88nMjKy1mNOuxmKHQ4H+/btIzw8HIvF4tJz5+XlkZSURHp6uk/Pfnw6XOfpcI2g6/Q1uk7fcTpcI9TvOg3DID8/n8TExGrLMtXktKu5sVqttGjRwq3vERER4dO/jFVOh+s8Ha4RdJ2+RtfpO06Ha4S6X+eJamyqqEOxiIiI+BSFGxEREfEpCjcuZLPZePjhh7HZbN4uiludDtd5Olwj6Dp9ja7Td5wO1wjuu87TrkOxiIiI+DbV3IiIiIhPUbgRERERn6JwIyIiIj5F4UZERER8isKNi7z00kskJycTFBRESkoKq1at8naRXOqRRx7BYrFUu3Xq1MnbxTpp3377LZdddhmJiYlYLBY+/vjjas8bhsGUKVNISEggODiYIUOG8Ouvv3qnsCfhRNd50003HfP5Dh061DuFbaBp06bRp08fwsPDiY2NZfjw4Wzbtq3aMSUlJYwfP56mTZsSFhbGVVddRWZmppdK3DB1uc5BgwYd83nedtttXipxw7zyyit0797dOblbv379+PLLL53P+8JnCSe+Tl/4LP/oySefxGKxcPfddzv3ufrzVLhxgblz5zJx4kQefvhh1q1bR48ePUhNTSUrK8vbRXOpM844g/379ztv33//vbeLdNIKCwvp0aMHL730Uo3PP/3007zwwgvMnDmTH3/8kdDQUFJTUykpKfFwSU/Oia4TYOjQodU+3/fff9+DJTx5y5YtY/z48fzwww8sXLiQ8vJyLrzwQgoLC53H/O1vf+N///sf8+fPZ9myZezbt48rr7zSi6Wuv7pcJ8DYsWOrfZ5PP/20l0rcMC1atODJJ59k7dq1rFmzhj/96U8MGzaMn3/+GfCNzxJOfJ3Q+D/Lo61evZpXX32V7t27V9vv8s/TkJPWt29fY/z48c7HdrvdSExMNKZNm+bFUrnWww8/bPTo0cPbxXArwPjoo4+cjx0OhxEfH28888wzzn05OTmGzWYz3n//fS+U0DX+eJ2GYRijR482hg0b5pXyuEtWVpYBGMuWLTMMw/zsAgICjPnz5zuP2bJliwEYK1eu9FYxT9ofr9MwDGPgwIHGXXfd5b1CuUmTJk2MN954w2c/yypV12kYvvVZ5ufnG+3btzcWLlxY7brc8Xmq5uYklZWVsXbtWoYMGeLcZ7VaGTJkCCtXrvRiyVzv119/JTExkTZt2jBy5EjS0tK8XSS32rVrFxkZGdU+28jISFJSUnzuswVYunQpsbGxdOzYkdtvv52DBw96u0gnJTc3F4Do6GgA1q5dS3l5ebXPs1OnTrRs2bJRf55/vM4q7777LjExMXTt2pVJkyZRVFTkjeK5hN1uZ86cORQWFtKvXz+f/Sz/eJ1VfOWzHD9+PJdcckm1zw3c82/ztFs409Wys7Ox2+3ExcVV2x8XF8fWrVu9VCrXS0lJYfbs2XTs2JH9+/fz6KOPct5557F582bCw8O9XTy3yMjIAKjxs616zlcMHTqUK6+8ktatW7Nz504eeOABLrroIlauXImfn5+3i1dvDoeDu+++m/79+9O1a1fA/DwDAwOJioqqdmxj/jxruk6A66+/nlatWpGYmMjGjRu577772LZtGx9++KEXS1t/mzZtol+/fpSUlBAWFsZHH31Ely5d2LBhg099lse7TvCdz3LOnDmsW7eO1atXH/OcO/5tKtxInVx00UXO7e7du5OSkkKrVq2YN28et9xyixdLJq5w7bXXOre7detG9+7dadu2LUuXLmXw4MFeLFnDjB8/ns2bN/tEv7DaHO86x40b59zu1q0bCQkJDB48mJ07d9K2bVtPF7PBOnbsyIYNG8jNzeWDDz5g9OjRLFu2zNvFcrnjXWeXLl184rNMT0/nrrvuYuHChQQFBXnkPdUsdZJiYmLw8/M7pld3ZmYm8fHxXiqV+0VFRdGhQwd27Njh7aK4TdXnd7p9tgBt2rQhJiamUX6+EyZM4LPPPmPJkiW0aNHCuT8+Pp6ysjJycnKqHd9YP8/jXWdNUlJSABrd5xkYGEi7du3o1asX06ZNo0ePHvzrX//yuc/yeNdZk8b4Wa5du5asrCzOOuss/P398ff3Z9myZbzwwgv4+/sTFxfn8s9T4eYkBQYG0qtXLxYvXuzc53A4WLx4cbU2U19TUFDAzp07SUhI8HZR3KZ169bEx8dX+2zz8vL48ccfffqzBdizZw8HDx5sVJ+vYRhMmDCBjz76iG+++YbWrVtXe75Xr14EBARU+zy3bdtGWlpao/o8T3SdNdmwYQNAo/o8a+JwOCgtLfWZz/J4qq6zJo3xsxw8eDCbNm1iw4YNzlvv3r0ZOXKkc9vln+fJ93+WOXPmGDabzZg9e7bxyy+/GOPGjTOioqKMjIwMbxfNZf7v//7PWLp0qbFr1y5j+fLlxpAhQ4yYmBgjKyvL20U7Kfn5+cb69euN9evXG4Dx/PPPG+vXrzd2795tGIZhPPnkk0ZUVJTxySefGBs3bjSGDRtmtG7d2iguLvZyyeuntuvMz8837rnnHmPlypXGrl27jEWLFhlnnXWW0b59e6OkpMTbRa+z22+/3YiMjDSWLl1q7N+/33krKipyHnPbbbcZLVu2NL755htjzZo1Rr9+/Yx+/fp5sdT1d6Lr3LFjhzF16lRjzZo1xq5du4xPPvnEaNOmjTFgwAAvl7x+7r//fmPZsmXGrl27jI0bNxr333+/YbFYjK+//towDN/4LA2j9uv0lc+yJn8cBebqz1PhxkVefPFFo2XLlkZgYKDRt29f44cffvB2kVxqxIgRRkJCghEYGGg0b97cGDFihLFjxw5vF+ukLVmyxACOuY0ePdowDHM4+OTJk424uDjDZrMZgwcPNrZt2+bdQjdAbddZVFRkXHjhhUazZs2MgIAAo1WrVsbYsWMbXTiv6foA46233nIeU1xcbNxxxx1GkyZNjJCQEOOKK64w9u/f771CN8CJrjMtLc0YMGCAER0dbdhsNqNdu3bG3//+dyM3N9e7Ba+nm2++2WjVqpURGBhoNGvWzBg8eLAz2BiGb3yWhlH7dfrKZ1mTP4YbV3+eFsMwjIbV+YiIiIicetTnRkRERHyKwo2IiIj4FIUbERER8SkKNyIiIuJTFG5ERETEpyjciIiIiE9RuBERERGfonAjIqc9i8XCxx9/7O1iiIiLKNyIiFfddNNNWCyWY25Dhw71dtFEpJHy93YBRESGDh3KW2+9VW2fzWbzUmlEpLFTzY2IeJ3NZiM+Pr7arUmTJoDZZPTKK69w0UUXERwcTJs2bfjggw+qvX7Tpk386U9/Ijg4mKZNmzJu3DgKCgqqHTNr1izOOOMMbDYbCQkJTJgwodrz2dnZXHHFFYSEhNC+fXs+/fRT9160iLiNwo2InPImT57MVVddxU8//cTIkSO59tpr2bJlCwCFhYWkpqbSpEkTVq9ezfz581m0aFG18PLKK68wfvx4xo0bx6ZNm/j0009p165dtfd49NFHueaaa9i4cSMXX3wxI0eO5NChQx69ThFxkZNe2lNE5CSMHj3a8PPzM0JDQ6vdHn/8ccMwzFWwb7vttmqvSUlJMW6//XbDMAzjtddeM5o0aWIUFBQ4n//8888Nq9XqXNk8MTHRePDBB49bBsB46KGHnI8LCgoMwPjyyy9ddp0i4jnqcyMiXnf++efzyiuvVNsXHR3t3O7Xr1+15/r168eGDRsA2LJlCz169CA0NNT5fP/+/XE4HGzbtg2LxcK+ffsYPHhwrWXo3r27czs0NJSIiAiysrIaekki4kUKNyLidaGhocc0E7lKcHBwnY4LCAio9thiseBwONxRJBFxM/W5EZFT3g8//HDM486dOwPQuXNnfvrpJwoLC53PL1++HKvVSseOHQkPDyc5OZnFixd7tMwi4j2quRERrystLSUjI6PaPn9/f2JiYgCYP38+vXv35txzz+Xdd99l1apVvPnmmwCMHDmShx9+mNGjR/PII49w4MAB7rzzTm688Ubi4uIAeOSRR7jtttuIjY3loosuIj8/n+XLl3PnnXd69kJFxCMUbkTE6xYsWEBCQkK1fR07dmTr1q2AOZJpzpw53HHHHSQkJPD+++/TpUsXAEJCQvjqq6+466676NOnDyEhIVx11VU8//zzznONHj2akpIS/vnPf3LPPfcQExPD1Vdf7bkLFBGPshiGYXi7ECIix2OxWPjoo48YPny4t4siIo2E+tyIiIiIT1G4EREREZ+iPjcickpTy7mI1JdqbkRERMSnKNyIiIiIT1G4EREREZ+icCMiIiI+ReFGREREfIrCjYiIiPgUhRsRERHxKQo3IiIi4lMUbkRERMSn/D8Mag0NeZ0gqAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history_bn3.history['loss'])\n",
        "plt.plot(history_bn3.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "X1Kru6fePPzR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 478ms/step - binary_accuracy: 0.8268 - loss: 0.3663 - val_binary_accuracy: 0.7851 - val_loss: 1.4798\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9430 - loss: 0.1520 - val_binary_accuracy: 0.9276 - val_loss: 0.2723\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9554 - loss: 0.1202 - val_binary_accuracy: 0.9364 - val_loss: 0.1843\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9561 - loss: 0.1145 - val_binary_accuracy: 0.9518 - val_loss: 0.1966\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9671 - loss: 0.1010 - val_binary_accuracy: 0.9518 - val_loss: 0.1860\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9627 - loss: 0.1020 - val_binary_accuracy: 0.9430 - val_loss: 0.1710\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9708 - loss: 0.0842 - val_binary_accuracy: 0.9539 - val_loss: 0.1573\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 0.0841 - val_binary_accuracy: 0.9518 - val_loss: 0.1526\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9722 - loss: 0.0733 - val_binary_accuracy: 0.9539 - val_loss: 0.1473\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9744 - loss: 0.0746 - val_binary_accuracy: 0.9561 - val_loss: 0.1375\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9817 - loss: 0.0659 - val_binary_accuracy: 0.9561 - val_loss: 0.1232\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9781 - loss: 0.0664 - val_binary_accuracy: 0.9561 - val_loss: 0.1356\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9744 - loss: 0.0655 - val_binary_accuracy: 0.9474 - val_loss: 0.1415\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9810 - loss: 0.0597 - val_binary_accuracy: 0.9605 - val_loss: 0.1274\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9795 - loss: 0.0602 - val_binary_accuracy: 0.9627 - val_loss: 0.1038\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9810 - loss: 0.0543 - val_binary_accuracy: 0.9583 - val_loss: 0.1070\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9846 - loss: 0.0497 - val_binary_accuracy: 0.9649 - val_loss: 0.0944\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9839 - loss: 0.0487 - val_binary_accuracy: 0.9671 - val_loss: 0.0935\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9868 - loss: 0.0530 - val_binary_accuracy: 0.9759 - val_loss: 0.0732\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0410 - val_binary_accuracy: 0.9737 - val_loss: 0.0883\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9861 - loss: 0.0327 - val_binary_accuracy: 0.9649 - val_loss: 0.1063\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0302 - val_binary_accuracy: 0.9781 - val_loss: 0.0705\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9963 - loss: 0.0167 - val_binary_accuracy: 0.9846 - val_loss: 0.0693\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9971 - loss: 0.0139 - val_binary_accuracy: 0.9846 - val_loss: 0.0733\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9971 - loss: 0.0117 - val_binary_accuracy: 0.9803 - val_loss: 0.0823\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9942 - loss: 0.0148 - val_binary_accuracy: 0.9868 - val_loss: 0.0788\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9876 - loss: 0.0323 - val_binary_accuracy: 0.9868 - val_loss: 0.0669\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9956 - loss: 0.0115 - val_binary_accuracy: 0.9693 - val_loss: 0.1280\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9963 - loss: 0.0152 - val_binary_accuracy: 0.9846 - val_loss: 0.0847\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9985 - loss: 0.0068 - val_binary_accuracy: 0.9868 - val_loss: 0.0610\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9971 - loss: 0.0115 - val_binary_accuracy: 0.9868 - val_loss: 0.0668\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9949 - loss: 0.0168 - val_binary_accuracy: 0.9737 - val_loss: 0.1172\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9927 - loss: 0.0169 - val_binary_accuracy: 0.9781 - val_loss: 0.1025\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9971 - loss: 0.0102 - val_binary_accuracy: 0.9781 - val_loss: 0.1135\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9985 - loss: 0.0080 - val_binary_accuracy: 0.9825 - val_loss: 0.0990\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9949 - loss: 0.0091 - val_binary_accuracy: 0.9868 - val_loss: 0.0782\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9985 - loss: 0.0057 - val_binary_accuracy: 0.9868 - val_loss: 0.0936\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9993 - loss: 0.0035 - val_binary_accuracy: 0.9825 - val_loss: 0.1001\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9993 - loss: 0.0040 - val_binary_accuracy: 0.9890 - val_loss: 0.0955\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9993 - loss: 0.0033 - val_binary_accuracy: 0.9890 - val_loss: 0.0872\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.826754</td>\n",
              "      <td>0.366343</td>\n",
              "      <td>0.785088</td>\n",
              "      <td>1.479760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.151981</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.272334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.955409</td>\n",
              "      <td>0.120235</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.184289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.114516</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.196606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.101042</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.185973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.102033</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.171028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>0.084217</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.157345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.084092</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.152600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.073252</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.147268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.074607</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.137510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.065868</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.123163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.066445</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.135630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.065547</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.141464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>0.059675</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.127369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.060225</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.103790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>0.054279</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.106957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.049733</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.094428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.048655</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.093503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.053022</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.073175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.040983</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.088298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.032719</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.106254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.030169</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.070524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.996345</td>\n",
              "      <td>0.016653</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.069327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.013896</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.073342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.011727</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.082268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.014847</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.078801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.032283</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.066946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.011538</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.127952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.996345</td>\n",
              "      <td>0.015181</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.084718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.006778</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.060978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.011476</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.066809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.994883</td>\n",
              "      <td>0.016842</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.117223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.016861</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.102489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.010237</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.113502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.007997</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.098972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.994883</td>\n",
              "      <td>0.009138</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.078203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.093617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.003527</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.100096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.003978</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.095520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.003322</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.087239</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.826754  0.366343             0.785088  1.479760\n",
              "1          0.942982  0.151981             0.927632  0.272334\n",
              "2          0.955409  0.120235             0.936404  0.184289\n",
              "3          0.956140  0.114516             0.951754  0.196606\n",
              "4          0.967105  0.101042             0.951754  0.185973\n",
              "5          0.962719  0.102033             0.942982  0.171028\n",
              "6          0.970760  0.084217             0.953947  0.157345\n",
              "7          0.969298  0.084092             0.951754  0.152600\n",
              "8          0.972222  0.073252             0.953947  0.147268\n",
              "9          0.974415  0.074607             0.956140  0.137510\n",
              "10         0.981725  0.065868             0.956140  0.123163\n",
              "11         0.978070  0.066445             0.956140  0.135630\n",
              "12         0.974415  0.065547             0.947368  0.141464\n",
              "13         0.980994  0.059675             0.960526  0.127369\n",
              "14         0.979532  0.060225             0.962719  0.103790\n",
              "15         0.980994  0.054279             0.958333  0.106957\n",
              "16         0.984649  0.049733             0.964912  0.094428\n",
              "17         0.983918  0.048655             0.967105  0.093503\n",
              "18         0.986842  0.053022             0.975877  0.073175\n",
              "19         0.988304  0.040983             0.973684  0.088298\n",
              "20         0.986111  0.032719             0.964912  0.106254\n",
              "21         0.988304  0.030169             0.978070  0.070524\n",
              "22         0.996345  0.016653             0.984649  0.069327\n",
              "23         0.997076  0.013896             0.984649  0.073342\n",
              "24         0.997076  0.011727             0.980263  0.082268\n",
              "25         0.994152  0.014847             0.986842  0.078801\n",
              "26         0.987573  0.032283             0.986842  0.066946\n",
              "27         0.995614  0.011538             0.969298  0.127952\n",
              "28         0.996345  0.015181             0.984649  0.084718\n",
              "29         0.998538  0.006778             0.986842  0.060978\n",
              "30         0.997076  0.011476             0.986842  0.066809\n",
              "31         0.994883  0.016842             0.973684  0.117223\n",
              "32         0.992690  0.016861             0.978070  0.102489\n",
              "33         0.997076  0.010237             0.978070  0.113502\n",
              "34         0.998538  0.007997             0.982456  0.098972\n",
              "35         0.994883  0.009138             0.986842  0.078203\n",
              "36         0.998538  0.005651             0.986842  0.093617\n",
              "37         0.999269  0.003527             0.982456  0.100096\n",
              "38         0.999269  0.003978             0.989035  0.095520\n",
              "39         0.999269  0.003322             0.989035  0.087239"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bn4 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.007, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer='zeros', # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer='ones'), # default is gamma_initializer='ones'),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.007, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer='zeros', # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer='ones'), # default is gamma_initializer='ones'),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_bn4.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bn4 = model_bn4.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bn4 = pd.DataFrame(history_bn4.history)\n",
        "df_bn4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history_bn4.history['loss'])\n",
        "plt.plot(history_bn4.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 453ms/step - binary_accuracy: 0.7573 - loss: 0.4951 - val_binary_accuracy: 0.7522 - val_loss: 1.0057\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9364 - loss: 0.1942 - val_binary_accuracy: 0.8048 - val_loss: 0.4612\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9459 - loss: 0.1464 - val_binary_accuracy: 0.9474 - val_loss: 0.1551\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9539 - loss: 0.1091 - val_binary_accuracy: 0.9496 - val_loss: 0.1186\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9642 - loss: 0.0983 - val_binary_accuracy: 0.9583 - val_loss: 0.1019\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 0.0808 - val_binary_accuracy: 0.9605 - val_loss: 0.0861\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9751 - loss: 0.0670 - val_binary_accuracy: 0.9627 - val_loss: 0.0811\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9751 - loss: 0.0657 - val_binary_accuracy: 0.9759 - val_loss: 0.0685\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9788 - loss: 0.0618 - val_binary_accuracy: 0.9781 - val_loss: 0.0676\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9846 - loss: 0.0537 - val_binary_accuracy: 0.9737 - val_loss: 0.0711\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9781 - loss: 0.0558 - val_binary_accuracy: 0.9715 - val_loss: 0.0609\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9861 - loss: 0.0445 - val_binary_accuracy: 0.9715 - val_loss: 0.0763\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9825 - loss: 0.0476 - val_binary_accuracy: 0.9759 - val_loss: 0.0502\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0387 - val_binary_accuracy: 0.9846 - val_loss: 0.0450\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9883 - loss: 0.0364 - val_binary_accuracy: 0.9759 - val_loss: 0.0597\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9912 - loss: 0.0262 - val_binary_accuracy: 0.9759 - val_loss: 0.0692\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9905 - loss: 0.0247 - val_binary_accuracy: 0.9825 - val_loss: 0.0518\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9927 - loss: 0.0230 - val_binary_accuracy: 0.9846 - val_loss: 0.0347\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9927 - loss: 0.0220 - val_binary_accuracy: 0.9868 - val_loss: 0.0406\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9963 - loss: 0.0213 - val_binary_accuracy: 0.9868 - val_loss: 0.0425\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9949 - loss: 0.0188 - val_binary_accuracy: 0.9846 - val_loss: 0.0426\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9963 - loss: 0.0114 - val_binary_accuracy: 0.9890 - val_loss: 0.0434\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9978 - loss: 0.0110 - val_binary_accuracy: 0.9912 - val_loss: 0.0281\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0082 - val_binary_accuracy: 0.9890 - val_loss: 0.0301\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9978 - loss: 0.0088 - val_binary_accuracy: 0.9912 - val_loss: 0.0385\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0077 - val_binary_accuracy: 0.9912 - val_loss: 0.0325\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9985 - loss: 0.0077 - val_binary_accuracy: 0.9890 - val_loss: 0.0295\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 1.0000 - loss: 0.0076 - val_binary_accuracy: 0.9868 - val_loss: 0.0266\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9971 - loss: 0.0091 - val_binary_accuracy: 0.9912 - val_loss: 0.0289\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9971 - loss: 0.0123 - val_binary_accuracy: 0.9890 - val_loss: 0.0298\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0050 - val_binary_accuracy: 0.9868 - val_loss: 0.0350\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0053 - val_binary_accuracy: 0.9890 - val_loss: 0.0318\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0040 - val_binary_accuracy: 0.9890 - val_loss: 0.0354\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0036 - val_binary_accuracy: 0.9890 - val_loss: 0.0331\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0021 - val_binary_accuracy: 0.9890 - val_loss: 0.0328\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0029 - val_binary_accuracy: 0.9890 - val_loss: 0.0324\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0016 - val_binary_accuracy: 0.9868 - val_loss: 0.0318\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0019 - val_binary_accuracy: 0.9890 - val_loss: 0.0259\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0015 - val_binary_accuracy: 0.9912 - val_loss: 0.0257\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9993 - loss: 0.0031 - val_binary_accuracy: 0.9912 - val_loss: 0.0361\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.757310</td>\n",
              "      <td>0.495060</td>\n",
              "      <td>0.752193</td>\n",
              "      <td>1.005691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.194178</td>\n",
              "      <td>0.804825</td>\n",
              "      <td>0.461198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.945906</td>\n",
              "      <td>0.146395</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.155145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.109138</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.118573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>0.098318</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.101912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.080802</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.086087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.067035</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.081095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.065688</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.068457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>0.061780</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.067572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.053745</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.071134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.055833</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.060929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.044485</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.076268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.047556</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.050205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.038656</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.044990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.036411</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.059719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.026240</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.069173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.024678</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.051815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.022983</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.034654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.021999</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.040633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.996345</td>\n",
              "      <td>0.021335</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.042457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.994883</td>\n",
              "      <td>0.018816</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.042554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.996345</td>\n",
              "      <td>0.011367</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.043393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.011050</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.028063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.008195</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.030136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.008848</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.038518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.007691</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.032461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.007669</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.029463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007576</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.026645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.009114</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.028867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.012330</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.029837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.005034</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.035040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.005312</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.031846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.004004</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.035373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003593</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.033098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002086</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.032768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.002882</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.032441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001586</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.031843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001929</td>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.025898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001547</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.025662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.003118</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.036088</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.757310  0.495060             0.752193  1.005691\n",
              "1          0.936404  0.194178             0.804825  0.461198\n",
              "2          0.945906  0.146395             0.947368  0.155145\n",
              "3          0.953947  0.109138             0.949561  0.118573\n",
              "4          0.964181  0.098318             0.958333  0.101912\n",
              "5          0.969298  0.080802             0.960526  0.086087\n",
              "6          0.975146  0.067035             0.962719  0.081095\n",
              "7          0.975146  0.065688             0.975877  0.068457\n",
              "8          0.978801  0.061780             0.978070  0.067572\n",
              "9          0.984649  0.053745             0.973684  0.071134\n",
              "10         0.978070  0.055833             0.971491  0.060929\n",
              "11         0.986111  0.044485             0.971491  0.076268\n",
              "12         0.982456  0.047556             0.975877  0.050205\n",
              "13         0.988304  0.038656             0.984649  0.044990\n",
              "14         0.988304  0.036411             0.975877  0.059719\n",
              "15         0.991228  0.026240             0.975877  0.069173\n",
              "16         0.990497  0.024678             0.982456  0.051815\n",
              "17         0.992690  0.022983             0.984649  0.034654\n",
              "18         0.992690  0.021999             0.986842  0.040633\n",
              "19         0.996345  0.021335             0.986842  0.042457\n",
              "20         0.994883  0.018816             0.984649  0.042554\n",
              "21         0.996345  0.011367             0.989035  0.043393\n",
              "22         0.997807  0.011050             0.991228  0.028063\n",
              "23         0.999269  0.008195             0.989035  0.030136\n",
              "24         0.997807  0.008848             0.991228  0.038518\n",
              "25         0.999269  0.007691             0.991228  0.032461\n",
              "26         0.998538  0.007669             0.989035  0.029463\n",
              "27         1.000000  0.007576             0.986842  0.026645\n",
              "28         0.997076  0.009114             0.991228  0.028867\n",
              "29         0.997076  0.012330             0.989035  0.029837\n",
              "30         1.000000  0.005034             0.986842  0.035040\n",
              "31         0.999269  0.005312             0.989035  0.031846\n",
              "32         1.000000  0.004004             0.989035  0.035373\n",
              "33         1.000000  0.003593             0.989035  0.033098\n",
              "34         1.000000  0.002086             0.989035  0.032768\n",
              "35         0.999269  0.002882             0.989035  0.032441\n",
              "36         1.000000  0.001586             0.986842  0.031843\n",
              "37         1.000000  0.001929             0.989035  0.025898\n",
              "38         1.000000  0.001547             0.991228  0.025662\n",
              "39         0.999269  0.003118             0.991228  0.036088"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bn5 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.006, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.7)), # default is gamma_initializer='ones'),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.006, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.7)), # default is gamma_initializer='ones'),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_bn5.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bn5 = model_bn5.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bn5 = pd.DataFrame(history_bn5.history)\n",
        "df_bn5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history_bn5.history['loss'])\n",
        "plt.plot(history_bn5.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.991\n"
          ]
        }
      ],
      "source": [
        "get_test_accuracy(model_bn5, test_set_x.T, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combination of batch normalization and drop out 0.2 and 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "-ptYfiP9PTEj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 503ms/step - binary_accuracy: 0.7449 - loss: 0.5088 - val_binary_accuracy: 0.5439 - val_loss: 3.9797\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9240 - loss: 0.2160 - val_binary_accuracy: 0.7478 - val_loss: 0.8486\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9481 - loss: 0.1554 - val_binary_accuracy: 0.9342 - val_loss: 0.2110\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9627 - loss: 0.1181 - val_binary_accuracy: 0.9518 - val_loss: 0.1521\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9671 - loss: 0.1068 - val_binary_accuracy: 0.9518 - val_loss: 0.1201\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9686 - loss: 0.0928 - val_binary_accuracy: 0.9627 - val_loss: 0.1118\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9751 - loss: 0.0797 - val_binary_accuracy: 0.9627 - val_loss: 0.1223\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9825 - loss: 0.0674 - val_binary_accuracy: 0.9671 - val_loss: 0.0994\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9861 - loss: 0.0546 - val_binary_accuracy: 0.9671 - val_loss: 0.0940\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.0517 - val_binary_accuracy: 0.9693 - val_loss: 0.0856\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9854 - loss: 0.0508 - val_binary_accuracy: 0.9693 - val_loss: 0.0910\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9868 - loss: 0.0435 - val_binary_accuracy: 0.9693 - val_loss: 0.0785\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9868 - loss: 0.0494 - val_binary_accuracy: 0.9671 - val_loss: 0.0804\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9868 - loss: 0.0499 - val_binary_accuracy: 0.9737 - val_loss: 0.0755\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0462 - val_binary_accuracy: 0.9715 - val_loss: 0.0684\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9905 - loss: 0.0310 - val_binary_accuracy: 0.9737 - val_loss: 0.0679\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9846 - loss: 0.0459 - val_binary_accuracy: 0.9737 - val_loss: 0.0612\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9912 - loss: 0.0292 - val_binary_accuracy: 0.9759 - val_loss: 0.0704\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9861 - loss: 0.0428 - val_binary_accuracy: 0.9671 - val_loss: 0.0712\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9905 - loss: 0.0347 - val_binary_accuracy: 0.9781 - val_loss: 0.0608\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9920 - loss: 0.0271 - val_binary_accuracy: 0.9759 - val_loss: 0.0488\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9927 - loss: 0.0291 - val_binary_accuracy: 0.9803 - val_loss: 0.0521\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9920 - loss: 0.0240 - val_binary_accuracy: 0.9781 - val_loss: 0.0623\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9927 - loss: 0.0263 - val_binary_accuracy: 0.9781 - val_loss: 0.0596\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9920 - loss: 0.0235 - val_binary_accuracy: 0.9737 - val_loss: 0.0659\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9905 - loss: 0.0231 - val_binary_accuracy: 0.9759 - val_loss: 0.0574\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9905 - loss: 0.0334 - val_binary_accuracy: 0.9825 - val_loss: 0.0505\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9942 - loss: 0.0201 - val_binary_accuracy: 0.9781 - val_loss: 0.0668\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9934 - loss: 0.0243 - val_binary_accuracy: 0.9737 - val_loss: 0.0531\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9890 - loss: 0.0288 - val_binary_accuracy: 0.9781 - val_loss: 0.0441\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9920 - loss: 0.0302 - val_binary_accuracy: 0.9715 - val_loss: 0.1124\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9927 - loss: 0.0262 - val_binary_accuracy: 0.9781 - val_loss: 0.0576\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9956 - loss: 0.0192 - val_binary_accuracy: 0.9715 - val_loss: 0.0555\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0315 - val_binary_accuracy: 0.9737 - val_loss: 0.0791\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0352 - val_binary_accuracy: 0.9693 - val_loss: 0.0605\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9934 - loss: 0.0204 - val_binary_accuracy: 0.9846 - val_loss: 0.0470\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9920 - loss: 0.0170 - val_binary_accuracy: 0.9803 - val_loss: 0.0408\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9985 - loss: 0.0084 - val_binary_accuracy: 0.9846 - val_loss: 0.0537\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9920 - loss: 0.0286 - val_binary_accuracy: 0.9803 - val_loss: 0.0465\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9956 - loss: 0.0110 - val_binary_accuracy: 0.9825 - val_loss: 0.0510\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.744883</td>\n",
              "      <td>0.508826</td>\n",
              "      <td>0.543860</td>\n",
              "      <td>3.979748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.923977</td>\n",
              "      <td>0.215982</td>\n",
              "      <td>0.747807</td>\n",
              "      <td>0.848588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.948099</td>\n",
              "      <td>0.155358</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.211011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.118139</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.152139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.106760</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.120075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>0.092808</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.111828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.079739</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.122320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.067368</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.099394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.054602</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.093971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.051682</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.085601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.050750</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.090964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.043513</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.078525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.049430</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.080443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.049933</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.075480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.046152</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.068383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.031010</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.067863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.045869</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.061242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.029246</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.070357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.042800</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.071195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.034742</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.060845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.027070</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.048811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.029135</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.052076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.024031</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.062304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.026337</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.059591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.023469</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.065853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.023141</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.057449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.033356</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.050475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.020126</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.066841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.024290</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.053083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.028809</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.044118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.030162</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.112412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.026204</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.057569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.019160</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.055511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.031459</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.079148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.035245</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.060460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.020413</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.046968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.017039</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.040753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.008368</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.053708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.028636</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.046490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.010979</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.050977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.744883  0.508826             0.543860  3.979748\n",
              "1          0.923977  0.215982             0.747807  0.848588\n",
              "2          0.948099  0.155358             0.934211  0.211011\n",
              "3          0.962719  0.118139             0.951754  0.152139\n",
              "4          0.967105  0.106760             0.951754  0.120075\n",
              "5          0.968567  0.092808             0.962719  0.111828\n",
              "6          0.975146  0.079739             0.962719  0.122320\n",
              "7          0.982456  0.067368             0.967105  0.099394\n",
              "8          0.986111  0.054602             0.967105  0.093971\n",
              "9          0.983918  0.051682             0.969298  0.085601\n",
              "10         0.985380  0.050750             0.969298  0.090964\n",
              "11         0.986842  0.043513             0.969298  0.078525\n",
              "12         0.986842  0.049430             0.967105  0.080443\n",
              "13         0.986842  0.049933             0.973684  0.075480\n",
              "14         0.988304  0.046152             0.971491  0.068383\n",
              "15         0.990497  0.031010             0.973684  0.067863\n",
              "16         0.984649  0.045869             0.973684  0.061242\n",
              "17         0.991228  0.029246             0.975877  0.070357\n",
              "18         0.986111  0.042800             0.967105  0.071195\n",
              "19         0.990497  0.034742             0.978070  0.060845\n",
              "20         0.991959  0.027070             0.975877  0.048811\n",
              "21         0.992690  0.029135             0.980263  0.052076\n",
              "22         0.991959  0.024031             0.978070  0.062304\n",
              "23         0.992690  0.026337             0.978070  0.059591\n",
              "24         0.991959  0.023469             0.973684  0.065853\n",
              "25         0.990497  0.023141             0.975877  0.057449\n",
              "26         0.990497  0.033356             0.982456  0.050475\n",
              "27         0.994152  0.020126             0.978070  0.066841\n",
              "28         0.993421  0.024290             0.973684  0.053083\n",
              "29         0.989035  0.028809             0.978070  0.044118\n",
              "30         0.991959  0.030162             0.971491  0.112412\n",
              "31         0.992690  0.026204             0.978070  0.057569\n",
              "32         0.995614  0.019160             0.971491  0.055511\n",
              "33         0.988304  0.031459             0.973684  0.079148\n",
              "34         0.988304  0.035245             0.969298  0.060460\n",
              "35         0.993421  0.020413             0.984649  0.046968\n",
              "36         0.991959  0.017039             0.980263  0.040753\n",
              "37         0.998538  0.008368             0.984649  0.053708\n",
              "38         0.991959  0.028636             0.980263  0.046490\n",
              "39         0.995614  0.010979             0.982456  0.050977"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bd1 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.006, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.7)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.006, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.7)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_bd1.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bd1 = model_bd1.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bd1 = pd.DataFrame(history_bd1.history)\n",
        "df_bd1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combination of batch normalization and drop out 0.3 and 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "6/6 - 3s - 471ms/step - binary_accuracy: 0.8070 - loss: 0.4281 - val_binary_accuracy: 0.7390 - val_loss: 1.3094\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 34ms/step - binary_accuracy: 0.9262 - loss: 0.1977 - val_binary_accuracy: 0.9057 - val_loss: 0.2969\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9364 - loss: 0.1779 - val_binary_accuracy: 0.9452 - val_loss: 0.2109\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9474 - loss: 0.1429 - val_binary_accuracy: 0.9452 - val_loss: 0.1570\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9525 - loss: 0.1253 - val_binary_accuracy: 0.9496 - val_loss: 0.1422\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9591 - loss: 0.1110 - val_binary_accuracy: 0.9452 - val_loss: 0.1457\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9635 - loss: 0.1029 - val_binary_accuracy: 0.9496 - val_loss: 0.1471\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9627 - loss: 0.1027 - val_binary_accuracy: 0.9518 - val_loss: 0.1312\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9715 - loss: 0.1004 - val_binary_accuracy: 0.9627 - val_loss: 0.1280\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9730 - loss: 0.0855 - val_binary_accuracy: 0.9627 - val_loss: 0.1247\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9737 - loss: 0.0908 - val_binary_accuracy: 0.9605 - val_loss: 0.1165\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9730 - loss: 0.0876 - val_binary_accuracy: 0.9605 - val_loss: 0.1104\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9700 - loss: 0.0760 - val_binary_accuracy: 0.9649 - val_loss: 0.1028\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9730 - loss: 0.0839 - val_binary_accuracy: 0.9649 - val_loss: 0.0966\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9766 - loss: 0.0691 - val_binary_accuracy: 0.9671 - val_loss: 0.0887\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9773 - loss: 0.0699 - val_binary_accuracy: 0.9627 - val_loss: 0.0867\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9810 - loss: 0.0701 - val_binary_accuracy: 0.9693 - val_loss: 0.0852\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9766 - loss: 0.0685 - val_binary_accuracy: 0.9693 - val_loss: 0.0783\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9803 - loss: 0.0618 - val_binary_accuracy: 0.9693 - val_loss: 0.0740\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9825 - loss: 0.0539 - val_binary_accuracy: 0.9737 - val_loss: 0.0673\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 0.9854 - loss: 0.0446 - val_binary_accuracy: 0.9715 - val_loss: 0.0703\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9876 - loss: 0.0379 - val_binary_accuracy: 0.9693 - val_loss: 0.0700\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9846 - loss: 0.0479 - val_binary_accuracy: 0.9781 - val_loss: 0.0596\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9846 - loss: 0.0493 - val_binary_accuracy: 0.9781 - val_loss: 0.0572\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0386 - val_binary_accuracy: 0.9759 - val_loss: 0.0566\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9898 - loss: 0.0279 - val_binary_accuracy: 0.9781 - val_loss: 0.0669\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9861 - loss: 0.0369 - val_binary_accuracy: 0.9781 - val_loss: 0.0595\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9905 - loss: 0.0214 - val_binary_accuracy: 0.9781 - val_loss: 0.0521\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9934 - loss: 0.0215 - val_binary_accuracy: 0.9803 - val_loss: 0.0616\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9883 - loss: 0.0289 - val_binary_accuracy: 0.9825 - val_loss: 0.0687\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9912 - loss: 0.0257 - val_binary_accuracy: 0.9803 - val_loss: 0.0578\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9898 - loss: 0.0311 - val_binary_accuracy: 0.9803 - val_loss: 0.0679\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9876 - loss: 0.0330 - val_binary_accuracy: 0.9825 - val_loss: 0.0666\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0322 - val_binary_accuracy: 0.9759 - val_loss: 0.0672\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9912 - loss: 0.0312 - val_binary_accuracy: 0.9583 - val_loss: 0.1427\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9912 - loss: 0.0275 - val_binary_accuracy: 0.9759 - val_loss: 0.0626\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9868 - loss: 0.0366 - val_binary_accuracy: 0.9803 - val_loss: 0.0556\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9912 - loss: 0.0271 - val_binary_accuracy: 0.9803 - val_loss: 0.0562\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9942 - loss: 0.0193 - val_binary_accuracy: 0.9781 - val_loss: 0.0576\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9898 - loss: 0.0256 - val_binary_accuracy: 0.9803 - val_loss: 0.0536\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.807018</td>\n",
              "      <td>0.428117</td>\n",
              "      <td>0.739035</td>\n",
              "      <td>1.309376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.926170</td>\n",
              "      <td>0.197718</td>\n",
              "      <td>0.905702</td>\n",
              "      <td>0.296944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.177880</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.210896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.142863</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.156982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.952485</td>\n",
              "      <td>0.125261</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.142192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.959064</td>\n",
              "      <td>0.111033</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.145738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>0.102918</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.147124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.102653</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.131160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.100392</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.127984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.085471</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.124653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.090845</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.116451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.087602</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.110400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.075983</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.102756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.083909</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.096558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.069149</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.088730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>0.069925</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.086724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>0.070053</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.085217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.068493</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.078325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.061764</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.074042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.053929</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.067335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.044607</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.070256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.037876</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.069992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.047880</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.059632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.049311</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.057187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.038636</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.056594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.027891</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.066896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.036880</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.059521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.021433</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.052148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.021464</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.061573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.028939</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.068683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.025671</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.057754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.031118</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.067868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.033005</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.066557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.032192</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.067240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.031166</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.142667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.027469</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.062563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.036610</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.055581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.027126</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.056210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.019272</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.057631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.025640</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.053587</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.807018  0.428117             0.739035  1.309376\n",
              "1          0.926170  0.197718             0.905702  0.296944\n",
              "2          0.936404  0.177880             0.945175  0.210896\n",
              "3          0.947368  0.142863             0.945175  0.156982\n",
              "4          0.952485  0.125261             0.949561  0.142192\n",
              "5          0.959064  0.111033             0.945175  0.145738\n",
              "6          0.963450  0.102918             0.949561  0.147124\n",
              "7          0.962719  0.102653             0.951754  0.131160\n",
              "8          0.971491  0.100392             0.962719  0.127984\n",
              "9          0.972953  0.085471             0.962719  0.124653\n",
              "10         0.973684  0.090845             0.960526  0.116451\n",
              "11         0.972953  0.087602             0.960526  0.110400\n",
              "12         0.970029  0.075983             0.964912  0.102756\n",
              "13         0.972953  0.083909             0.964912  0.096558\n",
              "14         0.976608  0.069149             0.967105  0.088730\n",
              "15         0.977339  0.069925             0.962719  0.086724\n",
              "16         0.980994  0.070053             0.969298  0.085217\n",
              "17         0.976608  0.068493             0.969298  0.078325\n",
              "18         0.980263  0.061764             0.969298  0.074042\n",
              "19         0.982456  0.053929             0.973684  0.067335\n",
              "20         0.985380  0.044607             0.971491  0.070256\n",
              "21         0.987573  0.037876             0.969298  0.069992\n",
              "22         0.984649  0.047880             0.978070  0.059632\n",
              "23         0.984649  0.049311             0.978070  0.057187\n",
              "24         0.988304  0.038636             0.975877  0.056594\n",
              "25         0.989766  0.027891             0.978070  0.066896\n",
              "26         0.986111  0.036880             0.978070  0.059521\n",
              "27         0.990497  0.021433             0.978070  0.052148\n",
              "28         0.993421  0.021464             0.980263  0.061573\n",
              "29         0.988304  0.028939             0.982456  0.068683\n",
              "30         0.991228  0.025671             0.980263  0.057754\n",
              "31         0.989766  0.031118             0.980263  0.067868\n",
              "32         0.987573  0.033005             0.982456  0.066557\n",
              "33         0.988304  0.032192             0.975877  0.067240\n",
              "34         0.991228  0.031166             0.958333  0.142667\n",
              "35         0.991228  0.027469             0.975877  0.062563\n",
              "36         0.986842  0.036610             0.980263  0.055581\n",
              "37         0.991228  0.027126             0.980263  0.056210\n",
              "38         0.994152  0.019272             0.978070  0.057631\n",
              "39         0.989766  0.025640             0.980263  0.053587"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bd2 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.9)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.3),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.9)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_bd2.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bd2 = model_bd2.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bd2 = pd.DataFrame(history_bd2.history)\n",
        "df_bd2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combination of batch normalization and drop out 0.2 and 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 547ms/step - binary_accuracy: 0.7156 - loss: 0.5768 - val_binary_accuracy: 0.6404 - val_loss: 1.7494\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9306 - loss: 0.1988 - val_binary_accuracy: 0.9320 - val_loss: 0.2689\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9401 - loss: 0.1761 - val_binary_accuracy: 0.9320 - val_loss: 0.2026\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9320 - loss: 0.1676 - val_binary_accuracy: 0.9408 - val_loss: 0.2012\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9510 - loss: 0.1462 - val_binary_accuracy: 0.9408 - val_loss: 0.1761\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9488 - loss: 0.1388 - val_binary_accuracy: 0.9408 - val_loss: 0.1684\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 27ms/step - binary_accuracy: 0.9627 - loss: 0.1094 - val_binary_accuracy: 0.9496 - val_loss: 0.1363\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9664 - loss: 0.1038 - val_binary_accuracy: 0.9561 - val_loss: 0.1311\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 33ms/step - binary_accuracy: 0.9693 - loss: 0.1016 - val_binary_accuracy: 0.9649 - val_loss: 0.1310\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9664 - loss: 0.0913 - val_binary_accuracy: 0.9474 - val_loss: 0.1425\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9664 - loss: 0.0936 - val_binary_accuracy: 0.9605 - val_loss: 0.1148\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9730 - loss: 0.0885 - val_binary_accuracy: 0.9627 - val_loss: 0.1029\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9766 - loss: 0.0829 - val_binary_accuracy: 0.9627 - val_loss: 0.1038\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9671 - loss: 0.0942 - val_binary_accuracy: 0.9649 - val_loss: 0.1003\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9737 - loss: 0.0811 - val_binary_accuracy: 0.9605 - val_loss: 0.1027\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9730 - loss: 0.0775 - val_binary_accuracy: 0.9605 - val_loss: 0.1051\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9708 - loss: 0.0930 - val_binary_accuracy: 0.9649 - val_loss: 0.0969\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9795 - loss: 0.0689 - val_binary_accuracy: 0.9649 - val_loss: 0.0981\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9766 - loss: 0.0705 - val_binary_accuracy: 0.9649 - val_loss: 0.0991\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9766 - loss: 0.0765 - val_binary_accuracy: 0.9649 - val_loss: 0.0986\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9795 - loss: 0.0791 - val_binary_accuracy: 0.9649 - val_loss: 0.0979\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9781 - loss: 0.0716 - val_binary_accuracy: 0.9649 - val_loss: 0.0982\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9766 - loss: 0.0724 - val_binary_accuracy: 0.9649 - val_loss: 0.0887\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9803 - loss: 0.0612 - val_binary_accuracy: 0.9649 - val_loss: 0.0809\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9744 - loss: 0.0746 - val_binary_accuracy: 0.9649 - val_loss: 0.0825\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9744 - loss: 0.0735 - val_binary_accuracy: 0.9671 - val_loss: 0.0841\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9817 - loss: 0.0613 - val_binary_accuracy: 0.9649 - val_loss: 0.0876\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9781 - loss: 0.0670 - val_binary_accuracy: 0.9671 - val_loss: 0.0765\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9817 - loss: 0.0614 - val_binary_accuracy: 0.9649 - val_loss: 0.0755\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9781 - loss: 0.0630 - val_binary_accuracy: 0.9649 - val_loss: 0.0770\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9846 - loss: 0.0544 - val_binary_accuracy: 0.9649 - val_loss: 0.0779\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9788 - loss: 0.0597 - val_binary_accuracy: 0.9671 - val_loss: 0.0766\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9846 - loss: 0.0547 - val_binary_accuracy: 0.9671 - val_loss: 0.0782\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9773 - loss: 0.0614 - val_binary_accuracy: 0.9671 - val_loss: 0.0778\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9803 - loss: 0.0568 - val_binary_accuracy: 0.9671 - val_loss: 0.0768\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9832 - loss: 0.0678 - val_binary_accuracy: 0.9649 - val_loss: 0.0727\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9788 - loss: 0.0575 - val_binary_accuracy: 0.9715 - val_loss: 0.0695\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9825 - loss: 0.0516 - val_binary_accuracy: 0.9715 - val_loss: 0.0669\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9854 - loss: 0.0456 - val_binary_accuracy: 0.9715 - val_loss: 0.0661\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9817 - loss: 0.0567 - val_binary_accuracy: 0.9715 - val_loss: 0.0633\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.715643</td>\n",
              "      <td>0.576780</td>\n",
              "      <td>0.640351</td>\n",
              "      <td>1.749407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.930556</td>\n",
              "      <td>0.198788</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.268916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.940058</td>\n",
              "      <td>0.176102</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.202559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.167640</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.201215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.951023</td>\n",
              "      <td>0.146244</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.176126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.948830</td>\n",
              "      <td>0.138766</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.168406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.109397</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.136323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>0.103809</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.131091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.101582</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.130983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>0.091350</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.142498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>0.093600</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.114768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.088495</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.102905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.082920</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.103779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.094175</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.100301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.081145</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.102717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.077499</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.105064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>0.092989</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.096942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.068925</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.098061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.070480</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.099147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.076506</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.098648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.079066</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.097921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.071633</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.098218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.072363</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.088702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.061160</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.080868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.074646</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.082465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.073466</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.084130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.061278</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.087587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.066965</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.076501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.061433</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.075458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.063016</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.077000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.054416</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.077899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>0.059693</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.076587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.054666</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.078247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>0.061368</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.077754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.056829</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.076775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.983187</td>\n",
              "      <td>0.067777</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.072730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>0.057459</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.069480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.051577</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.066895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.045576</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.066056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.056702</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.063279</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.715643  0.576780             0.640351  1.749407\n",
              "1          0.930556  0.198788             0.932018  0.268916\n",
              "2          0.940058  0.176102             0.932018  0.202559\n",
              "3          0.932018  0.167640             0.940789  0.201215\n",
              "4          0.951023  0.146244             0.940789  0.176126\n",
              "5          0.948830  0.138766             0.940789  0.168406\n",
              "6          0.962719  0.109397             0.949561  0.136323\n",
              "7          0.966374  0.103809             0.956140  0.131091\n",
              "8          0.969298  0.101582             0.964912  0.130983\n",
              "9          0.966374  0.091350             0.947368  0.142498\n",
              "10         0.966374  0.093600             0.960526  0.114768\n",
              "11         0.972953  0.088495             0.962719  0.102905\n",
              "12         0.976608  0.082920             0.962719  0.103779\n",
              "13         0.967105  0.094175             0.964912  0.100301\n",
              "14         0.973684  0.081145             0.960526  0.102717\n",
              "15         0.972953  0.077499             0.960526  0.105064\n",
              "16         0.970760  0.092989             0.964912  0.096942\n",
              "17         0.979532  0.068925             0.964912  0.098061\n",
              "18         0.976608  0.070480             0.964912  0.099147\n",
              "19         0.976608  0.076506             0.964912  0.098648\n",
              "20         0.979532  0.079066             0.964912  0.097921\n",
              "21         0.978070  0.071633             0.964912  0.098218\n",
              "22         0.976608  0.072363             0.964912  0.088702\n",
              "23         0.980263  0.061160             0.964912  0.080868\n",
              "24         0.974415  0.074646             0.964912  0.082465\n",
              "25         0.974415  0.073466             0.967105  0.084130\n",
              "26         0.981725  0.061278             0.964912  0.087587\n",
              "27         0.978070  0.066965             0.967105  0.076501\n",
              "28         0.981725  0.061433             0.964912  0.075458\n",
              "29         0.978070  0.063016             0.964912  0.077000\n",
              "30         0.984649  0.054416             0.964912  0.077899\n",
              "31         0.978801  0.059693             0.967105  0.076587\n",
              "32         0.984649  0.054666             0.967105  0.078247\n",
              "33         0.977339  0.061368             0.967105  0.077754\n",
              "34         0.980263  0.056829             0.967105  0.076775\n",
              "35         0.983187  0.067777             0.964912  0.072730\n",
              "36         0.978801  0.057459             0.971491  0.069480\n",
              "37         0.982456  0.051577             0.971491  0.066895\n",
              "38         0.985380  0.045576             0.971491  0.066056\n",
              "39         0.981725  0.056702             0.971491  0.063279"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bd3 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "                    BatchNormalization(momentum=0.95, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.8)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.8)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.1),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_bd3.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bd3 = model_bd3.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bd3 = pd.DataFrame(history_bd3.history)\n",
        "df_bd3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.989\n"
          ]
        }
      ],
      "source": [
        "get_test_accuracy(model_bd3, test_set_x.T, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combination of batch normalization and drop out 0.1 and 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 487ms/step - binary_accuracy: 0.7961 - loss: 0.4103 - val_binary_accuracy: 0.5439 - val_loss: 10.9879\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9152 - loss: 0.2216 - val_binary_accuracy: 0.5702 - val_loss: 7.6136\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9225 - loss: 0.1993 - val_binary_accuracy: 0.5789 - val_loss: 5.1122\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9291 - loss: 0.1846 - val_binary_accuracy: 0.6250 - val_loss: 2.9411\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9408 - loss: 0.1642 - val_binary_accuracy: 0.6645 - val_loss: 2.0422\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9459 - loss: 0.1584 - val_binary_accuracy: 0.7171 - val_loss: 1.4391\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9539 - loss: 0.1362 - val_binary_accuracy: 0.7346 - val_loss: 1.2346\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9561 - loss: 0.1383 - val_binary_accuracy: 0.7697 - val_loss: 0.8982\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 34ms/step - binary_accuracy: 0.9620 - loss: 0.1206 - val_binary_accuracy: 0.8575 - val_loss: 0.5429\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9547 - loss: 0.1294 - val_binary_accuracy: 0.9079 - val_loss: 0.3382\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9613 - loss: 0.1227 - val_binary_accuracy: 0.9079 - val_loss: 0.3504\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9649 - loss: 0.1174 - val_binary_accuracy: 0.9276 - val_loss: 0.2956\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9627 - loss: 0.1122 - val_binary_accuracy: 0.9101 - val_loss: 0.3274\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 27ms/step - binary_accuracy: 0.9649 - loss: 0.1136 - val_binary_accuracy: 0.9408 - val_loss: 0.2604\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9686 - loss: 0.1002 - val_binary_accuracy: 0.9386 - val_loss: 0.2679\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9664 - loss: 0.0919 - val_binary_accuracy: 0.9276 - val_loss: 0.2753\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9737 - loss: 0.0839 - val_binary_accuracy: 0.9211 - val_loss: 0.2850\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9693 - loss: 0.0984 - val_binary_accuracy: 0.9189 - val_loss: 0.3018\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9751 - loss: 0.0848 - val_binary_accuracy: 0.9518 - val_loss: 0.1866\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9715 - loss: 0.0831 - val_binary_accuracy: 0.9539 - val_loss: 0.1492\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9751 - loss: 0.0804 - val_binary_accuracy: 0.9539 - val_loss: 0.1421\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9744 - loss: 0.0824 - val_binary_accuracy: 0.9539 - val_loss: 0.1620\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.0690 - val_binary_accuracy: 0.9539 - val_loss: 0.1536\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9773 - loss: 0.0709 - val_binary_accuracy: 0.9539 - val_loss: 0.1189\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9766 - loss: 0.0688 - val_binary_accuracy: 0.9539 - val_loss: 0.1329\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9773 - loss: 0.0689 - val_binary_accuracy: 0.9627 - val_loss: 0.1083\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9795 - loss: 0.0646 - val_binary_accuracy: 0.9561 - val_loss: 0.1109\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9832 - loss: 0.0463 - val_binary_accuracy: 0.9627 - val_loss: 0.1028\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9825 - loss: 0.0509 - val_binary_accuracy: 0.9583 - val_loss: 0.1087\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.0533 - val_binary_accuracy: 0.9671 - val_loss: 0.0774\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9839 - loss: 0.0600 - val_binary_accuracy: 0.9671 - val_loss: 0.0908\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0406 - val_binary_accuracy: 0.9671 - val_loss: 0.1001\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9846 - loss: 0.0461 - val_binary_accuracy: 0.9671 - val_loss: 0.0889\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9876 - loss: 0.0364 - val_binary_accuracy: 0.9715 - val_loss: 0.0726\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9861 - loss: 0.0481 - val_binary_accuracy: 0.9671 - val_loss: 0.0747\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9854 - loss: 0.0433 - val_binary_accuracy: 0.9671 - val_loss: 0.0775\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9854 - loss: 0.0400 - val_binary_accuracy: 0.9693 - val_loss: 0.0640\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9927 - loss: 0.0291 - val_binary_accuracy: 0.9737 - val_loss: 0.0536\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9868 - loss: 0.0353 - val_binary_accuracy: 0.9759 - val_loss: 0.0600\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9890 - loss: 0.0330 - val_binary_accuracy: 0.9715 - val_loss: 0.0622\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.796053</td>\n",
              "      <td>0.410282</td>\n",
              "      <td>0.543860</td>\n",
              "      <td>10.987932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.915205</td>\n",
              "      <td>0.221627</td>\n",
              "      <td>0.570175</td>\n",
              "      <td>7.613576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.922515</td>\n",
              "      <td>0.199318</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>5.112213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.929094</td>\n",
              "      <td>0.184564</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>2.941089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.164193</td>\n",
              "      <td>0.664474</td>\n",
              "      <td>2.042243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.945906</td>\n",
              "      <td>0.158363</td>\n",
              "      <td>0.717105</td>\n",
              "      <td>1.439105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.136153</td>\n",
              "      <td>0.734649</td>\n",
              "      <td>1.234576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.138262</td>\n",
              "      <td>0.769737</td>\n",
              "      <td>0.898152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>0.120561</td>\n",
              "      <td>0.857456</td>\n",
              "      <td>0.542856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.954678</td>\n",
              "      <td>0.129386</td>\n",
              "      <td>0.907895</td>\n",
              "      <td>0.338225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.961257</td>\n",
              "      <td>0.122730</td>\n",
              "      <td>0.907895</td>\n",
              "      <td>0.350351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.117355</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.295572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.112199</td>\n",
              "      <td>0.910088</td>\n",
              "      <td>0.327410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.113551</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.260358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>0.100213</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.267902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>0.091903</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.275313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.083889</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.285015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.098370</td>\n",
              "      <td>0.918860</td>\n",
              "      <td>0.301814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.084849</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.186603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.083086</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.149192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.080381</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.142050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.082373</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.161971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.068956</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.153559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>0.070920</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.118929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.068820</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.132881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>0.068943</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.108341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.064607</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.110887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.983187</td>\n",
              "      <td>0.046313</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.102766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.050918</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.108722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.053300</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.077420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.060021</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.090772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.040596</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.100056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.046053</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.088914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.036393</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.072585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.048085</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.074690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.043269</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.077532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.040011</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.063980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.029144</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.053625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.035293</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.059961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.032961</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.062152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy   val_loss\n",
              "0          0.796053  0.410282             0.543860  10.987932\n",
              "1          0.915205  0.221627             0.570175   7.613576\n",
              "2          0.922515  0.199318             0.578947   5.112213\n",
              "3          0.929094  0.184564             0.625000   2.941089\n",
              "4          0.940789  0.164193             0.664474   2.042243\n",
              "5          0.945906  0.158363             0.717105   1.439105\n",
              "6          0.953947  0.136153             0.734649   1.234576\n",
              "7          0.956140  0.138262             0.769737   0.898152\n",
              "8          0.961988  0.120561             0.857456   0.542856\n",
              "9          0.954678  0.129386             0.907895   0.338225\n",
              "10         0.961257  0.122730             0.907895   0.350351\n",
              "11         0.964912  0.117355             0.927632   0.295572\n",
              "12         0.962719  0.112199             0.910088   0.327410\n",
              "13         0.964912  0.113551             0.940789   0.260358\n",
              "14         0.968567  0.100213             0.938596   0.267902\n",
              "15         0.966374  0.091903             0.927632   0.275313\n",
              "16         0.973684  0.083889             0.921053   0.285015\n",
              "17         0.969298  0.098370             0.918860   0.301814\n",
              "18         0.975146  0.084849             0.951754   0.186603\n",
              "19         0.971491  0.083086             0.953947   0.149192\n",
              "20         0.975146  0.080381             0.953947   0.142050\n",
              "21         0.974415  0.082373             0.953947   0.161971\n",
              "22         0.983918  0.068956             0.953947   0.153559\n",
              "23         0.977339  0.070920             0.953947   0.118929\n",
              "24         0.976608  0.068820             0.953947   0.132881\n",
              "25         0.977339  0.068943             0.962719   0.108341\n",
              "26         0.979532  0.064607             0.956140   0.110887\n",
              "27         0.983187  0.046313             0.962719   0.102766\n",
              "28         0.982456  0.050918             0.958333   0.108722\n",
              "29         0.983918  0.053300             0.967105   0.077420\n",
              "30         0.983918  0.060021             0.967105   0.090772\n",
              "31         0.988304  0.040596             0.967105   0.100056\n",
              "32         0.984649  0.046053             0.967105   0.088914\n",
              "33         0.987573  0.036393             0.971491   0.072585\n",
              "34         0.986111  0.048085             0.967105   0.074690\n",
              "35         0.985380  0.043269             0.967105   0.077532\n",
              "36         0.985380  0.040011             0.969298   0.063980\n",
              "37         0.992690  0.029144             0.973684   0.053625\n",
              "38         0.986842  0.035293             0.975877   0.059961\n",
              "39         0.989035  0.032961             0.971491   0.062152"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bd4 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "                    BatchNormalization(momentum=0.99, # default is 0.99\n",
        "                                    epsilon=0.007, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.8)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.1),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.99, # default is 0.99\n",
        "                                    epsilon=0.007, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.7)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.1),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_bd4.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bd4 = model_bd4.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bd4 = pd.DataFrame(history_bd4.history)\n",
        "df_bd4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combination of batch normalization and drop out 0.1 and 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 488ms/step - binary_accuracy: 0.7873 - loss: 0.4386 - val_binary_accuracy: 0.6557 - val_loss: 2.0613\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9203 - loss: 0.1886 - val_binary_accuracy: 0.8136 - val_loss: 0.6514\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9357 - loss: 0.1491 - val_binary_accuracy: 0.9123 - val_loss: 0.2321\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9539 - loss: 0.1284 - val_binary_accuracy: 0.9474 - val_loss: 0.1525\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9613 - loss: 0.1152 - val_binary_accuracy: 0.9518 - val_loss: 0.1392\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9635 - loss: 0.1009 - val_binary_accuracy: 0.9496 - val_loss: 0.1370\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9620 - loss: 0.0934 - val_binary_accuracy: 0.9474 - val_loss: 0.1391\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9751 - loss: 0.0852 - val_binary_accuracy: 0.9518 - val_loss: 0.1558\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9737 - loss: 0.0740 - val_binary_accuracy: 0.9518 - val_loss: 0.1601\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9766 - loss: 0.0816 - val_binary_accuracy: 0.9627 - val_loss: 0.1467\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9803 - loss: 0.0627 - val_binary_accuracy: 0.9627 - val_loss: 0.1439\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9751 - loss: 0.0704 - val_binary_accuracy: 0.9583 - val_loss: 0.1430\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9751 - loss: 0.0591 - val_binary_accuracy: 0.9627 - val_loss: 0.1402\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9795 - loss: 0.0628 - val_binary_accuracy: 0.9649 - val_loss: 0.1328\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9795 - loss: 0.0589 - val_binary_accuracy: 0.9649 - val_loss: 0.1261\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.0505 - val_binary_accuracy: 0.9671 - val_loss: 0.1241\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.0454 - val_binary_accuracy: 0.9671 - val_loss: 0.1204\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9803 - loss: 0.0566 - val_binary_accuracy: 0.9671 - val_loss: 0.1185\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9846 - loss: 0.0481 - val_binary_accuracy: 0.9671 - val_loss: 0.1083\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9868 - loss: 0.0480 - val_binary_accuracy: 0.9671 - val_loss: 0.1136\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9846 - loss: 0.0483 - val_binary_accuracy: 0.9671 - val_loss: 0.1220\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.0447 - val_binary_accuracy: 0.9671 - val_loss: 0.1104\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9861 - loss: 0.0459 - val_binary_accuracy: 0.9693 - val_loss: 0.0957\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9854 - loss: 0.0514 - val_binary_accuracy: 0.9693 - val_loss: 0.1014\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9861 - loss: 0.0488 - val_binary_accuracy: 0.9693 - val_loss: 0.1143\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.0464 - val_binary_accuracy: 0.9671 - val_loss: 0.1121\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9839 - loss: 0.0491 - val_binary_accuracy: 0.9649 - val_loss: 0.1068\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9854 - loss: 0.0456 - val_binary_accuracy: 0.9671 - val_loss: 0.1057\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9825 - loss: 0.0416 - val_binary_accuracy: 0.9693 - val_loss: 0.0994\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9883 - loss: 0.0388 - val_binary_accuracy: 0.9693 - val_loss: 0.1005\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9876 - loss: 0.0361 - val_binary_accuracy: 0.9715 - val_loss: 0.0610\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9920 - loss: 0.0336 - val_binary_accuracy: 0.9693 - val_loss: 0.0653\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9854 - loss: 0.0507 - val_binary_accuracy: 0.9693 - val_loss: 0.0754\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9876 - loss: 0.0364 - val_binary_accuracy: 0.9715 - val_loss: 0.0885\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9890 - loss: 0.0337 - val_binary_accuracy: 0.9715 - val_loss: 0.0891\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9868 - loss: 0.0390 - val_binary_accuracy: 0.9693 - val_loss: 0.0891\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9868 - loss: 0.0399 - val_binary_accuracy: 0.9693 - val_loss: 0.0979\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9876 - loss: 0.0330 - val_binary_accuracy: 0.9715 - val_loss: 0.0938\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9876 - loss: 0.0327 - val_binary_accuracy: 0.9715 - val_loss: 0.0977\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9883 - loss: 0.0348 - val_binary_accuracy: 0.9715 - val_loss: 0.0861\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.787281</td>\n",
              "      <td>0.438590</td>\n",
              "      <td>0.655702</td>\n",
              "      <td>2.061286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.920322</td>\n",
              "      <td>0.188598</td>\n",
              "      <td>0.813596</td>\n",
              "      <td>0.651409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.935673</td>\n",
              "      <td>0.149111</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.232140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.128411</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.152538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.961257</td>\n",
              "      <td>0.115211</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.139195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>0.100852</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.137005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>0.093377</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.139150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.085224</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.155840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.073976</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.160125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.081610</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.146721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.062702</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.143901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.070373</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.142976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.059145</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.140161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.062771</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.132756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>0.058908</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.126142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.050487</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.124134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.045402</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.120418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.056573</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.118530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.048119</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.108283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.047956</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.113550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.048256</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.122022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.044683</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.110359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.045916</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.095725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.051404</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.101414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.048822</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.114262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.046412</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.112091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.049118</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.106778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.045599</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.105748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.041565</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.099407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.038831</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.100521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.036117</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.060966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.033570</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.065294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.050701</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.075357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.036392</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.088469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.033712</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.089078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.038966</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.089079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.039910</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.097929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.032987</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.093768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.032694</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.097703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.034752</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.086112</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.787281  0.438590             0.655702  2.061286\n",
              "1          0.920322  0.188598             0.813596  0.651409\n",
              "2          0.935673  0.149111             0.912281  0.232140\n",
              "3          0.953947  0.128411             0.947368  0.152538\n",
              "4          0.961257  0.115211             0.951754  0.139195\n",
              "5          0.963450  0.100852             0.949561  0.137005\n",
              "6          0.961988  0.093377             0.947368  0.139150\n",
              "7          0.975146  0.085224             0.951754  0.155840\n",
              "8          0.973684  0.073976             0.951754  0.160125\n",
              "9          0.976608  0.081610             0.962719  0.146721\n",
              "10         0.980263  0.062702             0.962719  0.143901\n",
              "11         0.975146  0.070373             0.958333  0.142976\n",
              "12         0.975146  0.059145             0.962719  0.140161\n",
              "13         0.979532  0.062771             0.964912  0.132756\n",
              "14         0.979532  0.058908             0.964912  0.126142\n",
              "15         0.983918  0.050487             0.967105  0.124134\n",
              "16         0.983918  0.045402             0.967105  0.120418\n",
              "17         0.980263  0.056573             0.967105  0.118530\n",
              "18         0.984649  0.048119             0.967105  0.108283\n",
              "19         0.986842  0.047956             0.967105  0.113550\n",
              "20         0.984649  0.048256             0.967105  0.122022\n",
              "21         0.983918  0.044683             0.967105  0.110359\n",
              "22         0.986111  0.045916             0.969298  0.095725\n",
              "23         0.985380  0.051404             0.969298  0.101414\n",
              "24         0.986111  0.048822             0.969298  0.114262\n",
              "25         0.983918  0.046412             0.967105  0.112091\n",
              "26         0.983918  0.049118             0.964912  0.106778\n",
              "27         0.985380  0.045599             0.967105  0.105748\n",
              "28         0.982456  0.041565             0.969298  0.099407\n",
              "29         0.988304  0.038831             0.969298  0.100521\n",
              "30         0.987573  0.036117             0.971491  0.060966\n",
              "31         0.991959  0.033570             0.969298  0.065294\n",
              "32         0.985380  0.050701             0.969298  0.075357\n",
              "33         0.987573  0.036392             0.971491  0.088469\n",
              "34         0.989035  0.033712             0.971491  0.089078\n",
              "35         0.986842  0.038966             0.969298  0.089079\n",
              "36         0.986842  0.039910             0.969298  0.097929\n",
              "37         0.987573  0.032987             0.971491  0.093768\n",
              "38         0.987573  0.032694             0.971491  0.097703\n",
              "39         0.988304  0.034752             0.971491  0.086112"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bd5 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.7)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.1),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.7)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.01),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_bd5.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bd5 = model_bd5.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bd5 = pd.DataFrame(history_bd5.history)\n",
        "df_bd5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mixture of batch normalization, dropout, and L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 517ms/step - binary_accuracy: 0.7646 - loss: 8.5578 - val_binary_accuracy: 0.6206 - val_loss: 18.4723\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9028 - loss: 17.1355 - val_binary_accuracy: 0.8399 - val_loss: 17.5993\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9254 - loss: 16.8142 - val_binary_accuracy: 0.9254 - val_loss: 15.5864\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9364 - loss: 14.7545 - val_binary_accuracy: 0.9408 - val_loss: 13.3373\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9401 - loss: 12.5499 - val_binary_accuracy: 0.9408 - val_loss: 11.3031\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9488 - loss: 10.5930 - val_binary_accuracy: 0.9518 - val_loss: 9.5427\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9532 - loss: 8.9917 - val_binary_accuracy: 0.9232 - val_loss: 8.2311\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9598 - loss: 7.6704 - val_binary_accuracy: 0.9496 - val_loss: 7.0008\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9613 - loss: 6.6084 - val_binary_accuracy: 0.9101 - val_loss: 6.1538\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9766 - loss: 5.7058 - val_binary_accuracy: 0.9452 - val_loss: 5.3056\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9700 - loss: 4.9898 - val_binary_accuracy: 0.9430 - val_loss: 4.6697\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9715 - loss: 4.4010 - val_binary_accuracy: 0.9430 - val_loss: 4.1731\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9700 - loss: 3.9191 - val_binary_accuracy: 0.9605 - val_loss: 3.6754\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9613 - loss: 3.5257 - val_binary_accuracy: 0.9605 - val_loss: 3.3174\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9591 - loss: 3.1846 - val_binary_accuracy: 0.9605 - val_loss: 2.9965\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9708 - loss: 2.8477 - val_binary_accuracy: 0.9561 - val_loss: 2.7115\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9759 - loss: 2.5550 - val_binary_accuracy: 0.9583 - val_loss: 2.4300\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9803 - loss: 2.2996 - val_binary_accuracy: 0.9583 - val_loss: 2.2332\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9715 - loss: 2.0939 - val_binary_accuracy: 0.9518 - val_loss: 2.0355\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 1.9261 - val_binary_accuracy: 0.9079 - val_loss: 2.0432\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9627 - loss: 1.7911 - val_binary_accuracy: 0.9474 - val_loss: 1.7377\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9744 - loss: 1.6379 - val_binary_accuracy: 0.9605 - val_loss: 1.6036\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9788 - loss: 1.4953 - val_binary_accuracy: 0.9605 - val_loss: 1.4728\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9766 - loss: 1.3686 - val_binary_accuracy: 0.9627 - val_loss: 1.3388\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9737 - loss: 1.2852 - val_binary_accuracy: 0.9539 - val_loss: 1.2832\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9759 - loss: 1.1901 - val_binary_accuracy: 0.9452 - val_loss: 1.2082\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9759 - loss: 1.0986 - val_binary_accuracy: 0.9561 - val_loss: 1.1088\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9730 - loss: 1.0298 - val_binary_accuracy: 0.8991 - val_loss: 1.1357\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9773 - loss: 0.9575 - val_binary_accuracy: 0.9715 - val_loss: 0.9546\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9795 - loss: 0.8926 - val_binary_accuracy: 0.9605 - val_loss: 0.9079\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9773 - loss: 0.8289 - val_binary_accuracy: 0.9298 - val_loss: 0.9391\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9715 - loss: 0.7857 - val_binary_accuracy: 0.9496 - val_loss: 0.8510\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9635 - loss: 0.7647 - val_binary_accuracy: 0.8750 - val_loss: 1.0402\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9678 - loss: 0.7398 - val_binary_accuracy: 0.8596 - val_loss: 1.0816\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9737 - loss: 0.6932 - val_binary_accuracy: 0.8772 - val_loss: 0.9964\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9686 - loss: 0.6521 - val_binary_accuracy: 0.9452 - val_loss: 0.6791\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9715 - loss: 0.5976 - val_binary_accuracy: 0.9561 - val_loss: 0.6700\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9678 - loss: 0.5893 - val_binary_accuracy: 0.9583 - val_loss: 0.6142\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9708 - loss: 0.5502 - val_binary_accuracy: 0.9364 - val_loss: 0.6435\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9730 - loss: 0.5343 - val_binary_accuracy: 0.9605 - val_loss: 0.5435\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.744883</td>\n",
              "      <td>0.508826</td>\n",
              "      <td>0.543860</td>\n",
              "      <td>3.979748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.923977</td>\n",
              "      <td>0.215982</td>\n",
              "      <td>0.747807</td>\n",
              "      <td>0.848588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.948099</td>\n",
              "      <td>0.155358</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.211011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.118139</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.152139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.106760</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.120075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>0.092808</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.111828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.079739</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.122320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.067368</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.099394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.054602</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.093971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.051682</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.085601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.050750</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.090964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.043513</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.078525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.049430</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.080443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.049933</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.075480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.046152</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.068383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.031010</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.067863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.045869</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.061242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.029246</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.070357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.042800</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.071195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.034742</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.060845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.027070</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.048811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.029135</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.052076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.024031</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.062304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.026337</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.059591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.023469</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.065853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.023141</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.057449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.033356</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.050475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.020126</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.066841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.024290</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.053083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.028809</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.044118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.030162</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.112412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.026204</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.057569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.019160</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.055511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.031459</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.079148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.035245</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.060460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.020413</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.046968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.017039</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.040753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.008368</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.053708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.028636</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.046490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.995614</td>\n",
              "      <td>0.010979</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.050977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.744883  0.508826             0.543860  3.979748\n",
              "1          0.923977  0.215982             0.747807  0.848588\n",
              "2          0.948099  0.155358             0.934211  0.211011\n",
              "3          0.962719  0.118139             0.951754  0.152139\n",
              "4          0.967105  0.106760             0.951754  0.120075\n",
              "5          0.968567  0.092808             0.962719  0.111828\n",
              "6          0.975146  0.079739             0.962719  0.122320\n",
              "7          0.982456  0.067368             0.967105  0.099394\n",
              "8          0.986111  0.054602             0.967105  0.093971\n",
              "9          0.983918  0.051682             0.969298  0.085601\n",
              "10         0.985380  0.050750             0.969298  0.090964\n",
              "11         0.986842  0.043513             0.969298  0.078525\n",
              "12         0.986842  0.049430             0.967105  0.080443\n",
              "13         0.986842  0.049933             0.973684  0.075480\n",
              "14         0.988304  0.046152             0.971491  0.068383\n",
              "15         0.990497  0.031010             0.973684  0.067863\n",
              "16         0.984649  0.045869             0.973684  0.061242\n",
              "17         0.991228  0.029246             0.975877  0.070357\n",
              "18         0.986111  0.042800             0.967105  0.071195\n",
              "19         0.990497  0.034742             0.978070  0.060845\n",
              "20         0.991959  0.027070             0.975877  0.048811\n",
              "21         0.992690  0.029135             0.980263  0.052076\n",
              "22         0.991959  0.024031             0.978070  0.062304\n",
              "23         0.992690  0.026337             0.978070  0.059591\n",
              "24         0.991959  0.023469             0.973684  0.065853\n",
              "25         0.990497  0.023141             0.975877  0.057449\n",
              "26         0.990497  0.033356             0.982456  0.050475\n",
              "27         0.994152  0.020126             0.978070  0.066841\n",
              "28         0.993421  0.024290             0.973684  0.053083\n",
              "29         0.989035  0.028809             0.978070  0.044118\n",
              "30         0.991959  0.030162             0.971491  0.112412\n",
              "31         0.992690  0.026204             0.978070  0.057569\n",
              "32         0.995614  0.019160             0.971491  0.055511\n",
              "33         0.988304  0.031459             0.973684  0.079148\n",
              "34         0.988304  0.035245             0.969298  0.060460\n",
              "35         0.993421  0.020413             0.984649  0.046968\n",
              "36         0.991959  0.017039             0.980263  0.040753\n",
              "37         0.998538  0.008368             0.984649  0.053708\n",
              "38         0.991959  0.028636             0.980263  0.046490\n",
              "39         0.995614  0.010979             0.982456  0.050977"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bdl1 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,), kernel_regularizer=tf.keras.regularizers.l2(0.002)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.9)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.3),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.9)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation = 'sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.002))\n",
        "])\n",
        "\n",
        "model_bdl1.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bdl1 = model_bdl1.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bdl1 = pd.DataFrame(history_bd1.history)\n",
        "df_bdl1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mixture of batch normalization, dropout, and L2 0.001 penalty rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 520ms/step - binary_accuracy: 0.7982 - loss: 4.6964 - val_binary_accuracy: 0.5592 - val_loss: 14.9867\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 33ms/step - binary_accuracy: 0.9152 - loss: 9.8721 - val_binary_accuracy: 0.7675 - val_loss: 11.2579\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9349 - loss: 10.3013 - val_binary_accuracy: 0.9211 - val_loss: 10.0025\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9466 - loss: 9.5379 - val_binary_accuracy: 0.9496 - val_loss: 8.9263\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9598 - loss: 8.4982 - val_binary_accuracy: 0.9627 - val_loss: 7.8747\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9620 - loss: 7.5125 - val_binary_accuracy: 0.9561 - val_loss: 6.9581\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9642 - loss: 6.6247 - val_binary_accuracy: 0.9539 - val_loss: 6.1764\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 5.8572 - val_binary_accuracy: 0.9518 - val_loss: 5.4727\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9766 - loss: 5.2071 - val_binary_accuracy: 0.9583 - val_loss: 4.8912\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9730 - loss: 4.6636 - val_binary_accuracy: 0.9342 - val_loss: 4.4753\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9773 - loss: 4.1942 - val_binary_accuracy: 0.9452 - val_loss: 4.0221\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9678 - loss: 3.8139 - val_binary_accuracy: 0.9627 - val_loss: 3.6027\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9766 - loss: 3.4429 - val_binary_accuracy: 0.9693 - val_loss: 3.2843\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9839 - loss: 3.1441 - val_binary_accuracy: 0.9693 - val_loss: 3.0175\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9766 - loss: 2.8816 - val_binary_accuracy: 0.9561 - val_loss: 2.7764\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9832 - loss: 2.6305 - val_binary_accuracy: 0.9605 - val_loss: 2.5466\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9766 - loss: 2.4406 - val_binary_accuracy: 0.6009 - val_loss: 3.5203\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9693 - loss: 2.3034 - val_binary_accuracy: 0.8706 - val_loss: 2.4485\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9686 - loss: 2.1332 - val_binary_accuracy: 0.9430 - val_loss: 2.1532\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9788 - loss: 1.9751 - val_binary_accuracy: 0.7961 - val_loss: 2.2241\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9854 - loss: 1.8251 - val_binary_accuracy: 0.9671 - val_loss: 1.8205\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9730 - loss: 1.7172 - val_binary_accuracy: 0.9583 - val_loss: 1.7068\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9788 - loss: 1.5926 - val_binary_accuracy: 0.9539 - val_loss: 1.6028\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9781 - loss: 1.4926 - val_binary_accuracy: 0.9430 - val_loss: 1.5309\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9759 - loss: 1.3957 - val_binary_accuracy: 0.8640 - val_loss: 1.6184\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9795 - loss: 1.3095 - val_binary_accuracy: 0.9561 - val_loss: 1.3264\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9773 - loss: 1.2340 - val_binary_accuracy: 0.9671 - val_loss: 1.2528\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9700 - loss: 1.1811 - val_binary_accuracy: 0.9627 - val_loss: 1.1764\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9854 - loss: 1.0923 - val_binary_accuracy: 0.9539 - val_loss: 1.1470\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9766 - loss: 1.0430 - val_binary_accuracy: 0.8969 - val_loss: 1.2075\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9803 - loss: 0.9699 - val_binary_accuracy: 0.5417 - val_loss: 2.1946\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9737 - loss: 0.9395 - val_binary_accuracy: 0.4956 - val_loss: 2.3064\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9759 - loss: 0.8954 - val_binary_accuracy: 0.7193 - val_loss: 1.7476\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9788 - loss: 0.8637 - val_binary_accuracy: 0.7917 - val_loss: 1.5178\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9803 - loss: 0.8127 - val_binary_accuracy: 0.9298 - val_loss: 0.9846\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9781 - loss: 0.7833 - val_binary_accuracy: 0.6118 - val_loss: 2.3135\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9773 - loss: 0.7403 - val_binary_accuracy: 0.9496 - val_loss: 0.8853\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9795 - loss: 0.6986 - val_binary_accuracy: 0.7193 - val_loss: 1.3352\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9730 - loss: 0.6781 - val_binary_accuracy: 0.8158 - val_loss: 1.0525\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9795 - loss: 0.6440 - val_binary_accuracy: 0.5724 - val_loss: 2.4538\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.807018</td>\n",
              "      <td>0.428117</td>\n",
              "      <td>0.739035</td>\n",
              "      <td>1.309376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.926170</td>\n",
              "      <td>0.197718</td>\n",
              "      <td>0.905702</td>\n",
              "      <td>0.296944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.936404</td>\n",
              "      <td>0.177880</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.210896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.142863</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.156982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.952485</td>\n",
              "      <td>0.125261</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.142192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.959064</td>\n",
              "      <td>0.111033</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.145738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>0.102918</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.147124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.102653</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.131160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.100392</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.127984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.085471</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.124653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.090845</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.116451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.087602</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.110400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.075983</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.102756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.083909</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.096558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.069149</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.088730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>0.069925</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.086724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>0.070053</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.085217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.068493</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.078325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.061764</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.074042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.053929</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.067335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.044607</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.070256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.037876</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.069992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.047880</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.059632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.049311</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.057187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.038636</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.056594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.027891</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.066896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.036880</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.059521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.990497</td>\n",
              "      <td>0.021433</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.052148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.021464</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.061573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.028939</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.068683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.025671</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.057754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.031118</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.067868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.987573</td>\n",
              "      <td>0.033005</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.066557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.988304</td>\n",
              "      <td>0.032192</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.067240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.031166</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.142667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.027469</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.062563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.036610</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.055581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.027126</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.056210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.019272</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.057631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.989766</td>\n",
              "      <td>0.025640</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.053587</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.807018  0.428117             0.739035  1.309376\n",
              "1          0.926170  0.197718             0.905702  0.296944\n",
              "2          0.936404  0.177880             0.945175  0.210896\n",
              "3          0.947368  0.142863             0.945175  0.156982\n",
              "4          0.952485  0.125261             0.949561  0.142192\n",
              "5          0.959064  0.111033             0.945175  0.145738\n",
              "6          0.963450  0.102918             0.949561  0.147124\n",
              "7          0.962719  0.102653             0.951754  0.131160\n",
              "8          0.971491  0.100392             0.962719  0.127984\n",
              "9          0.972953  0.085471             0.962719  0.124653\n",
              "10         0.973684  0.090845             0.960526  0.116451\n",
              "11         0.972953  0.087602             0.960526  0.110400\n",
              "12         0.970029  0.075983             0.964912  0.102756\n",
              "13         0.972953  0.083909             0.964912  0.096558\n",
              "14         0.976608  0.069149             0.967105  0.088730\n",
              "15         0.977339  0.069925             0.962719  0.086724\n",
              "16         0.980994  0.070053             0.969298  0.085217\n",
              "17         0.976608  0.068493             0.969298  0.078325\n",
              "18         0.980263  0.061764             0.969298  0.074042\n",
              "19         0.982456  0.053929             0.973684  0.067335\n",
              "20         0.985380  0.044607             0.971491  0.070256\n",
              "21         0.987573  0.037876             0.969298  0.069992\n",
              "22         0.984649  0.047880             0.978070  0.059632\n",
              "23         0.984649  0.049311             0.978070  0.057187\n",
              "24         0.988304  0.038636             0.975877  0.056594\n",
              "25         0.989766  0.027891             0.978070  0.066896\n",
              "26         0.986111  0.036880             0.978070  0.059521\n",
              "27         0.990497  0.021433             0.978070  0.052148\n",
              "28         0.993421  0.021464             0.980263  0.061573\n",
              "29         0.988304  0.028939             0.982456  0.068683\n",
              "30         0.991228  0.025671             0.980263  0.057754\n",
              "31         0.989766  0.031118             0.980263  0.067868\n",
              "32         0.987573  0.033005             0.982456  0.066557\n",
              "33         0.988304  0.032192             0.975877  0.067240\n",
              "34         0.991228  0.031166             0.958333  0.142667\n",
              "35         0.991228  0.027469             0.975877  0.062563\n",
              "36         0.986842  0.036610             0.980263  0.055581\n",
              "37         0.991228  0.027126             0.980263  0.056210\n",
              "38         0.994152  0.019272             0.978070  0.057631\n",
              "39         0.989766  0.025640             0.980263  0.053587"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bdl2 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "                    BatchNormalization(momentum=0.95, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.9)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "                    BatchNormalization(momentum=0.95, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.9)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation = 'sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "])\n",
        "\n",
        "model_bdl2.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bdl2 = model_bdl2.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bdl2 = pd.DataFrame(history_bd2.history)\n",
        "df_bdl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.575\n"
          ]
        }
      ],
      "source": [
        "get_test_accuracy(model_bdl2, test_set_x.T, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mixture of batch normalization, dropout, and L2 0.003 penalty rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 524ms/step - binary_accuracy: 0.8004 - loss: 11.9977 - val_binary_accuracy: 0.7851 - val_loss: 23.0375\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9284 - loss: 23.4032 - val_binary_accuracy: 0.9386 - val_loss: 23.3410\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9364 - loss: 22.3794 - val_binary_accuracy: 0.9452 - val_loss: 20.4650\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9510 - loss: 19.2766 - val_binary_accuracy: 0.9057 - val_loss: 17.3751\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9569 - loss: 16.2064 - val_binary_accuracy: 0.7961 - val_loss: 14.8031\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9561 - loss: 13.6021 - val_binary_accuracy: 0.9123 - val_loss: 12.2742\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9635 - loss: 11.4342 - val_binary_accuracy: 0.9539 - val_loss: 10.3103\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9664 - loss: 9.6931 - val_binary_accuracy: 0.9627 - val_loss: 8.8111\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9627 - loss: 8.3425 - val_binary_accuracy: 0.6184 - val_loss: 8.6726\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9649 - loss: 7.2593 - val_binary_accuracy: 0.9123 - val_loss: 6.8186\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9737 - loss: 6.3482 - val_binary_accuracy: 0.9518 - val_loss: 5.9374\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9649 - loss: 5.6031 - val_binary_accuracy: 0.7829 - val_loss: 5.7082\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9598 - loss: 4.9762 - val_binary_accuracy: 0.9627 - val_loss: 4.6584\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9744 - loss: 4.4304 - val_binary_accuracy: 0.9715 - val_loss: 4.1564\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9715 - loss: 3.9769 - val_binary_accuracy: 0.9671 - val_loss: 3.7293\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 26ms/step - binary_accuracy: 0.9766 - loss: 3.5748 - val_binary_accuracy: 0.8860 - val_loss: 3.5646\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9751 - loss: 3.2531 - val_binary_accuracy: 0.9232 - val_loss: 3.1612\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9642 - loss: 2.9990 - val_binary_accuracy: 0.8509 - val_loss: 3.2037\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9664 - loss: 2.7515 - val_binary_accuracy: 0.9583 - val_loss: 2.6420\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9722 - loss: 2.5069 - val_binary_accuracy: 0.9430 - val_loss: 2.4737\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9766 - loss: 2.2563 - val_binary_accuracy: 0.9671 - val_loss: 2.1426\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9803 - loss: 2.0370 - val_binary_accuracy: 0.8224 - val_loss: 2.2299\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9744 - loss: 1.8757 - val_binary_accuracy: 0.5987 - val_loss: 2.8898\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9613 - loss: 1.7737 - val_binary_accuracy: 0.9583 - val_loss: 1.7342\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9708 - loss: 1.6767 - val_binary_accuracy: 0.9561 - val_loss: 1.6592\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9744 - loss: 1.5445 - val_binary_accuracy: 0.9364 - val_loss: 1.5894\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9751 - loss: 1.4339 - val_binary_accuracy: 0.7588 - val_loss: 1.8924\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9773 - loss: 1.3081 - val_binary_accuracy: 0.8772 - val_loss: 1.4866\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9788 - loss: 1.1936 - val_binary_accuracy: 0.9254 - val_loss: 1.2733\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9795 - loss: 1.1132 - val_binary_accuracy: 0.9671 - val_loss: 1.1120\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9722 - loss: 1.0423 - val_binary_accuracy: 0.9474 - val_loss: 1.1448\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9737 - loss: 1.0156 - val_binary_accuracy: 0.9386 - val_loss: 1.0907\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9759 - loss: 0.9942 - val_binary_accuracy: 0.7237 - val_loss: 2.0136\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9751 - loss: 0.9340 - val_binary_accuracy: 0.9605 - val_loss: 0.9150\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9817 - loss: 0.8437 - val_binary_accuracy: 0.9474 - val_loss: 0.8961\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9781 - loss: 0.7820 - val_binary_accuracy: 0.9408 - val_loss: 0.8371\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9686 - loss: 0.7511 - val_binary_accuracy: 0.9649 - val_loss: 0.7611\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9649 - loss: 0.7405 - val_binary_accuracy: 0.9561 - val_loss: 0.7936\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9678 - loss: 0.7204 - val_binary_accuracy: 0.9518 - val_loss: 0.7585\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9737 - loss: 0.6674 - val_binary_accuracy: 0.9737 - val_loss: 0.6720\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.800439</td>\n",
              "      <td>11.997660</td>\n",
              "      <td>0.785088</td>\n",
              "      <td>23.037493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.928363</td>\n",
              "      <td>23.403175</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>23.341022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.936404</td>\n",
              "      <td>22.379353</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>20.464977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.951023</td>\n",
              "      <td>19.276562</td>\n",
              "      <td>0.905702</td>\n",
              "      <td>17.375126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.956871</td>\n",
              "      <td>16.206404</td>\n",
              "      <td>0.796053</td>\n",
              "      <td>14.803060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.956140</td>\n",
              "      <td>13.602071</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>12.274210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>11.434214</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>10.310267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>9.693075</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>8.811103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.962719</td>\n",
              "      <td>8.342480</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>8.672556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.964912</td>\n",
              "      <td>7.259260</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>6.818630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>6.348196</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>5.937357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.964912</td>\n",
              "      <td>5.603112</td>\n",
              "      <td>0.782895</td>\n",
              "      <td>5.708203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.959795</td>\n",
              "      <td>4.976223</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>4.658385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>4.430394</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>4.156389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.971491</td>\n",
              "      <td>3.976900</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>3.729313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>3.574812</td>\n",
              "      <td>0.885965</td>\n",
              "      <td>3.564560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>3.253132</td>\n",
              "      <td>0.923246</td>\n",
              "      <td>3.161238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.964181</td>\n",
              "      <td>2.998993</td>\n",
              "      <td>0.850877</td>\n",
              "      <td>3.203697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>2.751534</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>2.642002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>2.506890</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>2.473679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>2.256250</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>2.142594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>2.037028</td>\n",
              "      <td>0.822368</td>\n",
              "      <td>2.229911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>1.875723</td>\n",
              "      <td>0.598684</td>\n",
              "      <td>2.889823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.961257</td>\n",
              "      <td>1.773739</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.734175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>1.676700</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>1.659207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>1.544525</td>\n",
              "      <td>0.936404</td>\n",
              "      <td>1.589407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>1.433867</td>\n",
              "      <td>0.758772</td>\n",
              "      <td>1.892417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>1.308137</td>\n",
              "      <td>0.877193</td>\n",
              "      <td>1.486606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>1.193583</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>1.273263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>1.113204</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>1.111959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>1.042311</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.144847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>1.015593</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>1.090680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.994238</td>\n",
              "      <td>0.723684</td>\n",
              "      <td>2.013612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.934010</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.914997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.843720</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.896142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.782022</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.837079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>0.751083</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.761125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.740504</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.793616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.967836</td>\n",
              "      <td>0.720389</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.758458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.667444</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.671977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy       loss  val_binary_accuracy   val_loss\n",
              "0          0.800439  11.997660             0.785088  23.037493\n",
              "1          0.928363  23.403175             0.938596  23.341022\n",
              "2          0.936404  22.379353             0.945175  20.464977\n",
              "3          0.951023  19.276562             0.905702  17.375126\n",
              "4          0.956871  16.206404             0.796053  14.803060\n",
              "5          0.956140  13.602071             0.912281  12.274210\n",
              "6          0.963450  11.434214             0.953947  10.310267\n",
              "7          0.966374   9.693075             0.962719   8.811103\n",
              "8          0.962719   8.342480             0.618421   8.672556\n",
              "9          0.964912   7.259260             0.912281   6.818630\n",
              "10         0.973684   6.348196             0.951754   5.937357\n",
              "11         0.964912   5.603112             0.782895   5.708203\n",
              "12         0.959795   4.976223             0.962719   4.658385\n",
              "13         0.974415   4.430394             0.971491   4.156389\n",
              "14         0.971491   3.976900             0.967105   3.729313\n",
              "15         0.976608   3.574812             0.885965   3.564560\n",
              "16         0.975146   3.253132             0.923246   3.161238\n",
              "17         0.964181   2.998993             0.850877   3.203697\n",
              "18         0.966374   2.751534             0.958333   2.642002\n",
              "19         0.972222   2.506890             0.942982   2.473679\n",
              "20         0.976608   2.256250             0.967105   2.142594\n",
              "21         0.980263   2.037028             0.822368   2.229911\n",
              "22         0.974415   1.875723             0.598684   2.889823\n",
              "23         0.961257   1.773739             0.958333   1.734175\n",
              "24         0.970760   1.676700             0.956140   1.659207\n",
              "25         0.974415   1.544525             0.936404   1.589407\n",
              "26         0.975146   1.433867             0.758772   1.892417\n",
              "27         0.977339   1.308137             0.877193   1.486606\n",
              "28         0.978801   1.193583             0.925439   1.273263\n",
              "29         0.979532   1.113204             0.967105   1.111959\n",
              "30         0.972222   1.042311             0.947368   1.144847\n",
              "31         0.973684   1.015593             0.938596   1.090680\n",
              "32         0.975877   0.994238             0.723684   2.013612\n",
              "33         0.975146   0.934010             0.960526   0.914997\n",
              "34         0.981725   0.843720             0.947368   0.896142\n",
              "35         0.978070   0.782022             0.940789   0.837079\n",
              "36         0.968567   0.751083             0.964912   0.761125\n",
              "37         0.964912   0.740504             0.956140   0.793616\n",
              "38         0.967836   0.720389             0.951754   0.758458\n",
              "39         0.973684   0.667444             0.973684   0.671977"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bdl3 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,), kernel_regularizer=tf.keras.regularizers.l2(0.003)),\n",
        "                    BatchNormalization(momentum=0.85, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.9)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.003)),\n",
        "                    BatchNormalization(momentum=0.85, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.9)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation = 'sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.003))\n",
        "])\n",
        "\n",
        "model_bdl3.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bdl3 = model_bdl3.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bdl3 = pd.DataFrame(history_bdl3.history)\n",
        "df_bdl3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mixture of batch normalization, dropout, and L2 0.004 penalty rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 520ms/step - binary_accuracy: 0.7624 - loss: 15.3460 - val_binary_accuracy: 0.6118 - val_loss: 36.7722\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9006 - loss: 27.4209 - val_binary_accuracy: 0.6053 - val_loss: 31.2299\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9159 - loss: 23.7711 - val_binary_accuracy: 0.6820 - val_loss: 22.6883\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9510 - loss: 18.6368 - val_binary_accuracy: 0.7303 - val_loss: 17.0079\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9613 - loss: 14.3543 - val_binary_accuracy: 0.5943 - val_loss: 14.2300\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9613 - loss: 11.1019 - val_binary_accuracy: 0.7610 - val_loss: 9.8828\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9583 - loss: 8.6939 - val_binary_accuracy: 0.9474 - val_loss: 7.5065\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9678 - loss: 6.8830 - val_binary_accuracy: 0.9561 - val_loss: 6.0120\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9620 - loss: 5.5514 - val_binary_accuracy: 0.9474 - val_loss: 4.9169\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9635 - loss: 4.5401 - val_binary_accuracy: 0.9276 - val_loss: 4.0854\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9693 - loss: 3.7470 - val_binary_accuracy: 0.9605 - val_loss: 3.4194\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9744 - loss: 3.1813 - val_binary_accuracy: 0.8969 - val_loss: 3.0479\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9795 - loss: 2.7138 - val_binary_accuracy: 0.7632 - val_loss: 2.8604\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9795 - loss: 2.2969 - val_binary_accuracy: 0.7873 - val_loss: 2.4169\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9788 - loss: 1.9748 - val_binary_accuracy: 0.6974 - val_loss: 2.2280\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9686 - loss: 1.7568 - val_binary_accuracy: 0.9539 - val_loss: 1.7396\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9620 - loss: 1.5797 - val_binary_accuracy: 0.9298 - val_loss: 1.5786\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 1.4072 - val_binary_accuracy: 0.9474 - val_loss: 1.3977\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9759 - loss: 1.2438 - val_binary_accuracy: 0.7566 - val_loss: 1.5290\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9722 - loss: 1.1321 - val_binary_accuracy: 0.7456 - val_loss: 1.4168\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9664 - loss: 1.0413 - val_binary_accuracy: 0.6711 - val_loss: 1.4152\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9759 - loss: 0.9517 - val_binary_accuracy: 0.9254 - val_loss: 1.0384\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9803 - loss: 0.8451 - val_binary_accuracy: 0.9408 - val_loss: 0.9342\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9678 - loss: 0.7718 - val_binary_accuracy: 0.6140 - val_loss: 1.2117\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9708 - loss: 0.7155 - val_binary_accuracy: 0.9452 - val_loss: 0.8551\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9766 - loss: 0.6870 - val_binary_accuracy: 0.9518 - val_loss: 0.7857\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9737 - loss: 0.6331 - val_binary_accuracy: 0.9561 - val_loss: 0.7714\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9708 - loss: 0.5822 - val_binary_accuracy: 0.8158 - val_loss: 0.8361\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9708 - loss: 0.5602 - val_binary_accuracy: 0.9496 - val_loss: 0.6631\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9744 - loss: 0.5023 - val_binary_accuracy: 0.9474 - val_loss: 0.6070\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9722 - loss: 0.4783 - val_binary_accuracy: 0.7259 - val_loss: 0.8446\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9591 - loss: 0.5015 - val_binary_accuracy: 0.8268 - val_loss: 0.7748\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9708 - loss: 0.4951 - val_binary_accuracy: 0.8662 - val_loss: 0.7189\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9715 - loss: 0.4511 - val_binary_accuracy: 0.8947 - val_loss: 0.6380\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9744 - loss: 0.4040 - val_binary_accuracy: 0.8706 - val_loss: 0.5773\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9737 - loss: 0.3575 - val_binary_accuracy: 0.7237 - val_loss: 0.7699\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9825 - loss: 0.3100 - val_binary_accuracy: 0.7390 - val_loss: 0.6961\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9817 - loss: 0.2959 - val_binary_accuracy: 0.9320 - val_loss: 0.4514\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 0.3255 - val_binary_accuracy: 0.7127 - val_loss: 0.6635\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9825 - loss: 0.3142 - val_binary_accuracy: 0.8311 - val_loss: 0.5881\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.762427</td>\n",
              "      <td>15.345957</td>\n",
              "      <td>0.611842</td>\n",
              "      <td>36.772247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.900585</td>\n",
              "      <td>27.420950</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>31.229904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.915936</td>\n",
              "      <td>23.771082</td>\n",
              "      <td>0.682018</td>\n",
              "      <td>22.688292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.951023</td>\n",
              "      <td>18.636759</td>\n",
              "      <td>0.730263</td>\n",
              "      <td>17.007868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.961257</td>\n",
              "      <td>14.354318</td>\n",
              "      <td>0.594298</td>\n",
              "      <td>14.229979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.961257</td>\n",
              "      <td>11.101918</td>\n",
              "      <td>0.760965</td>\n",
              "      <td>9.882815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.958333</td>\n",
              "      <td>8.693851</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>7.506510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.967836</td>\n",
              "      <td>6.882955</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>6.012006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>5.551365</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>4.916865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.963450</td>\n",
              "      <td>4.540086</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>4.085406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>3.746958</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>3.419385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>3.181266</td>\n",
              "      <td>0.896930</td>\n",
              "      <td>3.047949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>2.713829</td>\n",
              "      <td>0.763158</td>\n",
              "      <td>2.860382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.979532</td>\n",
              "      <td>2.296860</td>\n",
              "      <td>0.787281</td>\n",
              "      <td>2.416949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>1.974759</td>\n",
              "      <td>0.697368</td>\n",
              "      <td>2.227989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>1.756818</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>1.739617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.961988</td>\n",
              "      <td>1.579698</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>1.578615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>1.407209</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.397740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.975877</td>\n",
              "      <td>1.243782</td>\n",
              "      <td>0.756579</td>\n",
              "      <td>1.529044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>1.132103</td>\n",
              "      <td>0.745614</td>\n",
              "      <td>1.416779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>1.041338</td>\n",
              "      <td>0.671053</td>\n",
              "      <td>1.415182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.951657</td>\n",
              "      <td>0.925439</td>\n",
              "      <td>1.038350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.845076</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>0.934180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.967836</td>\n",
              "      <td>0.771810</td>\n",
              "      <td>0.614035</td>\n",
              "      <td>1.211673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>0.715477</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.855098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.686986</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.785732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.633146</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.771370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>0.582216</td>\n",
              "      <td>0.815789</td>\n",
              "      <td>0.836092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>0.560207</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.663126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.502339</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.607026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.478262</td>\n",
              "      <td>0.725877</td>\n",
              "      <td>0.844602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.959064</td>\n",
              "      <td>0.501540</td>\n",
              "      <td>0.826754</td>\n",
              "      <td>0.774823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>0.495078</td>\n",
              "      <td>0.866228</td>\n",
              "      <td>0.718922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.451122</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0.638042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.403987</td>\n",
              "      <td>0.870614</td>\n",
              "      <td>0.577349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.357501</td>\n",
              "      <td>0.723684</td>\n",
              "      <td>0.769870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.310012</td>\n",
              "      <td>0.739035</td>\n",
              "      <td>0.696124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.295907</td>\n",
              "      <td>0.932018</td>\n",
              "      <td>0.451444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.325522</td>\n",
              "      <td>0.712719</td>\n",
              "      <td>0.663534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.314229</td>\n",
              "      <td>0.831140</td>\n",
              "      <td>0.588125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy       loss  val_binary_accuracy   val_loss\n",
              "0          0.762427  15.345957             0.611842  36.772247\n",
              "1          0.900585  27.420950             0.605263  31.229904\n",
              "2          0.915936  23.771082             0.682018  22.688292\n",
              "3          0.951023  18.636759             0.730263  17.007868\n",
              "4          0.961257  14.354318             0.594298  14.229979\n",
              "5          0.961257  11.101918             0.760965   9.882815\n",
              "6          0.958333   8.693851             0.947368   7.506510\n",
              "7          0.967836   6.882955             0.956140   6.012006\n",
              "8          0.961988   5.551365             0.947368   4.916865\n",
              "9          0.963450   4.540086             0.927632   4.085406\n",
              "10         0.969298   3.746958             0.960526   3.419385\n",
              "11         0.974415   3.181266             0.896930   3.047949\n",
              "12         0.979532   2.713829             0.763158   2.860382\n",
              "13         0.979532   2.296860             0.787281   2.416949\n",
              "14         0.978801   1.974759             0.697368   2.227989\n",
              "15         0.968567   1.756818             0.953947   1.739617\n",
              "16         0.961988   1.579698             0.929825   1.578615\n",
              "17         0.969298   1.407209             0.947368   1.397740\n",
              "18         0.975877   1.243782             0.756579   1.529044\n",
              "19         0.972222   1.132103             0.745614   1.416779\n",
              "20         0.966374   1.041338             0.671053   1.415182\n",
              "21         0.975877   0.951657             0.925439   1.038350\n",
              "22         0.980263   0.845076             0.940789   0.934180\n",
              "23         0.967836   0.771810             0.614035   1.211673\n",
              "24         0.970760   0.715477             0.945175   0.855098\n",
              "25         0.976608   0.686986             0.951754   0.785732\n",
              "26         0.973684   0.633146             0.956140   0.771370\n",
              "27         0.970760   0.582216             0.815789   0.836092\n",
              "28         0.970760   0.560207             0.949561   0.663126\n",
              "29         0.974415   0.502339             0.947368   0.607026\n",
              "30         0.972222   0.478262             0.725877   0.844602\n",
              "31         0.959064   0.501540             0.826754   0.774823\n",
              "32         0.970760   0.495078             0.866228   0.718922\n",
              "33         0.971491   0.451122             0.894737   0.638042\n",
              "34         0.974415   0.403987             0.870614   0.577349\n",
              "35         0.973684   0.357501             0.723684   0.769870\n",
              "36         0.982456   0.310012             0.739035   0.696124\n",
              "37         0.981725   0.295907             0.932018   0.451444\n",
              "38         0.969298   0.325522             0.712719   0.663534\n",
              "39         0.982456   0.314229             0.831140   0.588125"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bdl4 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,), kernel_regularizer=tf.keras.regularizers.l2(0.004)),\n",
        "                    BatchNormalization(momentum=0.99, # default is 0.99\n",
        "                                    epsilon=0.002, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.9)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.004)),\n",
        "                    BatchNormalization(momentum=0.99, # default is 0.99\n",
        "                                    epsilon=0.002, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.9)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation = 'sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.004))\n",
        "])\n",
        "\n",
        "model_bdl4.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bdl4 = model_bdl4.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bdl4 = pd.DataFrame(history_bdl4.history)\n",
        "df_bdl4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mixture of batch normalization, dropout, and L2 mixture of changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 - 3s - 538ms/step - binary_accuracy: 0.7961 - loss: 8.1593 - val_binary_accuracy: 0.5285 - val_loss: 21.3448\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9203 - loss: 15.5369 - val_binary_accuracy: 0.6425 - val_loss: 16.5078\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9423 - loss: 14.5325 - val_binary_accuracy: 0.7961 - val_loss: 13.5245\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9496 - loss: 12.2186 - val_binary_accuracy: 0.6360 - val_loss: 11.5483\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9518 - loss: 10.0211 - val_binary_accuracy: 0.9408 - val_loss: 8.8292\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9613 - loss: 8.1625 - val_binary_accuracy: 0.8969 - val_loss: 7.3746\n",
            "Epoch 7/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 6.7051 - val_binary_accuracy: 0.9452 - val_loss: 6.0035\n",
            "Epoch 8/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9664 - loss: 5.5830 - val_binary_accuracy: 0.9496 - val_loss: 5.0711\n",
            "Epoch 9/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9664 - loss: 4.7098 - val_binary_accuracy: 0.7741 - val_loss: 4.6575\n",
            "Epoch 10/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9686 - loss: 4.0034 - val_binary_accuracy: 0.8070 - val_loss: 3.9246\n",
            "Epoch 11/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9678 - loss: 3.4416 - val_binary_accuracy: 0.8860 - val_loss: 3.3354\n",
            "Epoch 12/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9773 - loss: 2.9932 - val_binary_accuracy: 0.7456 - val_loss: 3.2251\n",
            "Epoch 13/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9693 - loss: 2.6567 - val_binary_accuracy: 0.7346 - val_loss: 2.9529\n",
            "Epoch 14/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9686 - loss: 2.3520 - val_binary_accuracy: 0.8838 - val_loss: 2.3705\n",
            "Epoch 15/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9664 - loss: 2.1091 - val_binary_accuracy: 0.8311 - val_loss: 2.2505\n",
            "Epoch 16/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9678 - loss: 1.9042 - val_binary_accuracy: 0.9737 - val_loss: 1.7902\n",
            "Epoch 17/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9759 - loss: 1.6978 - val_binary_accuracy: 0.9474 - val_loss: 1.6621\n",
            "Epoch 18/40\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9773 - loss: 1.5479 - val_binary_accuracy: 0.9671 - val_loss: 1.4791\n",
            "Epoch 19/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9781 - loss: 1.3902 - val_binary_accuracy: 0.9561 - val_loss: 1.3495\n",
            "Epoch 20/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9803 - loss: 1.2594 - val_binary_accuracy: 0.9561 - val_loss: 1.2507\n",
            "Epoch 21/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9686 - loss: 1.1829 - val_binary_accuracy: 0.9452 - val_loss: 1.2300\n",
            "Epoch 22/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9766 - loss: 1.1040 - val_binary_accuracy: 0.9671 - val_loss: 1.0792\n",
            "Epoch 23/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9803 - loss: 1.0175 - val_binary_accuracy: 0.7654 - val_loss: 1.4419\n",
            "Epoch 24/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9759 - loss: 0.9520 - val_binary_accuracy: 0.9386 - val_loss: 1.0131\n",
            "Epoch 25/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9803 - loss: 0.8913 - val_binary_accuracy: 0.9518 - val_loss: 0.9511\n",
            "Epoch 26/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9730 - loss: 0.8302 - val_binary_accuracy: 0.9518 - val_loss: 0.8705\n",
            "Epoch 27/40\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9737 - loss: 0.7855 - val_binary_accuracy: 0.9649 - val_loss: 0.8056\n",
            "Epoch 28/40\n",
            "6/6 - 0s - 27ms/step - binary_accuracy: 0.9730 - loss: 0.7488 - val_binary_accuracy: 0.8640 - val_loss: 1.0785\n",
            "Epoch 29/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9773 - loss: 0.7063 - val_binary_accuracy: 0.9079 - val_loss: 0.8377\n",
            "Epoch 30/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9708 - loss: 0.6785 - val_binary_accuracy: 0.8553 - val_loss: 1.0327\n",
            "Epoch 31/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9730 - loss: 0.6671 - val_binary_accuracy: 0.9430 - val_loss: 0.7516\n",
            "Epoch 32/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9744 - loss: 0.6578 - val_binary_accuracy: 0.9518 - val_loss: 0.6948\n",
            "Epoch 33/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9730 - loss: 0.6193 - val_binary_accuracy: 0.9561 - val_loss: 0.6602\n",
            "Epoch 34/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9788 - loss: 0.5560 - val_binary_accuracy: 0.9496 - val_loss: 0.6124\n",
            "Epoch 35/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9810 - loss: 0.5078 - val_binary_accuracy: 0.9627 - val_loss: 0.5593\n",
            "Epoch 36/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9737 - loss: 0.4852 - val_binary_accuracy: 0.9474 - val_loss: 0.5423\n",
            "Epoch 37/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9737 - loss: 0.4603 - val_binary_accuracy: 0.9693 - val_loss: 0.4740\n",
            "Epoch 38/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9839 - loss: 0.4483 - val_binary_accuracy: 0.9145 - val_loss: 0.5949\n",
            "Epoch 39/40\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9766 - loss: 0.4583 - val_binary_accuracy: 0.9583 - val_loss: 0.5354\n",
            "Epoch 40/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9730 - loss: 0.4860 - val_binary_accuracy: 0.9276 - val_loss: 0.5846\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.796053</td>\n",
              "      <td>8.159341</td>\n",
              "      <td>0.528509</td>\n",
              "      <td>21.344782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.920322</td>\n",
              "      <td>15.536882</td>\n",
              "      <td>0.642544</td>\n",
              "      <td>16.507824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.942251</td>\n",
              "      <td>14.532502</td>\n",
              "      <td>0.796053</td>\n",
              "      <td>13.524459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.949561</td>\n",
              "      <td>12.218595</td>\n",
              "      <td>0.635965</td>\n",
              "      <td>11.548332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.951754</td>\n",
              "      <td>10.021094</td>\n",
              "      <td>0.940789</td>\n",
              "      <td>8.829206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.961257</td>\n",
              "      <td>8.162505</td>\n",
              "      <td>0.896930</td>\n",
              "      <td>7.374580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>6.705086</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>6.003544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>5.583021</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>5.071074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>4.709757</td>\n",
              "      <td>0.774123</td>\n",
              "      <td>4.657483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>4.003407</td>\n",
              "      <td>0.807018</td>\n",
              "      <td>3.924613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.967836</td>\n",
              "      <td>3.441612</td>\n",
              "      <td>0.885965</td>\n",
              "      <td>3.335424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>2.993209</td>\n",
              "      <td>0.745614</td>\n",
              "      <td>3.225147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.969298</td>\n",
              "      <td>2.656669</td>\n",
              "      <td>0.734649</td>\n",
              "      <td>2.952945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>2.351988</td>\n",
              "      <td>0.883772</td>\n",
              "      <td>2.370498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.966374</td>\n",
              "      <td>2.109137</td>\n",
              "      <td>0.831140</td>\n",
              "      <td>2.250512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.967836</td>\n",
              "      <td>1.904158</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>1.790202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.975877</td>\n",
              "      <td>1.697831</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.662052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>1.547920</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>1.479126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.978070</td>\n",
              "      <td>1.390153</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>1.349533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>1.259354</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>1.250747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>1.182944</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>1.230036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>1.104016</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>1.079212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>1.017508</td>\n",
              "      <td>0.765351</td>\n",
              "      <td>1.441904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.952037</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>1.013077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.891346</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.951116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.830152</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.870533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.785505</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.805639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.748767</td>\n",
              "      <td>0.864035</td>\n",
              "      <td>1.078479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.977339</td>\n",
              "      <td>0.706335</td>\n",
              "      <td>0.907895</td>\n",
              "      <td>0.837678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.970760</td>\n",
              "      <td>0.678534</td>\n",
              "      <td>0.855263</td>\n",
              "      <td>1.032707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.667052</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.751650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.657809</td>\n",
              "      <td>0.951754</td>\n",
              "      <td>0.694764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.619332</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.660173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>0.555989</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.612378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>0.507764</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.559253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.485202</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.542324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.460274</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.474047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.448288</td>\n",
              "      <td>0.914474</td>\n",
              "      <td>0.594852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.976608</td>\n",
              "      <td>0.458315</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.535443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.972953</td>\n",
              "      <td>0.485962</td>\n",
              "      <td>0.927632</td>\n",
              "      <td>0.584552</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy       loss  val_binary_accuracy   val_loss\n",
              "0          0.796053   8.159341             0.528509  21.344782\n",
              "1          0.920322  15.536882             0.642544  16.507824\n",
              "2          0.942251  14.532502             0.796053  13.524459\n",
              "3          0.949561  12.218595             0.635965  11.548332\n",
              "4          0.951754  10.021094             0.940789   8.829206\n",
              "5          0.961257   8.162505             0.896930   7.374580\n",
              "6          0.969298   6.705086             0.945175   6.003544\n",
              "7          0.966374   5.583021             0.949561   5.071074\n",
              "8          0.966374   4.709757             0.774123   4.657483\n",
              "9          0.968567   4.003407             0.807018   3.924613\n",
              "10         0.967836   3.441612             0.885965   3.335424\n",
              "11         0.977339   2.993209             0.745614   3.225147\n",
              "12         0.969298   2.656669             0.734649   2.952945\n",
              "13         0.968567   2.351988             0.883772   2.370498\n",
              "14         0.966374   2.109137             0.831140   2.250512\n",
              "15         0.967836   1.904158             0.973684   1.790202\n",
              "16         0.975877   1.697831             0.947368   1.662052\n",
              "17         0.977339   1.547920             0.967105   1.479126\n",
              "18         0.978070   1.390153             0.956140   1.349533\n",
              "19         0.980263   1.259354             0.956140   1.250747\n",
              "20         0.968567   1.182944             0.945175   1.230036\n",
              "21         0.976608   1.104016             0.967105   1.079212\n",
              "22         0.980263   1.017508             0.765351   1.441904\n",
              "23         0.975877   0.952037             0.938596   1.013077\n",
              "24         0.980263   0.891346             0.951754   0.951116\n",
              "25         0.972953   0.830152             0.951754   0.870533\n",
              "26         0.973684   0.785505             0.964912   0.805639\n",
              "27         0.972953   0.748767             0.864035   1.078479\n",
              "28         0.977339   0.706335             0.907895   0.837678\n",
              "29         0.970760   0.678534             0.855263   1.032707\n",
              "30         0.972953   0.667052             0.942982   0.751650\n",
              "31         0.974415   0.657809             0.951754   0.694764\n",
              "32         0.972953   0.619332             0.956140   0.660173\n",
              "33         0.978801   0.555989             0.949561   0.612378\n",
              "34         0.980994   0.507764             0.962719   0.559253\n",
              "35         0.973684   0.485202             0.947368   0.542324\n",
              "36         0.973684   0.460274             0.969298   0.474047\n",
              "37         0.983918   0.448288             0.914474   0.594852\n",
              "38         0.976608   0.458315             0.958333   0.535443\n",
              "39         0.972953   0.485962             0.927632   0.584552"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bdl5 = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,), kernel_regularizer=tf.keras.regularizers.l2(0.002)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.005, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.8)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002)),\n",
        "                    BatchNormalization(momentum=0.95, # default is 0.99\n",
        "                                    epsilon=0.001, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.8)), # default is gamma_initializer='ones'),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(1, activation = 'sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.003))\n",
        "])\n",
        "\n",
        "model_bdl5.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_bdl5 = model_bdl5.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv))\n",
        "\n",
        "df_bdl5 = pd.DataFrame(history_bdl5.history)\n",
        "df_bdl5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "6/6 - 0s - 33ms/step - binary_accuracy: 1.0000 - loss: 0.0016 - val_binary_accuracy: 0.9649 - val_loss: 0.1166\n",
            "Epoch 2/40\n",
            "6/6 - 0s - 20ms/step - binary_accuracy: 1.0000 - loss: 0.0017 - val_binary_accuracy: 0.9649 - val_loss: 0.1246\n",
            "Epoch 3/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0017 - val_binary_accuracy: 0.9671 - val_loss: 0.1099\n",
            "Epoch 4/40\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 1.0000 - loss: 0.0017 - val_binary_accuracy: 0.9649 - val_loss: 0.1169\n",
            "Epoch 5/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0015 - val_binary_accuracy: 0.9649 - val_loss: 0.1230\n",
            "Epoch 6/40\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 1.0000 - loss: 0.0015 - val_binary_accuracy: 0.9649 - val_loss: 0.1159\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001605</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.116611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001727</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.124611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001652</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.109905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001665</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.116906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.122972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001513</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.115893</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0              1.0  0.001605             0.964912  0.116611\n",
              "1              1.0  0.001727             0.964912  0.124611\n",
              "2              1.0  0.001652             0.967105  0.109905\n",
              "3              1.0  0.001665             0.964912  0.116906\n",
              "4              1.0  0.001453             0.964912  0.122972\n",
              "5              1.0  0.001513             0.964912  0.115893"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history_es = model.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv), callbacks=[early_stopping])\n",
        "\n",
        "df_es = pd.DataFrame(history_es.history)\n",
        "df_es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Model (batch normalization), checkpoint and test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Tensorflow checkpoint object\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Create Tensorflow checkpoint object which monitors the validation accuracy\n",
        "\n",
        "checkpoint_best_path = 'model_checkpoints_best/checkpoint.weights.h5'\n",
        "checkpoint_best = ModelCheckpoint(filepath=checkpoint_best_path,\n",
        "                             save_freq='epoch',\n",
        "                             save_weights_only=True,\n",
        "                             monitor='val_binary_accuracy',\n",
        "                             save_best_only=True,\n",
        "                             verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\xiang\\Downloads\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_binary_accuracy improved from -inf to 0.59211, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 3s - 459ms/step - binary_accuracy: 0.7953 - loss: 0.5148 - val_binary_accuracy: 0.5921 - val_loss: 1.7136\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 2: val_binary_accuracy improved from 0.59211 to 0.83333, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 29ms/step - binary_accuracy: 0.9291 - loss: 0.1855 - val_binary_accuracy: 0.8333 - val_loss: 0.4518\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 3: val_binary_accuracy improved from 0.83333 to 0.92982, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 29ms/step - binary_accuracy: 0.9466 - loss: 0.1485 - val_binary_accuracy: 0.9298 - val_loss: 0.2357\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 4: val_binary_accuracy improved from 0.92982 to 0.93421, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 32ms/step - binary_accuracy: 0.9583 - loss: 0.1296 - val_binary_accuracy: 0.9342 - val_loss: 0.2112\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 5: val_binary_accuracy improved from 0.93421 to 0.93860, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 36ms/step - binary_accuracy: 0.9561 - loss: 0.1163 - val_binary_accuracy: 0.9386 - val_loss: 0.1804\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 6: val_binary_accuracy improved from 0.93860 to 0.94737, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 29ms/step - binary_accuracy: 0.9583 - loss: 0.1075 - val_binary_accuracy: 0.9474 - val_loss: 0.1685\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 7: val_binary_accuracy did not improve from 0.94737\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9656 - loss: 0.0971 - val_binary_accuracy: 0.9430 - val_loss: 0.1573\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 8: val_binary_accuracy improved from 0.94737 to 0.94956, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 30ms/step - binary_accuracy: 0.9686 - loss: 0.0901 - val_binary_accuracy: 0.9496 - val_loss: 0.1627\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 9: val_binary_accuracy did not improve from 0.94956\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9700 - loss: 0.0866 - val_binary_accuracy: 0.9496 - val_loss: 0.1412\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 10: val_binary_accuracy improved from 0.94956 to 0.95395, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 29ms/step - binary_accuracy: 0.9737 - loss: 0.0790 - val_binary_accuracy: 0.9539 - val_loss: 0.1361\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 11: val_binary_accuracy did not improve from 0.95395\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9751 - loss: 0.0728 - val_binary_accuracy: 0.9474 - val_loss: 0.1277\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 12: val_binary_accuracy improved from 0.95395 to 0.95833, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 30ms/step - binary_accuracy: 0.9744 - loss: 0.0691 - val_binary_accuracy: 0.9583 - val_loss: 0.1248\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 13: val_binary_accuracy improved from 0.95833 to 0.96272, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 31ms/step - binary_accuracy: 0.9751 - loss: 0.0728 - val_binary_accuracy: 0.9627 - val_loss: 0.1204\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 14: val_binary_accuracy did not improve from 0.96272\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9788 - loss: 0.0612 - val_binary_accuracy: 0.9627 - val_loss: 0.1162\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 15: val_binary_accuracy did not improve from 0.96272\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9810 - loss: 0.0608 - val_binary_accuracy: 0.9583 - val_loss: 0.1106\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 16: val_binary_accuracy did not improve from 0.96272\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9825 - loss: 0.0527 - val_binary_accuracy: 0.9583 - val_loss: 0.1116\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 17: val_binary_accuracy did not improve from 0.96272\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9839 - loss: 0.0524 - val_binary_accuracy: 0.9605 - val_loss: 0.1107\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 18: val_binary_accuracy improved from 0.96272 to 0.96491, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 30ms/step - binary_accuracy: 0.9817 - loss: 0.0513 - val_binary_accuracy: 0.9649 - val_loss: 0.0978\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 19: val_binary_accuracy did not improve from 0.96491\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9854 - loss: 0.0456 - val_binary_accuracy: 0.9649 - val_loss: 0.0984\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 20: val_binary_accuracy improved from 0.96491 to 0.96930, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 31ms/step - binary_accuracy: 0.9861 - loss: 0.0442 - val_binary_accuracy: 0.9693 - val_loss: 0.0931\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 21: val_binary_accuracy improved from 0.96930 to 0.97149, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 32ms/step - binary_accuracy: 0.9912 - loss: 0.0362 - val_binary_accuracy: 0.9715 - val_loss: 0.0884\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 22: val_binary_accuracy did not improve from 0.97149\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9890 - loss: 0.0366 - val_binary_accuracy: 0.9693 - val_loss: 0.0864\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 23: val_binary_accuracy improved from 0.97149 to 0.97588, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 30ms/step - binary_accuracy: 0.9927 - loss: 0.0316 - val_binary_accuracy: 0.9759 - val_loss: 0.0789\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 24: val_binary_accuracy did not improve from 0.97588\n",
            "6/6 - 0s - 25ms/step - binary_accuracy: 0.9949 - loss: 0.0262 - val_binary_accuracy: 0.9715 - val_loss: 0.0760\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 25: val_binary_accuracy did not improve from 0.97588\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9920 - loss: 0.0293 - val_binary_accuracy: 0.9715 - val_loss: 0.0764\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 26: val_binary_accuracy did not improve from 0.97588\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9934 - loss: 0.0234 - val_binary_accuracy: 0.9715 - val_loss: 0.0740\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 27: val_binary_accuracy did not improve from 0.97588\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9949 - loss: 0.0237 - val_binary_accuracy: 0.9715 - val_loss: 0.0744\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 28: val_binary_accuracy did not improve from 0.97588\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9920 - loss: 0.0286 - val_binary_accuracy: 0.9715 - val_loss: 0.0775\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 29: val_binary_accuracy did not improve from 0.97588\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9927 - loss: 0.0226 - val_binary_accuracy: 0.9671 - val_loss: 0.0807\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 30: val_binary_accuracy did not improve from 0.97588\n",
            "6/6 - 0s - 21ms/step - binary_accuracy: 0.9942 - loss: 0.0192 - val_binary_accuracy: 0.9693 - val_loss: 0.0802\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 31: val_binary_accuracy improved from 0.97588 to 0.97807, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 31ms/step - binary_accuracy: 0.9942 - loss: 0.0181 - val_binary_accuracy: 0.9781 - val_loss: 0.0618\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 32: val_binary_accuracy did not improve from 0.97807\n",
            "6/6 - 0s - 24ms/step - binary_accuracy: 0.9978 - loss: 0.0127 - val_binary_accuracy: 0.9781 - val_loss: 0.0638\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 33: val_binary_accuracy did not improve from 0.97807\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9985 - loss: 0.0101 - val_binary_accuracy: 0.9759 - val_loss: 0.0706\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 34: val_binary_accuracy improved from 0.97807 to 0.98026, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 29ms/step - binary_accuracy: 0.9971 - loss: 0.0090 - val_binary_accuracy: 0.9803 - val_loss: 0.0655\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 35: val_binary_accuracy improved from 0.98026 to 0.98465, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 32ms/step - binary_accuracy: 0.9993 - loss: 0.0081 - val_binary_accuracy: 0.9846 - val_loss: 0.0580\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 36: val_binary_accuracy did not improve from 0.98465\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9985 - loss: 0.0055 - val_binary_accuracy: 0.9846 - val_loss: 0.0582\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 37: val_binary_accuracy improved from 0.98465 to 0.98684, saving model to model_checkpoints_best/checkpoint.weights.h5\n",
            "6/6 - 0s - 34ms/step - binary_accuracy: 0.9985 - loss: 0.0059 - val_binary_accuracy: 0.9868 - val_loss: 0.0618\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 38: val_binary_accuracy did not improve from 0.98684\n",
            "6/6 - 0s - 22ms/step - binary_accuracy: 0.9993 - loss: 0.0048 - val_binary_accuracy: 0.9868 - val_loss: 0.0570\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 39: val_binary_accuracy did not improve from 0.98684\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 0.9993 - loss: 0.0051 - val_binary_accuracy: 0.9868 - val_loss: 0.0610\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 40: val_binary_accuracy did not improve from 0.98684\n",
            "6/6 - 0s - 23ms/step - binary_accuracy: 1.0000 - loss: 0.0034 - val_binary_accuracy: 0.9759 - val_loss: 0.0958\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.795322</td>\n",
              "      <td>0.514847</td>\n",
              "      <td>0.592105</td>\n",
              "      <td>1.713555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.929094</td>\n",
              "      <td>0.185457</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.451777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.946637</td>\n",
              "      <td>0.148511</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>0.235699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.129600</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.211219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.116270</td>\n",
              "      <td>0.938596</td>\n",
              "      <td>0.180366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.107457</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.168535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.965643</td>\n",
              "      <td>0.097053</td>\n",
              "      <td>0.942982</td>\n",
              "      <td>0.157298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.968567</td>\n",
              "      <td>0.090092</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.162677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.970029</td>\n",
              "      <td>0.086568</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.141239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.078978</td>\n",
              "      <td>0.953947</td>\n",
              "      <td>0.136089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.072837</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.127709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.974415</td>\n",
              "      <td>0.069105</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.124752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.975146</td>\n",
              "      <td>0.072828</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.120383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.978801</td>\n",
              "      <td>0.061163</td>\n",
              "      <td>0.962719</td>\n",
              "      <td>0.116164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.980994</td>\n",
              "      <td>0.060832</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.110629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.052717</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.111602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.983918</td>\n",
              "      <td>0.052354</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.110684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.981725</td>\n",
              "      <td>0.051313</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.097759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.045555</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.098395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.044198</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.093138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.036227</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.088366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.989035</td>\n",
              "      <td>0.036574</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.086351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.031571</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.078947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.994883</td>\n",
              "      <td>0.026157</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.075980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.029309</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.076444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.993421</td>\n",
              "      <td>0.023416</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.073967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.994883</td>\n",
              "      <td>0.023651</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.074422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.991959</td>\n",
              "      <td>0.028647</td>\n",
              "      <td>0.971491</td>\n",
              "      <td>0.077550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.992690</td>\n",
              "      <td>0.022578</td>\n",
              "      <td>0.967105</td>\n",
              "      <td>0.080705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.019174</td>\n",
              "      <td>0.969298</td>\n",
              "      <td>0.080184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.994152</td>\n",
              "      <td>0.018052</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.061775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.997807</td>\n",
              "      <td>0.012709</td>\n",
              "      <td>0.978070</td>\n",
              "      <td>0.063795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.010136</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.070558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.997076</td>\n",
              "      <td>0.009043</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.065485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.008127</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.057975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.005459</td>\n",
              "      <td>0.984649</td>\n",
              "      <td>0.058210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.998538</td>\n",
              "      <td>0.005855</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.061756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.004816</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.057037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.999269</td>\n",
              "      <td>0.005144</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.060977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003359</td>\n",
              "      <td>0.975877</td>\n",
              "      <td>0.095780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    binary_accuracy      loss  val_binary_accuracy  val_loss\n",
              "0          0.795322  0.514847             0.592105  1.713555\n",
              "1          0.929094  0.185457             0.833333  0.451777\n",
              "2          0.946637  0.148511             0.929825  0.235699\n",
              "3          0.958333  0.129600             0.934211  0.211219\n",
              "4          0.956140  0.116270             0.938596  0.180366\n",
              "5          0.958333  0.107457             0.947368  0.168535\n",
              "6          0.965643  0.097053             0.942982  0.157298\n",
              "7          0.968567  0.090092             0.949561  0.162677\n",
              "8          0.970029  0.086568             0.949561  0.141239\n",
              "9          0.973684  0.078978             0.953947  0.136089\n",
              "10         0.975146  0.072837             0.947368  0.127709\n",
              "11         0.974415  0.069105             0.958333  0.124752\n",
              "12         0.975146  0.072828             0.962719  0.120383\n",
              "13         0.978801  0.061163             0.962719  0.116164\n",
              "14         0.980994  0.060832             0.958333  0.110629\n",
              "15         0.982456  0.052717             0.958333  0.111602\n",
              "16         0.983918  0.052354             0.960526  0.110684\n",
              "17         0.981725  0.051313             0.964912  0.097759\n",
              "18         0.985380  0.045555             0.964912  0.098395\n",
              "19         0.986111  0.044198             0.969298  0.093138\n",
              "20         0.991228  0.036227             0.971491  0.088366\n",
              "21         0.989035  0.036574             0.969298  0.086351\n",
              "22         0.992690  0.031571             0.975877  0.078947\n",
              "23         0.994883  0.026157             0.971491  0.075980\n",
              "24         0.991959  0.029309             0.971491  0.076444\n",
              "25         0.993421  0.023416             0.971491  0.073967\n",
              "26         0.994883  0.023651             0.971491  0.074422\n",
              "27         0.991959  0.028647             0.971491  0.077550\n",
              "28         0.992690  0.022578             0.967105  0.080705\n",
              "29         0.994152  0.019174             0.969298  0.080184\n",
              "30         0.994152  0.018052             0.978070  0.061775\n",
              "31         0.997807  0.012709             0.978070  0.063795\n",
              "32         0.998538  0.010136             0.975877  0.070558\n",
              "33         0.997076  0.009043             0.980263  0.065485\n",
              "34         0.999269  0.008127             0.984649  0.057975\n",
              "35         0.998538  0.005459             0.984649  0.058210\n",
              "36         0.998538  0.005855             0.986842  0.061756\n",
              "37         0.999269  0.004816             0.986842  0.057037\n",
              "38         0.999269  0.005144             0.986842  0.060977\n",
              "39         1.000000  0.003359             0.975877  0.095780"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best = Sequential([\n",
        "                    Dense(64, activation = 'relu', input_shape = (12288,)),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.006, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.7)), # default is gamma_initializer='ones'),\n",
        "                    Dense(48, activation='relu'),\n",
        "                    BatchNormalization(momentum=0.90, # default is 0.99\n",
        "                                    epsilon=0.006, #default is 0.001\n",
        "                                    axis = -1, #default is -1 (meaning the channel dimension is the last dimension)\n",
        "                                    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), # default is beta_initializer='zeros'\n",
        "                                    gamma_initializer=tf.keras.initializers.Constant(value=0.7)), # default is gamma_initializer='ones'),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_best.compile(\n",
        "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.050353614650626294),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "history_best = model_best.fit(train_set_x, y_train, epochs = 40, batch_size = 256, verbose = 2, validation_data=(cv_x,y_cv), callbacks= [checkpoint_best])\n",
        "\n",
        "df_best = pd.DataFrame(history_best.history)\n",
        "df_best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.987\n"
          ]
        }
      ],
      "source": [
        "get_test_accuracy(model_best, test_set_x.T, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the training and validation loss\n",
        "\n",
        "plt.plot(history_best.history['loss'])\n",
        "plt.plot(history_best.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot accuracy curves\n",
        "\n",
        "df_best.plot(y=['binary_accuracy', 'val_binary_accuracy'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
